Published as a conference paper at ICLR 2025
LONG PO: L ONG CONTEXT SELF -EVOLUTION OF
LARGE LANGUAGE MODELS THROUGH SHORT-TO-
LONG PREFERENCE OPTIMIZATION
Guanzheng Chen1,2,3,∗ Xin Li2,3,† Michael Qizhe Shieh1 Lidong Bing4
1National University of Singapore 2DAMO Academy, Alibaba Group
3Hupan Lab, 310023, Hangzhou, China
4Shanda AI Research Institute
gc.chen@u.nus.edu, xinting.lx@alibaba-inc.com
michaelshieh@comp.nus.edu.sg, lidong.bing@shanda.com
ABSTRACT
Large Language Models (LLMs) have demonstrated remarkable capabilities
through pretraining and alignment. However, superior short-context LLMs may
underperform in long-context scenarios due to insufficient long-context alignment.
This alignment process remains challenging due to the impracticality of human
annotation for extended contexts and the difficulty in balancing short- and long-
context performance. To address these challenges, we introduce LongPO, that
enables short-context LLMs to self-evolve to excel on long-context tasks by in-
ternally transferring short-context capabilities. LongPO harnesses LLMs to learn
from self-generated short-to-long preference data, comprising paired responses
generated for identical instructions with long-context inputs and their compressed
short-context counterparts, respectively. This preference reveals capabilities and
potentials of LLMs cultivated during short-context alignment that may be dimin-
ished in under-aligned long-context scenarios. Additionally, LongPO incorporates
a short-to-long KL constraint to mitigate short-context performance decline during
long-context alignment. When applied to Mistral-7B-Instruct-v0.2 from 128K to
512K context lengths, LongPO fully retains short-context performance and largely
outperforms naive SFT and DPO in both long- and short-context tasks. Specifi-
cally, LongPO-trained models can achieve results on long-context benchmarks
comparable to, or even surpassing, those of superior LLMs (e.g., GPT-4-128K)
that involve extensive long-context annotation and larger parameter scales. Our
code is available at https://github.com/DAMO-NLP-SG/LongPO.
1 I NTRODUCTION
Recent advancements in Large Language Models (LLMs) have revealed remarkable capabilities
through extensive pretraining and subsequent alignment with human intentions. The alignment pro-
cess, including methods such as Supervised Fine-Tuning (SFT) (Wei et al., 2022), Direct Preference
Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback
(RLHF) (Christiano et al., 2017; Ouyang et al., 2022; Stiennon et al., 2020), has effectively un-
leashed the potential of LLMs acquired during pretraining to achieve desired behaviors.
Although off-the-shelf alignment methods have made significant strides in short-context settings,
their application to long-context situations remains challenging (Bai et al., 2024). First, the scarcity
of high-quality, long-context annotated data poses a significant hurdle. Human annotation becomes
impractical and less-reliable as context length increases (Dubey et al., 2024), while synthetic data
generation using advanced LLMs lacks scalability and remains resource-intensive. Moreover, sim-
ply concatenating existing short-context datasets has been shown to yield unsatisfactory long-context
performance (Liu et al., 2024b). Second, long-context alignment methods grapple with the balance
∗This work was done during the internship of Guanzheng Chen at Alibaba DAMO Academy.
†Corresponding Author.
1
arXiv:2502.13922v3  [cs.CL]  1 Mar 2025
Published as a conference paper at ICLR 2025
60 65 70 75 80 85
Short-Context Performance
15
20
25
30
35
40Long-Context Performance
GPT-4-128K
LLaMA3.1-8B-Instruct-128K
GLM-4-9B-Chat-128K
Mistral-7B-LongPO-128K
Mistral-7B-Instruct-v0.2
GLM-4-9B-Chat-1M
+25.45
Figure 1: The comparison of long-context ( ∞Bench) and short-context (MMLU) performance
among GPT-4-128K and smaller LLMs.
between preserving short-context proficiency and cultivating long-context capabilities (Liu et al.,
2024b). For instance, the LLaMA-3.1 series incorporate merely 0.1% long-context data with over
99% short-context data during alignment to maintain the short-context performance (Liu et al.,
2024b). This limited exposure to natural long-context data may result in insufficient alignment,
potentially blocking the intrinsic long-context capabilities in LLMs.
The challenges of long-context alignment suggest that the full potential of LLMs may remain un-
tapped for long-context tasks. As illustrated in Figure 1, even superior models such as GPT-4,
which excel in short-context tasks, unexpectedly underperform in long-context scenarios. Interest-
ingly, despite the much stronger short-context capabilities, GPT-4 is still inferior to LLaMA3.1-8B
on long-context tasks. This disparity underscores the need for more effective long-context alignment
methods to fully unleash the intrinsic power of LLMs across variable context lengths.
In this work, we posit that the capabilities, deeply ingrained during short-context pretraining and
alignment, can be effectively transferred to longer contexts without external guidance. To this end,
we introduce Short-to- Long Preference Optimization (LongPO), to steer long-context alignment
by injecting internal short-context preferences into long-context scenarios. Specifically, we propose
to construct the preference data pairs by prompting the short-context LLM (e.g., Mistral-Instruct)
with two inputs: (1) a long input comprising an instruction over a long document and, (2) a short
input with the identical instruction over the relevant shortened chunk within the same document.
We then designate the responses to short and long inputs as chosen and rejected responses, respec-
tively. The short-to-long preference, i.e., the discrepancies between each paired response, reveal
the capabilities and potentials cultivated during short-context alignment that may be diminished in
under-aligned long-context scenarios. In order to bring forward the established capabilities, LongPO
is utilized to optimize the model towards short-to-long preferences using DPO-style objectives upon
long contexts. Furthermore, to maintain the short-context performance, we incorporate a short-to-
long constraint in LongPO by applying Kullback-Leibler (KL) divergence between the response
distributions to short and long inputs, respectively. This constraint, inspired by the KL constraint in
RLHF (Ouyang et al., 2022; Stiennon et al., 2020), guides the policy model to minimize the deviation
from its short-context output distribution when giving the long context during training. We found
that this straightforward constraint largely enhances the retention of short-context performance after
the long-context alignment.
We apply LongPO to Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) and Qwen2.5-7B-Instruct, while
iteratively extending their context lengths up to 512K, with the self-generated short-to-long pref-
erence data only. The experimental results demonstrate that LongPO, as a long-context alignment
method, surpasses naive SFT and DPO by large margins (over 10 points) in both long- and short-
context tasks. Notably, LongPO fully retains the performance of short-context LLMs after long-
context alignment, whereas SFT and DPO yield substantial performance degradation (10∼20 points
on most tasks). In terms of long-context performance, LongPO largely improves the Mistral-7B-
Instruct-v0.2 by 25.45 points on ∞Bench. Specifically, as depicted in Figure 1, the resulting model
2
Published as a conference paper at ICLR 2025
is comparable with superior long-context LLMs at various scales (e.g., Mistral-7B-LongPO-128K of
39.27 vs. GPT-4-128K of 34.81 on ∞Bench), despite the latter often involving extensive continual
training on hundreds of billions of tokens (Dubey et al., 2024) or labor-intensive long-context data
annotation (Zeng et al., 2024). These findings underscore the efficacy of our proposed method in
addressing the challenges of long-context alignment while simultaneously preserving short-context
capabilities, offering a more efficient and balanced approach to the development of long-context
LLMs.
2 P RELIMINARIES
In this section, we introduce two key methods for aligning language models with human preferences:
Reinforcement Learning from Human Feedback (RLHF, §2.1) and Direct Preference Optimization
(DPO, §2.2).
2.1 RLHF
Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Stiennon et al., 2020)
aims to optimize the policy model πθ to maximize rewards while maintaining proximity to a refer-
ence policy πref. Formally, the objective is
max
πθ
Ex∼D,y∼πθ(y∣x)[rϕ(x, y)]−βDKL[πθ(y ∣ x) ∣∣πref(y ∣ x)], (1)
where rϕ is the reward model that has been trained on ranked responses to reflect human prefer-
ence, β is a hyper-parameter controlling the deviation from reference policy, and DKL denotes the
Kullback-Leibler divergence. Typically, bothπθ and πref are initialized with identical model.
2.2 DPO
Considering the instability and difficulty of RLHF training, DPO (Rafailov et al., 2023) offers an
alternative approach by reparameterizing the reward functionr that incorporates the optimal policy:
r(x, y) =β log πθ(y ∣ x)
πref(y ∣ x) +β log Z(x), (2)
where Z(x) is the partition function. DPO assumes access to preference data D, which consists
of paired responses (yw, yl) to an instruction x. Specifically, the yw and yl represent the preferred
(winning) and dispreferred (losing) responses, respectively, based on human preference. Inspired by
the Bradley-Terry (BT) theory that models the preference distribution p∗ by
p∗(yw ≻yl ∣ x) =σ(r(x, yw)−r(x, yl)), (3)
where σ is the sigmoid function. DPO derives the preference optimization objective for the policy
model πθ as
LDPO(πθ; πref) =−E(x,yw,yl)∼D [σ(rθ(x, yw)−rθ(x, yl))]
=−E(x,yw,yl)∼D [log σ (β log πθ(yw ∣ x)
πref(yw ∣ x) −β log πθ(yl ∣ x)
πref(yl ∣ x))]. (4)
3 L ONG PO: S HORT-TO-LONG PREFERENCE OPTIMIZATION
Motivated by the challenges of data annotation and performance balance during long-context align-
ment, we introduce the Short-to-Long Preference Optimization (LongPO), to effectively empowers
a short-context LLM self-evolve to a long-context counterpart while preserving its original short-
context capabilities. The foundation of LongPO lies in the transfer of capabilities deeply ingrained
during short-context alignment to long-context scenarios by learning from short-to-long preference
(§3.1). Additionally, LongPO incorporates a short-to-long constraint based on the KL divergence
between short- and long-context models during training, to maintain the short-context performance
in a simple yet effective way (§3.2). In §3.3, we present the details of curating short-to-long prefer-
ence data without external guidance and self-evolving long context training process using LongPO.
3
Published as a conference paper at ICLR 2025
3.1 L EARNING FROM SHORT-TO-LONG PREFERENCE
As outlined in §2, aligning LLMs with human preference typically relies on datasets comprising
ranked responses to identical prompts or instructions. However, in long-context scenarios, con-
structing such datasets becomes impractical due to the extensive effort required for annotation. To
circumvent the external data annotation, we leverage theshort-to-long preferenceto internally trans-
fer capabilities well-established in the short-context alignment of LLMs to long-context counterpart.
Concretely, we assume access solely to a short-context LLM πS that has been well aligned. Given
a long input xL = [CL; IL] where CL is the long context and IL is the instruction, we can acquire
the response yL ∼πS(y ∣ xL) by conditioning on the entire context. Due to the limitations of πS in
handling long contexts, yL is likely to be of lower quality.
We then hypothesize an ideal extractor F that can rigorously identify and extract all essential infor-
mation CS within CL relevant to addressing IL:
CS =F(CL, IL). (5)
By querying the instruction IL based on CS, we obtain a new answer yS ∼ πS(y ∣ xS), where
xS = [CS; IL]. As CS is a shortened context for IL, the well-aligned short-context model πS should
be capable of producing a high-quality answer that aligns with human preferences.
Intuitively, yS can serve as a high-quality answer even when giving the whole long context, as
its conditioned context is self-contained for instruction IL. Hence, we definite the short-to-long
preference distribution pSL based on Bradley-Terry (BT) model following Eq. (3):
pSL(yS ≻yL ∣ xL) =σ(r(xL, yS)−r(xL, yL)). (6)
We now steer a policy model πθ (initialized with πS) to follow the preference distribution pSL,
forming the LongPO objective:
LLongPO(πθ; πref) =−E(xS,xL,yS,yL)∼DSL [σ(rθ(xL, yS)−rθ(xL, yL))], (7)
where DSL is the short-to-long preference data consisting of quadruples(xS, xL, yS, yL). This objec-
tive encourages the policy model to consistently accommodate the well-aligned short-context pref-
erence while deviating the under-aligned long-context preference. Therefore, LongPO internally
transfers preferences from short to long contexts without requiring external supervision, effectively
addressing the challenge of long-context data annotation.
3.2 S HORT-TO-LONG CONSTRAINT
Long-context alignment often leads to an imbalance between long- and short-context performance.
While this issue can be mitigated by carefully calibrating the scale and mixing proportion of long
and short data across various context lengths, such an approach is resource-intensive and time-
consuming. Moreover, an excessive incorporation of short-context data may inadvertently lead to
insufficient long-context alignment. In LongPO, we recognize that the degradation in short-context
performance during long-context alignment may be attributed to an improper (or missing) constraint
in current alignment methods.
Specifically, the RLHF and DPO objectives (implicitly) include a KL divergence term,βDKL[πθ(y ∣
x) ∣∣ πref(y ∣ x)], which serves as a constraint to prevent excessive deviation from the reference
model in Eq. (1). For a long input xL, this constraint C is expressed as:
C =βDKL[πθ(y ∣ xL) ∣∣πref(y ∣ xL)]. (8)
However, the reference model is typically the short-context model πS itself, which is not adept at
handling long contexts. This results in a problematic reference distribution πref(y ∣ xL), leading to
undesired deviation from the short-context model distribution.
To address this issue, we propose a short-to-long constraint leveraging the quadruples introduced
in Eq. (7). Recall that xS contains all the essential information from xL required to generate a
satisfactory response, πS can serve as a proficient reference model conditioned on xS. While for an
ideal reference model π∗
ref capable of handling context lengths from short to long, we should have:
DKL[π∗
ref(y ∣ xL) ∣∣π∗
ref(y ∣ xS)] =DKL[π∗
ref(y ∣ xS) ∣∣πS(y ∣ xS)] =0, (9)
4
Published as a conference paper at ICLR 2025
Figure 2: The procedure of generating short-to-long preference data from step 1 to 7.
namely π∗
ref(y ∣ xL) and πS(y ∣ xS) are identical distribution following Gibbs’ inequality. We
hence derive an adjusted short-to-long constraint between short-context reference model and “long-
context” policy model given contexts of different lengths:
C′ =βDKL[πθ(y ∣ xL) ∣∣πS(y ∣ xS)]. (10)
This refined constraint ensures that the policy model πθ operating on long contexts does not deviate
significantly from the short-context model πS when provided with the essential information. By
enforcing this constraint, we aim to preserve the short-context performance during long-context
alignment, thereby addressing the imbalance issue in a more principled manner.
By incorporating the short-to-long constraint in Eq. (2), we have a refined reward function for long
input xL (following derivation in Appx. §A.1):
rLongPO
θ (xL, y) =β log πθ(y ∣ xL)
πS (y ∣ xS) +β log Z(xL, xS), (11)
where xS is extracted from xL as illustrated in Eq. (5). Hence we access the LongPO objective:
LLongPO(πθ; πS) =−E(xS,xL,yS,yL)∼DSL [σ(rLongPO
θ (xL, yS)−rLongPO
θ (xL, yL))]
=−E(xS,xL,yS,yL)∼DSL [log σ (β log πθ(yS ∣ xL)
πS(yS ∣ xS) −β log πθ(yL ∣ xL)
πS(yL ∣ xS))].
(12)
3.3 S ELF -EVOLVING
Initialization. LongPO relies solely on access to a well-aligned short-context LLM, i.e., πS, in
conjunction with a long-context plain corpus. Note that the long-context corpus need not be metic-
ulously crafted, as it can be sampled and extracted from existing pretraining corpora of LLMs.
Construction of short-to-long preference data. The construction of short-to-long preference
data DSL introduced in §3.1 assumes an impractical extractor capable of retrieving essential in-
formation from long contexts for each instruction. To satisfy this hypothesis, we reversely prompt
πS to generate instructions for shortened chunks within long documents. This ensures that the short
context information is self-contained for instructions. Concretely, our data construction process
involves two steps as displayed in Figure 2:
1. Instruction Generation. For each long document CL, we randomly sample a shortened
chunk CS and prompt the πS to generate an instruction via the Self-Instruct (Wang et al.).
To ensure the diversity of instructions, the model is prompted to generate an instruction
pool first and then we randomly sample an instruction IL from this pool.
2. Response Generation. Using the generated instruction IL, we prompt πS to produce two
responses: a chosen response yS ∼πS(y ∣ xS) based on the short context xS, and a rejected
response yL ∼πS(y ∣ xL) derived from the long context xL.
5
Published as a conference paper at ICLR 2025
Iterative self-evolving training with LongPO. LongPO employs an iterative process to extend
LLM context length. Initially, a short-context LLM πS generates short-to-long preference data for
documents of length L1. The resulting model after LongPO training, now capable of handling L1
contexts, then serves as the new “short-context LLM” for the next iteration, generating data for an
extended length L2. This process repeats, progressively increasing context length capacity.
As there are multiple short chunks CS = {Ci
S}n
i=1 within a long document CL, we collect the
instruction-response triples (Ii
L, yi
S, yi
L) for each chunk within identical long document, to form a
multi-turn dataset ˆDSL. We then aggregate the probabilities across all turns to produce a multi-turn
LongPO objective:
LMT
LongPO(πθ; πS) =−E(xS,xL,yS,yL)∼ ˆDSL [log σ (β log ∑n
i=1 πθ(yi
S ∣ xi
L)
∑n
i=1 πS(yi
S ∣ Ci
S) −β log ∑n
i=1 πθ(yi
L ∣ xi
L)
∑n
i=1 πS(yi
L ∣ Ci
S))],
(13)
where xS ={[Ci
S; Ii
L]}n
i=1, xL ={[CL; Ii
L]}n
i=1, yS ={yi
S}n
i=1, and yL ={yi
L}n
i=1. LLMs trained with
LongPO do not necessarily involve continual training before, which may lead to instability when
processing long contexts. To address this issue and stabilize the training process, we incorporate
a continual training objective following Pang et al. (2024b). Specifically, we add the negative log-
likelihood (NLL) loss over entire long chosen sequencesSL =[xL; {Ii
L; yi
S}n
i=1] to LongPO objective.
Thus, our final training objective is:
Lθ =λ ⋅LMT
LongPO(πθ; πS)+LNLL(πθ; SL) =λ ⋅LMT
LongPO(πθ; πS)+ πθ(SL)
∣SL∣ . (14)
4 E XPERIMENTAL SETUP
4.1 T RAINING SETUP
Data Curation Details. We curate the short-to-long preference data based on a long-context cor-
pus sampled from the Book and ArXiv subsets of Long-Data-Collection1, and the GitHub subset of
RedPajama (Computer, 2023). For a specific target length (e.g., 128K tokens), we filter the corpus
to include only documents that are shorter than this length but longer than 64K tokens. Each long
document is then segmented into chunks of up to 32K tokens, with a maximum of 4 randomly-
sampled chunks retained per document. For instruction generation, we prompt short-context models
to generate 4 instructions per document, from which we randomly select one for further use. Af-
ter filtering undesired (censored and repetitive) responses, we collect 45K, 16K, 2.5K multi-turn
instruction-response samples of 128K, 256K, and 512K tokens for Mistral-7B, and 32K samples of
128K tokens for Qwen2.5-7B. More details are listed in Appx. §B.1.
Training Details. We extend the context length of Mistral-7B and Qwen2.5-7B using our LongPO
on short-to-long preference data specifically generated by models themselves. The training pro-
cess begins with utilizing Mistral-7B/Qwen2.5-7B to generate data with a length of 128K and ex-
tend the context length to 128K. To investigate the scalability, we utilize the resulting Mistral-7B-
LongPO-128K to generate data with lengths of 256K/512K and further extend the context length
to 256K/512K. We leverage Deepspeed-Ulysses (Jacobs et al., 2023) for sequence parallelism and
employ Flash Attention (Dao et al., 2022; Dao, 2023) for efficient computation. All models are
optimized using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 5e-7. We set the
margin β in Eq. (13) to 0.1 and the weighting factor λ in Eq. (14) to 0.01. RoPE θ is set as 1e7 and
the batch size is set as 8. More details are listed in Appx. §B.3.
4.2 E VALUATION BENCHMARKS
We assess both the long- and short-context capabilities of our models against baselines. The long-
context evaluation utilizes the following benchmarks:
• ∞Bench (Zhang et al.). We evaluate all models on three tasks in this benchmark: sum-
marization (En.Sum), long-book question answering (En.QA), and multi-choice question-
answering (En.MC). The evaluation length is beyond 100K.
1https://huggingface.co/datasets/togethercomputer/Long-Data-Collections
6
Published as a conference paper at ICLR 2025
• RULER (Hsieh et al., 2024). This benchmark comprises four types of synthetic tasks
across variable sequence lengths (4K to 128K): Needle-in-a-haystack (NIAH) retrieval,
Multi-hop Tracing with Variable Tracking (VT), Aggregation, and Question Answering
(QA). We exclude the Aggregation tasks, which involve word frequency counting within
the context, since they present challenges in word counting beyond mere long-context ca-
pabilities that current LLMs still struggle in.
• LongBench-Chat (Bai et al., 2024). This benchmark assesses instruction-following abil-
ities over long contexts (10K to 100K tokens), employing GPT-4-128K as an impartial
judge to evaluate model-generated responses. We filter out the English samples for fair
comparison across different models.
For short-context evaluation, we employ MMLU (Hendrycks et al., 2021), ARC-C (Clark et al.,
2018), Hellaswag (Zellers et al., 2019) and Winogrande (Sakaguchi et al., 2019) for assessing the
general language understanding and reasoning capabilities, and MT-Bench (Zheng et al., 2023) for
assessing instruction-following capability. More details are listed in Appx. §B.2.
4.3 B ASELINES
We train our LongPO on Mistral-7B-Instruct-v0.2 (denoted as Mistral-7B) and Qwen2.5-7B-
Instruct (denoted as Qwen2.5-7B), comparing them against a range of powerful LLMs including
GPT-4-128K, Qwen2-72B-Instruct (Yang et al., 2024), LLaMA-3.1-70B, LLaMA-3.1-8B, GLM-
4-9B-Chat, GLM-4-9B-Chat-1M, LWM-Text-Chat-1M (Liu et al., 2024b), and Yarn-Mistral-7b-
128k (Peng et al., 2023). Additionally, we establish baselines using Mistral-7B trained with conven-
tional SFT and DPO on the same dataset used by LongPO.
For short-context evaluation, we primarily compare the performance of naive LLMs against their
counterparts post-trained with SFT, DPO, and LongPO on our synthetic data. To provide a more
comprehensive comparison, we also include two series of open-source long-context language mod-
els: GLM-4-9B-Chat versus GLM-4-9B-Chat-1M, and LWM-Text-Chat-128k versus LWM-Text-
Chat-1M. This allows us to assess the effectiveness of our LongPO to maintain the short-context
performance during long-context alignment, comparing with baselines utilizing various strategies.
5 R ESULTS AND ANALYSES
In this section, we demonstrate the exceptional effectiveness of LongPO through two types of com-
parisons: (1) comparison with naive SFT and DPO trained on identical models and datasets; (2)
comparison with SOTA long-context LLMs.
5.1 C OMPARISON WITH SFT AND DPO
We first compare LongPO with conventional SFT and DPO using identical LLM (Mistral-7B). All
models are trained on equivalent self-generated datasets, as detailed in §4.1. Given the inability of
SFT to leverage preference data, we apply it to the instructions paired with chosen responses.
LongPO exhibits superior performance over SFT and DPO. The experimental results, illus-
trated in Table 1, reveal consistent and substantial performance gains (10 to 20+ points) of LongPO
over SFT and DPO across a diverse range of long-context tasks. Crucially, as depicted in Figure 3,
LongPO maintains robust short-context performance compared with original short-context LLMs
(59.99 vs 59.15 on MMLU), whereas SFT and DPO exhibit notable degradation in short-context
scenarios after long-context alignment process.
The performance disparity between LongPO and SFT can be attributed to the explicit integration
of short-to-long preference in LongPO, which is either absent or merely implicit in the chosen re-
sponses utilized by SFT. While both LongPO and DPO leverage the proposed short-to-long prefer-
ence data, the pivotal difference lies in the short-to-long constraint introduced in §3.2. The marked
performance gaps between LongPO and DPO, observed across both long- and short-context tasks,
highlight the effectiveness of the proposed constraint for successfully mitigating the problematic
limitations in DPO and retaining the short-context performance during long-context training. More
ablations are detailed in §5.3.
7
Published as a conference paper at ICLR 2025
Table 1: Long-Context Performance of our LongPO compared with baselines. Higher is better for
all metrics. Results marked with ♭ are evaluated by ourselves, while other results of baselines are
sourced from the original benchmarks. Full results on RULER are listed in Table 2.
Model Train/Claimed∞Bench RULER LongBench-Length En.Sum En.QA En.MC A VG. NIAH VT QA A VG. Chat (EN)GPT-4-128K 128K 14.73 22.44 67.25 34.81 95.4 99.9 70.3 88.53 8.40Qwen2-72B 128K 24.32♭ 7.03♭72.05♭34.47♭ 88.6 95.7 66.7 83.67 7.72♭LLaMA 3.1-70B 128K 33.55♭36.08♭69.00♭46.21♭ 96.1 93.2 67.8 85.7 6.67♭LLaMA 3.1-8B 128K 28.06♭30.47♭58.08♭38.87♭ 97.93 91.4 64.7 84.68 6.22♭GLM-4-9B 128K 14.84♭ 9.51♭67.25♭30.53♭96.51♭97.3♭64.8♭0 86.20♭ 5.67♭GLM-4-9B-1M 1M 28.3 9.7 68.6 35.53 98.2 99.4 69.4 89.0 5.03♭LWM-7B-1M 1M 4.33♭ 0.0♭ 3.06♭ 2.46♭ 87.20 57.5 56.4 67.03 1.25♭YaRN-Mistral-7B 128K 9.09 9.55 27.95 15.53 63.4 36.1 25.9 41.8 -Mistral-7B 32K 22.13 4.93 14.41 13.82 72.60 74.40 52.2 66.4 4.10- SFT 128K 23.44 13.45 53.21 30.03 88.73 79.64 51.08 73.15 4.25- DPO 128K 15.21 10.34 48.14 25.56 74.25 72.36 50.24 65.62 4.08- LongPO (iter1)128K27.0523.5167.2539.2796.8896.4964.8186.065.42- LongPO (iter2)256K28.1624.4366.3539.6596.8097.064.8786.225.48- LongPO (iter3)512K29.1027.8566.6741.2197.2897.4864.9286.565.80Qwen2.5-7B 128K 22.89 6.08 52.4 27.12 82.1 80.09 54.30 72.16 5.80- LongPO (iter1)128K32.0617.3272.0540.4895.8189.7159.481.645.75
MMLU ARC-C Hellaswag Winogrande MT-Bench
-10.0
-1.0
0.0
1.0
Margin
0.84
0.08
-0.21
0.13 0.10
0.32
1.02
-0.06
-0.26
0.40
-25.74
-14.80
-10.77 -14.11
-19.90
-14.04
-9.73 -9.07 -12.15
-22.00
-0.42
-1.11 -1.12
-0.55
-0.40
-1.03
-0.51
-0.25
-1.02
-2.40
LongPO (iter1) LongPO (iter2) SFT DPO GLM LWM
Figure 3: The margins of the short-context performance of Mistral-7B-LongPO and baselines rela-
tive to corresponding base model. GLM and LWM refer to the margins of GLM-9B-1M and LWM-
7B-1M over GLM-9B-128K and LWM-7B-128K, respectively. MT-Bench metrics ( ∈[0, 10]) are
linearly scaled to [0, 100] for better comparability across tasks. See numerical results in Table 3.
5.2 C OMPARISON WITH SOTA LONG -CONTEXT LLM S
To further substantiate the efficacy of LongPO, we conducted an extensive comparison between our
LongPO-trained Mistral-7B and leading long-context LLMs across varying model scales.
LongPO demonstrates exceptional competitiveness at similar scale. As detailed in Table 1,
LongPO demonstrates formidable competitiveness in terms of models at similar scales. For exam-
ple, Mistral-7B-LongPO significantly outperforms some established long-context models, including
LWM-7B and YaRN-Mistral, across all long-context tasks in ∞Bench and RULER. Remarkably,
Mistral-7B-LongPO-128K surpasses GLM-4-9B (39.27 vs. 30.53 on ∞Bench and 86.06 vs. 86.20
on RULER), although the latter is training on manually annotated long-context data spanning up
to 128K sequence length. Moreover, GLM-4-9B-1M, an extension of GLM-4-9B trained on con-
texts up to 1M tokens, demonstrates slightly superior performance than LongPO on the RULER
benchmark. However, these performance gains come at the costs of degenerated short-context per-
formance (0.41 on MMLU) and long-context instruction-following capability(0.64 on LongBench-
Chat (EN)) as illustrated in Figure 3. Notably, our models still outperform GLM-4-9B-1M on
∞Bench even trained with substantially shorter sequences. These results underscore the exceptional
efficiency of LongPO in transferring performance from short to long contexts through self-evolution,
thereby circumventing the need for extensive manual annotation.
8
Published as a conference paper at ICLR 2025
0 1000 2000 3000 4000 5000
Training Steps
20
40
60
80RULER-/glyph1197IAH
0 1000 2000 3000 4000 5000
Training Steps
40
50
60MMLU
LongPO LongPO (w/o NLL) SFT-Chosen SFT-Rejected DPO SFT-Chosen-Constraint
Figure 4: Long- and short-context performance comparison among LongPO, SFT on chosen re-
sponses (SFT-Chosen), SFT on rejected responses ( SFT-Rejected), DPO, and SFT on chosen re-
sponses with short-to-long constraint (SFT-Chosen-Constraint).
Long-context annotation is not sufficient. The superiority of our approach is particularly evident
in the En.QA task within ∞Bench, which involves complex free-form question answering over
extensive book-length contexts. In this challenging task, our models surpass both GLM-4-9B and
GLM-4-9B-1M by substantial margins (10+ points). The inherent difficulty of such task, which
poses challenges even for human annotators, highlights the limitations of relying solely on manually
annotated long-context data. By effectively transferring short-context capabilities to long-context
scenarios, LongPO demonstrates superior scalability and efficacy across diverse and intricate tasks.
Superior LLMs Yet to Dominate Long-Context Scenarios When benchmarked against leading
models such as GPT-4-128K, our LongPO-trained models still exhibit comparable or even superior
long-context performance (e.g., Mistral-7B-LongPO-128K of 39.27 vs. GPT-4-128K of 34.81 on
∞Bench), despite being based on significantly smaller Mistral-7B. This observation reveals that
even the most advanced LLMs have not yet achieved the same level of dominance in long-context
scenarios as they have in short-context tasks. This performance gap can be attributed primarily to the
scarcity of high-quality, large-scale long-context training data. The dearth of such data is particularly
impactful for larger LLMs, given the established scaling laws in language model training. This
finding underscores the potential of LongPO for enhanced performance without the requirement for
externally annotated long-context datasets.
5.3 A BLATION STUDIES
We conduct comprehensive ablation studies to investigate the efficacy of components in LongPO:
Effectiveness of short-to-long preference. The core of LongPO is learning the short-to-long pref-
erence between chosen and rejected responses given short and long contexts, respectively. To evalu-
ate this component’s effectiveness, we compare LongPO with two baseline methods: SFT on chosen
responses (SFT-Chosen) and on rejected responses (SFT-Rejected). SFT-Chosen implicitly incor-
porates short-context preference, while SFT-Rejected entirely omits it. As illustrated in Figure 4,
LongPO consistently outperforms both SFT variants in long-context performance (RULER-NIAH)
throughout the training process. This substantial improvement underscores the efficacy of our short-
to-long preference approach in enhancing long-context capabilities.
Effectiveness of short-to-long constraint. To assess the impact of our short-to-long constraint,
we compare LongPO with DPO upon short-to-long preference that removes this constraint. As evi-
dent in Figure 4, the unconstrained DPO demonstrates markedly inferior performance throughout the
training process, both in long- and short-context tasks. Notably, short-context capabilities degrade
rapidly in DPO during the initial training. Conversely, when we apply our short-to-long constraint to
naive SFT without explicit short-to-long preference, the model maintains short-context performance
on par with the original LLMs, even after long-context alignment. These results demonstrate the
crucial role of our short-to-long constraint in preserving short-context capabilities.
Impact of NLL loss. We investigate the effect of incorporating a negative log-likelihood (NLL)
loss over long context input and chosen response in Eq. (14) during LongPO training. As shown
in Figure 4, removing the NLL loss significantly degrades the long-context performance of LongPO
9
Published as a conference paper at ICLR 2025
across the training procedure. Specifically, the convergence of training for long-context performance
becomes slower. This demonstrates the crucial role of NLL loss in enhancing long-context capabil-
ities without resorting to continual training on long data.
6 R ELATED WORK
Alignment of LLMs. Aligning Large Language Models (LLMs) with human preferences and val-
ues has been crucial to unlocking their full potential from large-scale pretraining. The typical align-
ment process begins with Supervised Finetuning (SFT) on annotated instruction-response pairs. This
is followed by Reinforcement Learning from Human Feedback (RLHF), which aligns LLMs more
closely with human intentions through reward model training and policy optimization (Christiano
et al., 2017; Ouyang et al., 2022; Bai et al., 2022; Stiennon et al., 2020). To streamline RLHF train-
ing, Direct Preference Optimization (DPO) (Rafailov et al., 2023) and its variants (Ethayarajh et al.,
2024; Azar et al., 2023; Pang et al., 2024a; Hong et al., 2024; Meng et al., 2024) have been pro-
posed, eliminating the need for explicit reward model training by learning preferences directly from
human-ranked response pairs. While these alignment methods have shown significant success, they
heavily rely on human-annotated data. This reliance becomes problematic for long-context data,
where human annotation is both challenging and potentially less reliable.
Long-context extending of LLMs. Extending the context length of LLMs has been approached
through various methods. Some techniques involve scaling the rotary position embedding (Su et al.,
2022) followed by continual training on a small corpus of long documents (Chen et al., 2023b; Peng
et al., 2023; Rozière et al., 2023; Chen et al., 2023a). Alternative approaches, such as those proposed
by Jin et al. (2024); An et al. (2024), introduce hierarchical or chunked attention mechanisms to ex-
tend context length without additional training. However, these methods often involve limitations in
practical applications. Recent advancements include the work of Dubey et al. (2024), who proposed
continual pretraining on a massive long-context corpus (800B tokens) and incorporating a small
fraction (0.1%) of long-context data during SFT to enhance long-context capabilities. Zeng et al.
(2024) utilizes human-annotated long-context data for SFT and DPO to align long-context LLMs.
Despite their effectiveness, these methods require either extensive training or human annotation of
long-context data, making them prohibitively expensive and lack scalability.
Self-Evolving LLMs. Recent works (Yuan et al., 2024; Liu et al., 2024a; Li et al., 2024) have
unveiled the remarkable capability of Large Language Models (LLMs) to evolve from relatively
weak to significantly stronger performance through self-augmented data. Yuan et al. (2024);
Liu et al. (2024a) leverage iterative training on model-generated responses, ranked by LLM-as-
a-Judge (Zheng et al., 2023) prompting, to enhance model itself. Li et al. (2024) introduces the
instruction backtranslation to produce self-augmenting data that further enhances model capabili-
ties. Our work first extends the self-evolution property to the context length, to develop long-context
LLMs without relying on external annotations.
7 C ONCLUSION AND DISCUSSION
In this work, we propose LongPO, a novel long-context alignment method that enables LLMs to ef-
fectively transfer their short-context capabilities to long-context scenarios. Our approach addresses
key challenges in long-context alignment by leveraging intrinsic model knowledge, eliminating the
need for external long-context annotated data. LongPO is built on short-to-long preference data,
comprising paired responses for the same instruction given a long context and relevant shortened
chunk, respectively. By steering the policy model to learn from the discrepancies within these paired
responses, LongPO facilitates the transfer of established capabilities from short to long contexts. In
addition, LongPO incorporates a short-to-long constraint using KL divergence, that effectively pre-
serve short-context performance during training. Experimental results demonstrate that LongPO
significantly improves long-context performance across various tasks, outperforming existing align-
ment methods and even surpassing more sophisticated models. Importantly, this improvement is
achieved without sacrificing short-context proficiency. The success of LongPO highlights the poten-
tial of leveraging internal model knowledge for alignment tasks, opening new avenues for efficient
adaptation of LLMs to diverse context lengths.
10
Published as a conference paper at ICLR 2025
ACKNOWLEDGMENTS
This work was supported by DAMO Academy through DAMO Academy Research Intern Program.
REFERENCES
Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong.
Training-free long-context scaling of large language models, 2024.
Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal
Valko, and Rémi Munos. A general theoretical paradigm to understand learning from human
preferences. ArXiv, abs/2310.12036, 2023. URL https://api.semanticscholar.org/
CorpusID:264288854.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Ols-
son, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-
Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse,
Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mer-
cado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna
Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Con-
erly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario
Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai:
Harmlessness from ai feedback. 2022. URL https://arxiv.org/abs/2212.08073.
Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi
Li. Longalign: A recipe for long context alignment of large language models. 2024. URL
https://arxiv.org/abs/2401.18058.
Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen
Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard
(2023-2024). https://huggingface.co/spaces/open-llm-leaderboard-old/
open_llm_leaderboard, 2023.
Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Li Bing. Clex: Continuous length
extrapolation for large language models. ArXiv, abs/2310.16450, 2023a. URL https://api.
semanticscholar.org/CorpusID:264451707.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window
of large language models via positional interpolation. 2023b. URL https://arxiv.org/
abs/2306.15595.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. Advances in neural information processing sys-
tems, 30, 2017.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning
challenge. ArXiv, abs/1803.05457, 2018. URL https://api.semanticscholar.org/
CorpusID:3922816.
Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.
URL https://github.com/togethercomputer/RedPajama-Data.
Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.
URL https://arxiv.org/abs/2307.08691.
Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and
memory-efficient exact attention with IO-awareness. InAdvances in Neural Information Process-
ing Systems, 2022. URL https://arxiv.org/abs/2205.14135.
11
Published as a conference paper at ICLR 2025
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha
Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.
arXiv preprint arXiv:2407.21783, 2024.
Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model
alignment as prospect theoretic optimization. ArXiv, abs/2402.01306, 2024. URL https://
api.semanticscholar.org/CorpusID:267406810.
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Fos-
ter, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muen-
nighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lin-
tang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework
for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/
12608602.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Ja-
cob Steinhardt. Measuring massive multitask language understanding. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
d7KBjmI3GmQ.
Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without
reference model. ArXiv, abs/2403.07691, 2024. URL https://api.semanticscholar.
org/CorpusID:268363309.
Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang
Zhang, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language
models? arXiv preprint arXiv:2404.06654, 2024.
Sam Adé Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song, Samyam Ra-
jbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training
of extreme long sequence transformer models. ArXiv, abs/2309.14509, 2023. URL https:
//api.semanticscholar.org/CorpusID:262826014.
Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lu-
cile Saulnier, L’elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. ArXiv,
abs/2310.06825, 2023. URL https://api.semanticscholar.org/CorpusID:
263830494.
Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan
Chen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning, 2024.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Yoshua
Bengio and Yann LeCun (eds.),3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980.
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and
Mike Lewis. Self-alignment with instruction backtranslation, 2024. URL https://arxiv.
org/abs/2308.06259.
Aiwei Liu, Haoping Bai, Zhiyun Lu, Xiang Kong, Simon Wang, Jiulong Shan, Meng Cao, and
Lijie Wen. Direct large language model alignment through self-rewarding contrastive prompt
distillation, 2024a. URL https://arxiv.org/abs/2402.11907.
Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video
and language with ringattention. arXiv preprint, 2024b. URL https://arxiv.org/abs/
2402.08268.
Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization
with a reference-free reward. ArXiv, abs/2405.14734, 2024. URL https://api.
semanticscholar.org/CorpusID:269983560.
12
Published as a conference paper at ICLR 2025
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel-
ton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano,
Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human
feedback. ArXiv, abs/2203.02155, 2022. URL https://api.semanticscholar.org/
CorpusID:246426909.
Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason
Weston. Iterative reasoning preference optimization. ArXiv, abs/2404.19733, 2024a. URL
https://api.semanticscholar.org/CorpusID:269457506.
Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason
Weston. Iterative reasoning preference optimization, 2024b. URL https://arxiv.org/
abs/2404.19733.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window
extension of large language models. 2023. URL https://arxiv.org/abs/2309.00071.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and
Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.
In NeurIPS, 2023.
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton,
Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez,
Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and
Gabriel Synnaeve. Code llama: Open foundation models for code. 2023. URL https://
arxiv.org/abs/2308.12950.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver-
sarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances
in Neural Information Processing Systems, 33:3008–3021, 2020.
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: En-
hanced transformer with rotary position embedding. 2022. URLhttps://arxiv.org/abs/
2104.09864.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and
Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In
Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers). URL https://aclanthology.org/2023.acl-long.754.
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Interna-
tional Conference on Learning Representations, 2022. URL https://openreview.net/
forum?id=gEZrGCozdqR.
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,
Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang,
Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jin-
gren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin
Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao,
Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wen-
bin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng
Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu,
Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL
https://arxiv.org/abs/2407.10671.
13
Published as a conference paper at ICLR 2025
Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason
Weston. Self-rewarding language models. ArXiv, abs/2401.10020, 2024. URL https://api.
semanticscholar.org/CorpusID:267035293.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma-
chine really finish your sentence? In Annual Meeting of the Association for Computational Lin-
guistics, 2019. URL https://api.semanticscholar.org/CorpusID:159041722.
Team Glm Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu
Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng,
Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Ming yue Liu,
Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao,
Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiaoyu Xia, Xiaohan Zhang, Xiaotao
Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yi An, Yifan
Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang,
Zhenyi Yang, Zhengxiao Du, Zhen-Ping Hou, and Zihan Wang. Chatglm: A family of large
language models from glm-130b to glm-4 all tools. ArXiv, abs/2406.12793, 2024. URL https:
//api.semanticscholar.org/CorpusID:270562306.
Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen
Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. ∞Bench: Extending long context evaluation
beyond 100K tokens. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings
of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). URL https://aclanthology.org/2024.acl-long.814.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph Gonzalez, and Ion Stoica.
Judging llm-as-a-judge with mt-bench and chatbot arena. ArXiv, abs/2306.05685, 2023. URL
https://api.semanticscholar.org/CorpusID:259129398.
14
Published as a conference paper at ICLR 2025
A M ATHEMATICAL DERIVATIONS
A.1 D ERIVING THE LONG PO O BJECTIVE
In this section, we will derive the reward function of our LongPO objective in Eq. (11) by incorporat-
ing short-to-long constraint in Eq. (10). Starting from RLHF objective in Eq. (1) with short-to-long
constraint, we have
max
π
ExL∼D,y∼π[r(xL, y)]−βDKL[π(y∣xL)∣∣πS(y∣xS)] (15)
Following the DPO derivation process (Rafailov et al., 2023), we have:
max
π
E(xL,xS)∼ ˆDSL,y∼π(y∣xL)[r(xL, y)]−βDKL[π(y∣xL) ∣∣πS(y∣xS)]
=max
π
E(xL,xS)∼ ˆDSL Ey∼π(y∣xL) [r(xL, y)−β log π(y∣xL)
πS(y∣xS)]
=min
π
E(xL,xS)∼ ˆDSL Ey∼π(y∣xL) [log π(y∣xL)
πS(y∣xS) − 1
β r(xL, y)]
=min
π
E(xL,xS)∼ ˆDSL Ey∼π(y∣xL)
⎡⎢⎢⎢⎢⎢⎣
log π(y∣xL)
1
Z(xL,xS) πS(y∣xS)exp (1
β r(xL, y))
−log Z(xL, xS)
⎤⎥⎥⎥⎥⎥⎦
,
(16)
where we have partition function:
Z(xL, xS) =∑
y
πS(y∣xS)exp (1
β r(xL, y)).
The partition function is only related to xL, xS, and original short-context LLM πS. Hence we have
the optimal solution following Rafailov et al. (2023):
π∗(y∣xL) = 1
Z(xL, xS)πS(y∣xS)exp (1
β r(xL, y)). (17)
The optimal reward function would be derived:
r∗(xL, y) =β log π∗(y∣xL)
πS(y∣xS) +β log Z(xL, xS). (18)
We thus have:
p∗(y1 ≻y2∣xL) = exp (r∗(xL, y1))
exp (r∗(xL, y1))+exp (r∗(xL, y2))
=
exp (β log π∗(y1∣xL)
πS(y1∣xS) +β log Z(xL, xS))
exp (β log π∗(y1∣xL)
πS(y1∣xS) +β log Z(xL, xS))+exp (β log π∗(y2∣xL)
πS(y2∣xS) +β log Z(xL, xS))
= 1
1 +exp (β log π∗(y2∣xL)
πS(y2∣xS) −β log π∗(y1∣xL)
πS(y1∣xS) )
=σ (β log π∗(y1∣xL)
πS(y1∣xS) −β log π∗(y2∣xL)
πS(y2∣xS)).
By optimizing the πθ towards the optimal policy π∗, we finally access the objective of LongPO
in Eq. (12).
B E XPERIMENTAL DETAILS
B.1 D ATA CONSTRUCTION DETAILS
We prompt the Mistral-7B-Instruct-v0.2 to generate instructions with decode parameters of temper-
ature T = 0.7 and p = 0.9. The prompt of Self-Instruct to generate an instruction pool is shown
15
Published as a conference paper at ICLR 2025
Figure 5: The prompt for generating instruction pool.
in Figure 5. For generating the corresponding responses, we directly concatenate the short or long
context with corresponding instructions and adopt the greedy decoding to maintain the deterministic
behaviour of LLMs. As shown in Figure 6, the model would tend to prefer the high-quality chosen
response and deviate from the low-quality rejected response over long context, hence improve the
long-context capabilities.
B.2 E VALUATION DETAILS
On long-context benchmarks InfiniteBench and RULER, we evaluate our models and all baselines
following the settings in the original benchmarks. For short-context evaluation, we utilize the lm-
evaluaton-harness framework (Gao et al., 2024) and following the evaluation settings in (Beeching
et al., 2023): 5-shots for MMLU, 25-shots for ARC-C, 10-shots for Hellaswag, and 5-shots for
Winogrande. We use GPT-4-Turbo-1106-Preview as the judge for MT-Bench and LongBench-Chat
evaluation.
B.3 M ORE TRAINING DETAILS
Leveraging the DeepSpeed-Ulysses sequence parallel framework, we train the Mistral-7B/Qwen2.5-
7B with a sequence length of 128K on an 8 ×A800 80GB, achieving a throughput of 4,401 tokens
per second. For sequence lengths of 256K and 512K, the models are trained on a 16 ×A800 80GB,
yielding throughputs of 4,120 tokens per second and 2,744 tokens per second, respectively. To
facilitate a comparison with standard LLM alignment methods, we train Mistral-7B using SFT and
DPO utilizing the same short-to-long preference data of LongPO. For DPO training, we apply the
same settings as LongPO outlined in §4.1, but excluding the short-to-long constraint of LongPO
introduced in §3.2. Since SFT cannot utilize paired responses within preference data, we train it
using only the chosen responses provided alongside long context inputs. The hyperparameters for
SFT remain unchanged, except for an increase in the learning rate to 2e-5.
16
Published as a conference paper at ICLR 2025
(a) The rewards for chosen response during training.
 (b) The rewards for rejected response during training.
Figure 6: The chosen and rejected rewards during the training of Mistral-7B-LongPO-128K.
Table 2: Full results on 13 tasks of RULER benchmark. The bold values denote the average score
of 13 tasks in RULER over various context lengths.
Model Category 4k 8k 16k 32k 64k 128k A VG
Qwen2.5-7B-Instruct
NIAH 99.69 98.45 97.82 95.24 74.56 26.86 82.10
VT 99.88 99.72 96.24 96.44 81.44 6.84 80.09
AGG 92.52 89.78 92.08 81.93 62.48 28.23 74.50
QA 71.00 65.30 64.00 58.70 46.80 19.99 54.30
A VG (13 tasks) 94.19 92.11 91.61 87.66 68.96 24.47 76.50
Qwen2.5-7B-LongPO-128K
NIAH 99.64 98.97 97.80 95.54 94.80 88.15 95.82
VT 99.96 99.92 96.12 86.24 78.20 77.80 89.71
AGG 95.50 86.12 91.75 82.56 66.31 49.81 78.67
QA 70.00 64.00 62.70 57.70 53.00 49.00 59.40
A VG (13 tasks) 94.47 91.69 91.34 87.00 82.71 75.43 87.11
Mistral-7B-LongPO-128K
NIAH 99.43 98.64 98.09 97.84 95.82 91.44 96.88
VT 99.40 99.16 98.08 96.36 92.80 93.12 96.49
AGG 88.31 82.91 92.23 72.775 46.305 46.79 71.55
QA 71.10 70.15 66.60 65.80 61.00 54.20 64.81
A VG (13 tasks) 93.36 91.88 92.35 88.94 82.61 78.97 88.02
Mistral-7B-LongPO-256K
NIAH 99.16 97.79 98.02 97.76 96.53 91.54 96.80
VT 99.40 99.20 97.96 97.72 94.21 93.52 97.00
AGG 87.40 76.59 89.03 72.20 45.17 44.47 69.14
QA 71.50 69.50 66.70 64.30 60.80 56.40 64.87
A VG (13 tasks) 93.11 90.28 91.81 88.68 82.95 79.04 87.65
Mistral-7B-LongPO-512K
NIAH 99.19 97.78 98.06 97.69 96.62 94.36 97.28
VT 99.44 99.16 98.04 97.80 95.92 94.52 97.48
AGG 87.56 76.71 88.95 72.70 44.93 44.51 69.22
QA 71.40 69.50 66.40 64.50 60.60 57.10 64.92
A VG (13 tasks) 93.14 90.29 91.78 88.75 83.07 80.97 88.00
Table 3: Performance on short-context tasks.
Model MMLU ARC-C Hellaswag Winogrande MT-Bench
Mistral-7B-Instruct-v0.2 59.15 59.26 83.2 78.4 6.34
Mistral-7B-LongPO-128K 59.99 59.34 82.99 78.53 6.35
Mistral-7B-LongPO-256K 59.47 60.28 83.14 78.14 6.38
Mistral-7B-LongPO-512K 59.51 60.58 82.87 77.66 6.34
Qwen2.5-7B-Instruct 74.28 67.15 81.41 74.66 7.30
Qwen2.5-7B-LongPO-128K 73.64 65.70 80.82 74.98 7.62
17