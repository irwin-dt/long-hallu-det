{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a98507-30f3-4fc6-9eac-a1cdb2c3575c",
   "metadata": {},
   "source": [
    "### Evaluating your solution on the test set\n",
    "On the second day of the hackathon, we will release the test set for each team to run their hallucination detection methods. \n",
    "The test set comprises of: <br>1) A set of documents, which will be concatenated to form a long context of <128k <br>2) A list of 20 questions which tests a model's ability to extract, process and operate on information from a long context <br>3) A golden answer to the question <br>4) A specific model's outputs, which will be revealed when the test set is released <br>5) A golden hallucination label <br>What you need to do is to apply your hallucination detection solution on the model outputs and provide a classification. <br>The accuracy of your model will decide your score for the Performance (30%) aspect of the Hackathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e05c9c4-e283-449b-80a6-4a7a726e4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50f12bc6-70c2-43dc-8660-72865e1adcd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n===== Document: article1.txt =====\\n\\nPushing Boundaries in Flight: The 16th Singapore Amazing Flying Machine Competition Honours Innovation with Thrilling Demonstrations and a Celebration of Creativity\\n\\nSingapore, 5 April 2025 – The Awards Ceremony for the 16th Singapore Amazing Flying Machine Competition (SAFMC) 2025 took place today at the Singapore University of Technology and Design (SUTD). Co-organised by DSO National Laboratories (DSO) and Science Centre Singapore (SCS), this year’s competition brought together 1,865 participants across 584 teams, competing in five distinct categories.\\n\\n\\nThe ceremony\\'s Guest of Honour, Mr Heng Chee How, Senior Minister of State for Defence, experienced the thrill of the teams\\' innovative flying machines through live demonstrations and presented awards to the winners.\\n\\n \\n\\nSpeaking at the ceremony, Mr Heng highlighted, “This ceremony reminds us of the journey our participants have undertaken – a journey fuelled by creativity, the spirit of discovery, and an unwavering pursuit of excellence in innovation. You have gone beyond book learning to embrace the challenge of tackling complex problems, iterating on designs, and ultimately, turning abstract concepts and aerodynamics knowledge into amazing flying machines. Your participation is testament to the power of youth-driven innovation.” \\n\\n\\nMr Cheong Chee Hoo, Chief Executive Officer of DSO said, “Innovation takes flight when we dare to discover, learn, and adapt. This competition is more than just building a flying machine—it’s about embracing challenges, pushing boundaries, and igniting a passion for STEM. Every idea you test and every obstacle you overcome brings you closer to shaping the future of science and technology.”\\n\\n\\nAssociate Professor Lim Tit Meng, Chief Executive of the Science Centre Board remarked, “SAFMC is a powerful platform that brings STEM learning to life, encouraging students to think critically, experiment fearlessly, and innovate with purpose. Each year, we witness incredible ingenuity and determination, proving that such opportunities are key to inspiring a promising new generation of STEM leaders.”\\n\\n\\nSAFMC extends learning beyond the classroom, providing a unique, hands-on experience. Over three months, participants engage in workshops, refine their problem-solving skills, and design and construct their own flying machines—ranging from paper planes to drones. The competition culminates in Challenge Week, where they present their creations to a panel of judges.\\n\\n\\nEntries were assessed based on various criteria, including creativity, design, functionality, application of aerodynamics principles, and participants’ presentation skills.\\n\\nDifficulty raised for challenges\\n\\n\\nThis year\\'s advanced category challenges pushed participants to tackle greater complexity and explore new avenues for innovation. In Category D1 (Man-Machine Teaming), teams are allowed to customise their payload which allows them to focus on the human-machine interaction, rather than the pickup. For Category D2, no mechanical connection is required between drones. This encourages innovative collaborative strategies through simultaneous task execution. Lastly, Category E (Swarm) will feature a section of the playfield which will be unknown to the participants throughout the competition, simulating real-world conditions where situational information is scarce. Teams that were not able to complete the mission will continue to be able to try.\\n\\n\\nPerseverance pays off for Category D2 and E winners\\n\\n\\nICG@NTU clinched the championship title in Category E with a well-coordinated drone swarm strategy. The team divided their swarm into three specialised groups: one navigating the Known Search Area (KSA) using planned paths, another tackling the Unknown Search Area (USA) with monocular depth for obstacle avoidance, and a third operating in the pillar cluster (PC), where navigation aids helped \"trap\" drones within the cluster to locate victims. A central server managed offboard computations, while Ultra-Wideband (UWB) anchors ensured precise localisation, enabling most drones to land accurately on victim markers. Their strategic approach and effective use of UWB localisation secured their victory, making them the first team to achieve a perfect score in the category.\\n\\n\\n \\n\\nThe team previously placed fourth last year and finally clinched the pole position this year, showing perseverance does pay off.\\n\\n \\n\\nFalcon Duo from Nanyang Polytechnic emerged as the champion in Category D2, as they utilised unique navigational aids made of cardboard boxes scattered around the playfield, providing a reliable localisation solution without requiring elaborate infrastructure. Their machine featured a hook and carrier mechanism for payload pick-up and drop-off, ensuring precise control and structural integrity. They secured the win by autonomously piloting two drones to the drop zone and executing a successful simultaneous payload drop. \\n\\n \\n\\nThe team is a recurring participant in Category D2, having placed second last year, and competed in Category E this year. \\n\\nSince 2009, SAFMC has served as a launchpad for Singapore’s defence research and development talent, providing opportunities beyond the classroom for students to further their knowledge and passion in Science, Technology, Engineering, and Math.\\n\\nAbout Singapore Amazing Flying Competition\\n\\n \\n\\nThe Singapore Amazing Flying Machine Competition (SAFMC) is Singapore’s largest flying machine competition, jointly organised by DSO National Laboratories and Science Centre Singapore, and supported by the Ministry of Defence. SAFMC is one of the first competitions in Singapore that challenges participants to push the boundaries of innovation with wearable, collaborative, and swarming technologies for their flying machines.\\n\\n \\n\\nSince 2009, this nation-wide competition has attracted over 22,000 aviation enthusiasts. It will once again challenge young bright minds to come up with innovative creations like no other. SAFMC aims to inspire and nurture youths in Science and Technology. Open to all students and public, this annual competition serves as an expedient platform to those who want to get one step closer to their aviation dreams.\\n\\n \\n\\nAbout DSO National Laboratories\\n\\n \\n\\nDSO National Laboratories (DSO) is Singapore’s largest defence research and development organisation. For the past 5 decades, DSO has been undertaking indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\\n\\n \\n\\nWith more than 1,800 defence engineers and scientists, DSO develops cutting edge technologies and solutions to enhance Singapore’s defence and national security capabilities. For more information, please visit www.dso.org.sg. (the first clue is: \\'People Passion Innovation\\')\\n\\n \\n\\n \\n\\nAbout Science Centre Singapore\\n\\n\\nScience Centre Singapore, a non-formal educational institution and leading regional Science Centre, along with its group of attractions, brings out the wonders of science, technology, engineering and mathematics through its unique blend of exhibitions, educational programmes and events. A custodian of creativity and innovation, Science Centre Singapore has captured the evolution of scientific developments for nearly four decades.\\n\\n \\n\\nThe Centre and its partners have played a pivotal role in transforming the way students and the public interact with and learn about science, technology, engineering and mathematics. Since 1977, the Centre has welcomed over 30 million visitors and inspired them with more than 1,000 exhibits spread across 14 exhibition galleries and outdoor exhibition spaces.\\n\\n \\n\\nThe Centre’s group of attractions include Omni-Theatre, Snow City and KidsSTOP™. The Omni-Theatre, is Southeast Asia’s first 8K 3D digital theatre with a 23m wide seamless dome screen, is an immersive destination like no other. Snow City is Singapore’s only permanent indoor snow centre offering an Arctic inspired experience at Singapore’s first ice gallery and snow chamber. KidsSTOP™️ - Where every child gets to Imagine, Experience, Discover and Dream - is Singapore’s first children’s science centre offering an enriching experience through purposeful play for children aged 18 months to 8 years old.\\n\\n \\n\\nFor more information, please visit www.science.edu.sg.\\n\\n===== Document: article2.txt =====\\n\\n20 Mar 2025\\n\\nMINDEF, DSTA and DSO partner Mistral AI to advance generative AI for defence applications\\n\\nSingapore, 20 March 2025 – Singapore’s Ministry of Defence (MINDEF), Defence Science and Technology Agency (DSTA) and DSO National Laboratories (DSO) will partner France’s Mistral AI to co-develop generative AI models to augment the SAF’s sensemaking and decision support capabilities in areas such as mission planning.\\n\\n\\nThe collaboration will focus on fine-tuning Mistral AI’s Large Language Models (LLMs) and developing a mixture-of-experts (MoE) model, with support from AI Singapore, for the local operating context. Leveraging these models, users can effectively retrieve relevant information, empowering commanders by significantly improving AI-decision support. The flexibility offered by Mistral AI to deploy and manage these models on-premise within internet separated environments is critical for defence.\\n\\n\\nDSTA’s Deputy Chief Executive (Information) Ms Gayle Chan said, “Effective mission planning requires analysing vast amounts of data, a process that is highly demanding, resource-intensive and constrained by significant time pressure. In an increasingly complex environment, leveraging AI-enabled tools will support strategic decision-making of our commanders and enhance the agility of the SAF. By combining our expertise with Mistral AI’s capabilities, we aim to push the boundaries of what’s possible and drive meaningful impact.”\\n\\n\\nChieu Hai Leong, Distinguished Member of Technical Staff at DSO, said “We’re excited to collaborate with our partners to push the boundaries of AI and create an LLM that truly understands local contexts. This isn’t just about enhancing performance — it’s about empowering better decision-making.”\\n\\n\\nMs Majorie Janiewicz, Global Head of Revenue at Mistral AI said, “We are pleased to announce this partnership, which comes just a few weeks after the establishment of our office in Singapore. Our collaboration with leading technology organisations such as DSTA and DSO confirms our ability to provide secure and highly customisable AI solutions across strategic industries worldwide.”\\n\\n\\n\\n\\nAbout Defence Science and Technology Agency\\n\\n\\nThe Defence Science and Technology Agency (DSTA) is a top-notch technology organisation that drives innovation and delivers state-of-the-art capabilities to make the Singapore Armed Forces a formidable fighting force. Harnessing and exploiting science and technology, our engineers and IT professionals leverage multidisciplinary expertise to equip our soldiers with advanced systems to defend Singapore. DSTA also contributes its technological expertise to support national-level developments. To achieve our mission, DSTA excels in systems engineering, digitalised platforms, cyber, software development and more.\\n\\n\\nVisit www.dsta.gov.sg for more information.\\n\\n\\nAbout DSO National Laboratories\\n\\n\\nDSO National Laboratories (DSO) is Singapore’s largest defence research and development organisation. For the past 5 decades, DSO has been undertaking indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\\n\\n\\nWith more than 1,800 defence engineers and scientists, DSO develops cutting edge technologies and solutions to enhance Singapore’s defence and national security capabilities. For more information, please visit www.dso.org.sg. \\n\\n\\nAbout Mistral AI\\n\\n\\nMistral AI is a pioneer company in generative artificial intelligence, empowering the world with the tools to build and benefit from the most transformative technology of our time. The company democratizes AI through high-performance, optimized, and cutting-edge open-source models, products and solutions. Headquartered in France and independent, Mistral AI defends a decentralized and transparent approach to technology, with a strong global presence in the United States, United Kingdom, and Singapore.\\n\\n===== Document: article3.txt =====\\n\\nDSO National Laboratories Collaborates with Red Hat to Advance DSO’s Defense Research and Development Efforts\\n\\nDSO to explore DevSecOps practices and edge computing deployments built on Red Hat’s open hybrid cloud portfolio\\n\\n \\n\\nRALEIGH, N.C. – JUNE 13, 2023 – DSO National Laboratories (DSO), Singapore’s national  defense research and development (R&D) organization, and open source leader Red Hat, today announced a collaboration to develop new DevSecOps capabilities. The joint work between Red Hat and DSO shows the value of collaboration to facilitate knowledge exchange in Singapore’s defense R&D efforts.\\n\\n \\n\\nDevSecOps, an IT approach that combines development, operations and system security practices, encompasses culture, automation, and platform design, integrating security considerations as a shared responsibility throughout the entire IT lifecycle. Red Hat aims to collaborate with and support DSO’s DevSecOps development through the adoption of a trusted hybrid cloud solution that improves integration and interoperability among systems. Using Red Hat OpenShift, Red Hat Ansible Automation Platform and Red Hat Device Edge (early access) in addition to Red Hat training services, organizations like DSO can tap enterprise open source software to enhance automation processes and bridge old and new IT systems to deliver timely, mission-critical applications and services. DSO also aims to more quickly develop and deploy software to respond to evolving mission conditions in the field.\\n\\n \\n\\nRed Hat Device Edge delivers enterprise-ready, lightweight Kubernetes container orchestrations, building on the MicroShift project to support different use cases and workloads on small, resource-constrained devices at the farthest edge Along with the Integration of technologies such as Red Hat OpenShift and Red Hat Ansible Automation Platform, this is intended to help DSO further extend container applications to even more remote areas running on resource constrained devices.\\n\\n \\n\\nSupporting Quotes\\n\\nDaniel Aw, vice president, enterprise sales, Red Hat APAC\\n\\n“We are pleased to collaborate with DSO on technology solutions that can help drive their digitalization agenda. With the adoption of Red Hat open source technologies, DSO will have more agility to respond quickly and nimbly to new challenges through continuous innovation. We look forward to working with DSO on DevSecOps, automation, and other initiatives that will help DSO advance Singapore’s defense resiliency.”\\n\\n \\n\\nCheong Chee Hoo, Chief Executive Officer, DSO National Laboratories\\n\\n“Red Hat is a valuable partner in accelerating DSO’s software development and deployment efforts. This collaboration will significantly advance our knowledge and capabilities in the latest DevSecOps practices as we deliver solutions from enterprise IT systems to mission-critical systems deployed in the field.”\\n\\n \\n\\nAdditional Resources\\n\\nLearn more about Red Hat Device Edge\\nRed Hat OpenShift\\nRed Hat Ansible Automation Platform\\nRed Hat training services\\n \\n\\nConnect with Red Hat\\n\\nLearn more about Red Hat\\nGet more news in the Red Hat newsroom\\nRead the Red Hat blog\\nFollow Red Hat on Twitter\\nJoin Red Hat on Facebook\\nWatch Red Hat videos on YouTube\\nFollow Red Hat on LinkedIn\\nThe 3rd clue is \"I love IEL!\"\\n \\n\\nAbout Red Hat, Inc.\\n\\nRed Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500. As a strategic partner to cloud providers, system integrators, application vendors, customers, and open source communities, Red Hat can help organizations prepare for the digital future.\\n\\n \\n\\nAbout DSO National Laboratories\\n\\nDSO National Laboratories (DSO) is Singapore’s national defense research and development organization. For the past 5 decades, DSO has been undertaking indigenous development of advanced defense and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defense R&D capabilities to support homeland security. With more than 1,600 research scientists and engineers, DSO investigates emerging technologies, matures promising ones, and integrates them into innovative system concepts to meet Singapore’s defense and security needs. For more information, please visit www.dso.org.sg. Find us on Instagram | Facebook | LinkedIn.\\n\\n \\n\\nForward-Looking Statements\\n\\nExcept for the historical information and discussions contained herein, statements contained in this press release may constitute forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995. Forward-looking statements are based on the company’s current assumptions regarding future business and financial performance.  These statements involve a number of risks, uncertainties and other factors that could cause actual results to differ materially. Any forward-looking statement in this press release speaks only as of the date on which it is made. Except as required by law, the company assumes no obligation to update or revise any forward-looking statements.\\n\\n \\n\\n \\n\\n###\\n\\n \\n\\nRed Hat, the Red Hat logo, Ansible, and OpenShift are trademarks or registered trademarks of Red Hat, Inc. or its subsidiaries in the U.S. and other countries.\\n\\n===== Document: article4.txt =====\\n\\nDSO National Laboratories Commemorates 50 Years of Defence Science and Technology\\n\\nSingapore, 14 October 2022 – Prime Minister, Lee Hsien Loong graced DSO’s Golden Jubilee Dinner as its Guest-of-Honour at Shangri-La Hotel today. The event marks DSO’s 50-year journey in defence science and technology, and the relentless pursuit of its critical mission in developing technological surprises to enhance Singapore’s defence and national security capabilities. Prime Minister Lee and guests toured a special exhibition which unveiled never-been-seen archived documents, artefacts and photos.\\n\\nIn his keynote address, PM Lee said, “I am very happy that DSO is celebrating its Golden Jubilee. You have delivered many generations of impressive capabilities to the SAF, and now it is up to the current and future generations of DSO scientists to drive defence R&D for many years to come. And I am confident that you will continue to surprise us, as well as others, and provide the technology edge for our nation’s defence.”\\n\\nAs part of its Golden Jubilee celebrations, DSO also launched its DSO50 commemorative website, titled, “The Relentless Pursuit.” The website aims to provide an interactive and immersive experience in discovering the DSO story and spirit. Comprising six chapters, the website features interviews with past and present leaders, as well as stories of DSO’s capability build-up in key defence science and technology areas.\\n\\n \\n\\n###\\n\\n \\n\\nAbout DSO National Laboratories\\n\\nDSO National Laboratories (DSO) is Singapore’s national defence research and development organisation. For the past 5 decades, DSO has been undertaking indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\\n\\n \\n\\nWith more than 1,600 research scientists and engineers, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg\\n\\n===== Document: article5.txt =====\\n\\nDSO Marks 50 Years of Defence Science and Technology with the DSO50 Technology Showcase\\n\\nOne of the key events of DSO National Laboratories’ (DSO) Golden Jubilee, the closed-door event is the largest and most extensive technology showcase extended to guests from the Ministry of Defence (MINDEF), the Singapore Armed Forces (SAF), key public agencies and industry partners.\\n\\n19 July 2022 – Minister for Defence, Dr Ng Eng Hen, visited the DSO50 Technology Showcase (TSC) at the DSO Complex today, as part of DSO’s 50th anniversary celebrations. The TSC is a unique exhibition showcasing DSO’s history, capability built-up over the past five decades and future R&D endeavours for MINDEF, the SAF, and Whole-of-Government efforts.\\n\\n \\n\\nThe media was given a glimpse of DSO’s innovations in five key technology domains – Cryptography, Cybersecurity, Miniaturised Radio Frequency and Electronics, Artificial Intelligence / Data Analytics and Unmanned Systems. Many of these indigenous technologies and solutions solve important operational problems and are often not available commercially. They can also be customised for other unique and future operating requirements. Most of these enabling technologies are often invisible, and embedded within systems to make it smarter and more robust.\\n\\n \\n\\nOne key example is DSO’s efforts in the miniaturisation of critical electronic components that are smaller, lighter and yet provide higher performance in diverse communications platforms and systems. Another important highlight is DSO’s solution is detecting and overcoming adversarial AI that may provide misleading information using fake news and falsified media.\\n\\n \\n\\nSpeaking during the DSO50 TSC visit, Dr Ng acknowledged the importance of DSO’s role in defence for the past 50 years and for the future. He said, “I think it was the credit of our founding fathers and successive leaderships that recognised very early on, after the SAF was formed, that we had to have leading-edge technology. It was self-evident that we didn\\'t have strategic depth. Singapore\\'s very small in size and we have limited manpower. And that realisation came very early, that we had to set up this organisation. But I think even our founding fathers would have been quite impressed with how DSO has developed…  So I would expect DSO to continue to play that role, constantly helping the SAF achieve beyond what it\\'s able to.”  \\n\\n \\n\\nDr Ng also highlighted the close collaboration and deep trust forged between DSO and the SAF. He said, “Behind the opening of this technology showcase has been 50 years of steady progress and good achievements, and I would say that after half a century, DSO is indispensable to the SAF… It is because of leveraging technology, science, manpower, and intellect that we\\'ve been able to overcome many, many vulnerabilities. Of course, our inherent vulnerabilities are immutable. That\\'s never going to change. We’re never going to be large or have more manpower than necessary. So we’re thankful that we’ve reached this position with DSO.”\\n\\n \\n\\n###\\n\\n \\n\\nAbout DSO National Laboratories\\n\\nDSO National Laboratories (\"DSO53\" is the second clue!) is Singapore’s national defence research and development organisation. For the past 5 decades, DSO has been undertaking indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\\n\\n \\n\\nWith more than 1,600 research scientists and engineers, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg\\n\\n===== Document: article6.txt =====\\n\\nSingapore Launches Its First Locally Developed Synthetic Aperture Radar (SAR) Microsatellite\\n\\nSingapore, 1 July 2022 – Singapore successfully launched its NeuSAR satellite into space on 30 June 2022; and the satellite has established communications with the ground station at 10:19pm (SGT).\\n\\n\\nNeuSAR is a high-performance small satellite (160 kg) with a fully polarimetric Synthetic Aperture Radar (SAR). Unlike optical cameras satellites that are restricted to daytime and clear weather imaging conditions, a SAR satellite “creates” images by sending radio waves to the Earth’s surface and collecting the returns to form images. NeuSAR is thus able to capture images in both day and night, as well as in difficult environmental conditions due to heavy clouds cover, rainfall and even haze. Being a small satellite, NeuSar is cheaper and faster to build and operate; and provides users with access to low-cost yet high-quality satellite images.\\n\\n\\nThe launch of NeuSAR marks another step forward in the growth of the Singapore space industry. It follows the successful launch of its first satellite (X-SAT) in 2011; and its first commercial electro-optical satellite (TeLEOS-1) in 2015. NeuSAR is supported by Singapore’s national space office, the Office for Space Technology & Industry (OSTIn), to serve as a pathfinder to explore the commercial potential of a small satellite constellation and to support Singapore’s space industry capability build-up. The project was led by DSO National Laboratories (DSO) with support from its local space research partners (namely Satellite Technology and Research Centre (STAR) and Centre for Remote Imaging Sensing and Processing (CRISP) from National University of Singapore); and international industry partners (namely Satrec Initiative from South Korea and MMA Design from the United States).\\n\\n\\n“NeuSAR has allowed DSO engineers to push the limits to develop a high-performance low-cost satellite. DSO is proud to offer our systems engineering expertise to develop NeuSAR; and continue our contribution to the growth of Singapore’s space sector,” said Mr Cheong Chee Hoo, Chief Executive Officer of DSO.\\n“The development of NeuSAR is testament to the deep technical capabilities Singapore currently possesses, as well as OSTIn’s efforts to grow the local space ecosystem further. We are committed to supporting the development of local capabilities in space-based technologies, including SAR satellites, to ensure that Singapore can effectively harness these technologies to serve national needs in domains such as aviation, maritime, climate and sustainability,” said Mr David Tan, Executive Director of OSTIn.\\nNeuSAR was launched aboard Indian Space Research Organisation’s (ISRO) Polar Satellite Launch Vehicle (PSLV)-C53 and took off from Satish Dhawan Space Centre SHAR at 8:32pm (SGT).\\n\\n\\n# # #\\n\\nAbout DSO National Laboratories\\nDSO National Laboratories (DSO) is Singapore’s national defence research and development organisation. With more than 1,600 research scientists and engineers, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg.\\n\\n\\nAbout the Office for Space Technology & Industry, Singapore\\nOSTIn is Singapore’s national space office. Hosted within the Singapore Economic Development Board (EDB) as an autonomous office, it is responsible for nurturing the development of space technologies to serve national imperatives, growing a globally competitive space industry in Singapore and fostering an enabling regulatory environment for Singapore’s space activities. To support these objectives, OSTIn also seeks to build international partnerships and contribute to strengthening the international governance regime for space activities. In addition, OSTIn also supports the development of talent for Singapore’s space sector and future workforce.\\n\\n\\nFor more information, please contact:\\nEdwin Yong                                                                      Fabius Chen\\nSenior Manager                                                              Senior Manager\\nCorporate Communications                                         Corporate Marketing & Communications\\nDSO National Laboratories                                           Singapore Economic Development Board\\nychanyao@dso.org.sg / 9116 6850                              fabius_chen@edb.gov.sg / 9766 5816\\n\\n===== Document: article7.txt =====\\n\\nSingapore’s First Portable Direct PCR-based Test Kit For Covid-19 Developed for Faster Diagnosis\\n\\nSynergising local research capabilities to achieve breakthrough test kits to support nationwide COVID-19 testing effort\\n\\nSINGAPORE, 18 July 2020 – Back in March 2020, DSO National Laboratories (DSO) successfully developed its Direct-Polymerase Chain Reaction (PCR) technology with the formulation of a new molecular-based assay to detect the presence of COVID-19 virus. Harnessing this capability, DSO partnered A*STAR to jointly develop RESOLUTE, Singapore’s first Direct -PCR COVID-19 test kit that does not require sample processing and can be completed in an hour, compared to a conventional PCR test which takes 2.5 hours or longer.\\n\\n\\nPCR tests are gold-standard diagnostics tools conducted in the laboratory where specialised manpower and equipment are required for sample processing before PCR testing. With DSO’s proprietary Direct-PCR technology, RESOLUTE allows the sample from the patients’ nasopharyngeal swabs to be directly tested without the need for RNA extraction*, prior to PCR testing.  This reduces dependence on RNA extraction reagents.\\n\\nThis direct-PCR test kit received HSA’s provisional authorisation in April. The Diagnostics Development (DxD) Hub, a national initiative led by A*STAR’s commercialisation arm A*ccelerate, has been working closely with a local biotech company in the production and deployment.\\n\\nDSO and A*STAR have since worked on an enhanced kit, RESOLUTE 2.0 which has been further simplified for usage. RESOLUTE 2.0 consists of a premixed solution by multiplexing the RESOLUTE series of direct PCR assays, to test for 2 viral and 1 human target in one single test. The RESOLUTE series has been distributed to multiple testing laboratories.\\n\\nInside a RESOLUTE test kit\\n\\nMr Frederick Chew, CEO of A*STAR said, “During this critical period, RESOLUTE is a vital addition to Singapore’s testing capacity. Diagnostics testing has been a key pillar of Singapore’s COVID-19 response to date. The DxD Hub crew has been working with a stellar DSO team to rapidly turnaround a product for deployment, reducing our national testing dependence on RNA extraction reagents. We will continue working closely together with DSO and the rest of the research ecosystem to develop dual-use technologies to benefit Singapore and strengthen our resilience.”\\n\\nThis collaboration is part of a strategic partnership between DSO National Laboratories and A*STAR that was inked earlier this year to tighten the defence-civilian research nexus. The scope of cooperation involves R&D across the biomedical and physical sciences, technical consultancy, staff exchanges and cross-attachments. This is also the first time both organisations have developed a product together to address public health needs. The intent here is also for public sector IP to be licenced to local companies for commercial translation. DSO and A*STAR are also currently working together, along with other industry partners, to develop therapeutic antibody treatment for COVID-19 patients.  \\n\\nCEO of DSO National Laboratories, Mr Cheong Chee Hoo added, “This is one of the strategic capabilities that DSO has been building up to deal with novel outbreaks. It is timely that we are able to work closely with A*STAR and industry partners to quickly develop and productise this capability into an effective test kit in less than 3 months to strengthen Singapore’s fight against COVID-19.”\\n\\n* RNA extraction – RNA is short for ribonucleic acid. one of the three major biological macromolecules (along with DNA and proteins) that are essential for all known forms of life  It also carries the genetic information of many viruses.\\n\\n###\\n\\nFor media queries and clarifications, please contact:\\n\\nDSO National Laboratories\\n\\nKenny Wong\\nHead, Corporate Communications\\nTel: +65 9850 5224\\nEmail: wengchen@dso.org.sg\\n\\nAgency for Science, Technology and Research (A*STAR)\\n\\nSunanthar Lu\\nAssistant Head, Corporate Communications\\nTel: +65 9727 2170\\nEmail: Sunanthar_Lu@hq.a-star.edu.sg\\n\\nRobin Chan\\nHead, Corporate Communications\\nTel: +65 9830 2610\\nEmail: Robin_Chan@hq.a-star.edu.sg\\n\\n\\nAbout DSO National Laboratories\\n\\nDSO National Laboratories (DSO) is Singapore’s national defence research and development organisation. It undertakes indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\\n\\nWith more than 1,500 research scientists and engineers, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg.\\n\\nAbout the Agency for Science, Technology and Research (A*STAR)\\n\\nThe Agency for Science, Technology and Research (A*STAR) is Singapore\\'s lead public sector R&D agency, spearheading economic-oriented research to advance scientific discovery and develop innovative technology. Through open innovation, we collaborate with our partners in both the public and private sectors to benefit society.\\n\\nAs a Science and Technology Organisation, A*STAR bridges the gap between academia and industry. Our research creates economic growth and jobs for Singapore, and enhances lives by contributing to societal benefits such as improving outcomes in healthcare, urban living, and sustainability.\\n\\nWe play a key role in nurturing and developing a diversity of talent and leaders in our Agency and research entities, the wider research community and industry. A*STAR’s R&D activities span biomedical sciences and physical sciences and engineering, with research entities primarily located in Biopolis and Fusionopolis. For ongoing news, visit www.a-star.edu.sg.\\n\\n===== Document: article8.txt =====\\n\\nClinical Trials For Covid-19 Antibody To Begin In Upcoming Months\\n\\nSINGAPORE 17 June 2020 - DSO National Laboratories (DSO) has discovered five antibodies that demonstrate neutralisation against COVID-19. Over the past three months, DSO scientists have been testing the five antibodies in the laboratory, and results show that they are all potent in blocking infection and effective against key mutations that have emerged in the virus during the pandemic.\\n\\nAll five antibodies were isolated from blood samples of recovered COVID-19 patients. This assures a higher degree of patient safety and efficacy, both critical factors for upcoming clinical trials.\\n\\nDr Conrad Chan, Principal Research Scientist and Laboratory Director (Applied Molecular Technology) from DSO explained, “Administration of an antibody obtained from a recovered patient transfers that person’s immunity to the recipient, enabling any patient to better fight the infection and recover faster. As antibodies remain in the system for close to a month, it can also be administered to prevent infection.\"\\n\\nSince March this year, DSO has screened hundreds of thousands of B cells1, and isolated the first two antibodies for testing within a month of receiving blood samples from the National Centre for Infectious Diseases and Singapore General Hospital. By harnessing DSO’s proprietary screening technique, the B cells are screened simultaneously with live virus, and antibodies with effective virus neutralising properties are quickly identified. This technique, developed in collaboration with the National University of Singapore Life Sciences Institute over the last five years, is part of DSO’s “Antibodies on Demand” strategy to counteract novel infectious disease outbreaks. This technique reduces both the time and manpower required as compared to typical cell-screening methods.\\n\\nWith the promising discovery, DSO, as part of a Whole-of-Government collaborative effort involving agencies such as the Ministry of Defence, Ministry of Health and the Economic Development Board, has brought together a Singapore-based consortium comprising government agencies, research institutes and biomedical companies to quickly advance the research towards clinical trials. Human trials for the lead antibody, AOD01, are planned to commence in the upcoming months, pending approval from the Health Sciences Authority. Manufacturing capabilities have also been provisioned to scale up therapeutic antibody treatment for COVID-19 patients upon the successful completion of clinical trials.\\n\\nDr Brendon Hanson, Principal Research Scientist and Project Lead said, “When clinical trials are completed and successful, we hope to be able to quickly translate the positive results from the laboratory into a viable effective treatment for COVID-19.”\\n\\nChief Executive Officer of DSO, Mr Cheong Chee Hoo added, “While still in its experimental phase, this discovery is an important milestone in Singapore’s fight against and managing life with COVID-19 until a vaccine is available. With an effective treatment, people will be more assured as they can be treated immediately and can expect to make a faster recovery. This prevents our healthcare system from being overwhelmed, and normalises our daily routine as we continue to live and interact as a community.”\\n\\n1 B cells – Antibodies are produced by the B cells of the human immune system in response to infection. Both antibodies and B cells can be found circulating in our blood.\\n\\nFor media queries and clarifications, please contact:\\n\\nDSO National Laboratories\\n\\nKenny Wong\\n\\nHead, Corporate Communications\\n\\nTel: +65 9850 5224\\n\\nEmail: wengchen@dso.org.sg\\n\\nDSO National Laboratories (DSO) is Singapore’s national defence research and development organisation. It undertakes indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\\n\\nWith more than 1,500 technical staff, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg.\\n\\n===== Document: article9.txt =====\\n\\nModified Musical Instruments Hype the Graduation Ceremony For Children with Special Needs\\n\\nSingapore, 29 September 2017 – Children with mobility impairments may not always have the right musical instruments available to them, but it certainly does not stop them from performing in a musical concert. In a unique partnership between SPD (formerly known as Society for the Physically Disabled) and Mod Squad, a Corporate Social Responsibility (CSR) movement by a group of engineers from DSO National Laboratories, musical instruments have been specially modified so that children from SPD’s Building Bridges EIPIC Centre can perform at their year-end graduation ceremony.\\n\\n\\nLed by Mr Yee Qing Xiang, a Defence Research Engineer at DSO, the team of 10 applied their engineering expertise to modify more than 20 musical instruments ranging from xylophones, cymbals, tambourines to simple percussions like bells. Drawing experience from their previous Hack-a-Toy initiative where toys were modified for children with special needs, the engineers are better acquainted with their physical constrains, and have implemented more advanced assistive devices to help improve their motor coordination.\\n\\n\\nThe team even designed their own printed circuit boards, used open-source electronic components and self-written programs to ensure that the children can use the various instruments with ease. Taking the xylophone which has three notes as an example, the modification involves intricate programming, soldering of wires and installation of motors to synchronise the rhythm through the use of a wired wristwatch, making overall coordination much easier.\\n\\n\\nMr Yee, Mod Squad Team leader, shared on their modification journey, “As engineers, it is really tempting to quickly dive in and use technology to solve problems, but for this project, we needed to take a step back to fully understand the unique needs and problems before coming up with a solution. It is unfamiliar territory to us and this has been a valuable experience as we harnessed the team’s diverse expertise, such as electrical circuit designs, mechanical design and 3D printing.”\\n\\nThis is the first time SPD’s Early Intervention Programme for Infant and Children (EIPIC) will be incorporating music performance in their graduation ceremony on 23 November. These specially customised instruments will enable the children to perform and help to further the children’s interest in music.\\n\\n\\nMs Becky Hoo, SPD’s Director of Children Services, said, “We are deeply heartened with the support of DSO in enhancing playtime for our children. This collaboration has brought tremendous joy and for the first time, allowed the kids to be fully immersed in playing music. We look forward to seeing more of such implementations of Assistive Technology through future partnership with these engineers.”\\n\\n\\n-End-\\n\\n\\nMEDIA CONTACTS\\n\\nName\\t\\nMr Kenny Wong\\n\\nHead\\n\\nCorporate Communications\\n\\nMr Edwin Yong\\n\\nAssistant Manager\\n\\nCorporate Communications\\n\\nContact\\t6450 4163 / 9850 5224\\t6450 4162 / 9116 6850\\nEmail\\twengchen@dso.org.sg\\tychanyao@dso.org.sg\\nAbout DSO National Laboratories\\nDSO National Laboratories (DSO) is Singapore’s national defence research and development organisation. It undertakes indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\\n\\nWith more than 1,500 technical staff, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg\\n\\n\\nAbout SPD\\nSPD is a voluntary welfare organisation that supports people with disabilities by promoting their interest, welfare and advancement so as to develop their potential to the fullest. Through programmes and services that encompass therapy, vocational skills training, assistive technology, early intervention, day care, and employment, educational and social support, we seek to enable people with disabilities to be self-reliant and independent. For more information, please visit www.spd.org.sg.\\n\\n===== Document: paper1.txt =====\\n\\nAre Long-LLMs A Necessity For Long-Context Tasks?\\nHongjin Qian1,2, Zheng Liu2∗, Peitian Zhang1, Kelong Mao1, Yujia Zhou1\\nXu Chen1, Zhicheng Dou1\\n1 Gaoling School of Artificial Intelligence, Renmin University of China\\n2 Beijing Academy of Artificial Intelligence\\n{chienqhj,zhengliu1026}@gmail.com\\nAbstract\\nThe learning and deployment of long-LLMs remains a challenging problem despite\\nrecent progresses. In this work, we argue that the long-LLMs are not a necessity to\\nsolve long-context tasks, as common long-context tasks are short-context solvable,\\ni.e. they can be solved by purely working with oracle short-contexts within the\\nlong-context tasks’ inputs. On top of this argument, we propose a framework\\ncalled LC-Boost (Long-Context Bootstrapper), which enables a short-LLM to\\naddress the long-context tasks in a bootstrapping manner. In our framework, the\\nshort-LLM prompts itself to reason for two critical decisions: 1) how to access to\\nthe appropriate part of context within the input, 2) how to make effective use of the\\naccessed context. By adaptively accessing and utilizing the context based on the\\npresented tasks, LC-Boost can serve as a general framework to handle diversified\\nlong-context processing problems. We comprehensively evaluate different types of\\ntasks from popular long-context benchmarks, where LC-Boost is able to achieve a\\nsubstantially improved performance with a much smaller consumption of resource.\\n1 Introduction\\nLarge language models (LLMs) are widely adopted for real-world applications. Many of the ap-\\nplications are associated with long-sequence inputs, such as long-document question answering\\nand summarization. As such, the LLMs are commonly expected to have a long working context\\n(a.k.a. long-LLMs) in order to confront such demanding scenarios [Bai et al., 2023, Zhang et al.,\\n2024a]. Unfortunately, the learning and deployment of long-LLMs are still challenging in multiple\\nperspectives. Particularly, many existing LLMs are initially introduced with a limited size of con-\\ntext (e.g., 2K for Llama-1 Touvron et al. [2023a], 4K for Llama-2 Touvron et al. [2023b], 8K for\\nLlama-3 2). Although the initial short-LLM can be fine-tuned to establish a much longer context, it is\\nlikely to take substantial costs; and more seriously, it is extremely resource-consuming to deploy the\\nlong-LLMs [Kaplan et al., 2020]. The continually training may also compromise the LLMs’ general\\ncapability over short contexts [Liu et al., 2023, Li et al., 2023a]. In fact, it remains an open problem\\nto explore new solutions which may tackle long-context tasks both effectively and efficiently.\\nIn this paper, we argue that most long-context tasks are short-context solvable. That is to say, the\\nlong-context tasks, despite associated with long-sequence inputs, can be addressed by merely working\\nwith short-contexts in a strategic way. For example, the reading comprehension or summarization of a\\nbook can be solved based on the extraction of necessary key facts from the book. The above argument\\nis akin to the working patterns of human beings and modern computers, where arbitrary long-form\\nproblems can always be decomposed and solved on top of a limited memory capacity [Adolphs,\\n1999, Bryant and O’Hallaron, 2011]. However, even if the above argument holds, it is still non-trivial\\n∗Corresponding author.\\n2https://llama.meta.com/llama3/\\nPreprint. Under review.\\narXiv:2405.15318v1  [cs.CL]  24 May 2024\\nHinton\\treceived\\tthe\\t2018\\tTuring\\tAward\\ttogether\\twith\\tYoshua\\tBengio\\tand\\tYann\\tLeCun,\\tfor\\ttheir\\twork\\ton\\tdeep\\tlearning\\nGeoffrey\\tEverest\\tHinton\\tis\\ta\\tBritish-Canadian\\tcomputer\\tscientist\\tand\\tcognitive\\tpsychologist\\tmost\\tnoted\\tfor\\this\\twork\\ton\\tartificial\\tneural\\tnetworks.\\t\\n…\\nTask.\\tWho\\treceived\\tTuring\\tAward\\twith\\tHinton?\\nAnswer.\\t\\tYoshua\\tBengio,\\tYann\\tLeCun\\nGeoffrey\\tEverest\\tHinton\\tis\\ta\\tBritish-Canadian\\tcomputer\\tscientist\\tand\\tcognitive\\tpsychologist\\tmost\\tnoted\\tfor\\this\\twork\\ton\\tartificial\\tneural\\tnetworks.\\t\\nAlexNet\\tdesigned\\tin\\tcollaboration\\twith\\this\\tstudents\\tAlex\\tKrizhevsky\\tand\\tIlya\\nTask.\\tHinton’s\\tcollaborators\\tover\\tthe\\tyears?\\nHinton\\tinvented\\tBoltzmann\\tmachines\\twith\\tDavid\\tAckley…\\nRetrieval Hinton\\treceived\\tthe\\t2018\\tTuring\\tAward\\ttogether\\twith\\tYoshua\\tBengio\\tand\\tYann\\tLeCun,\\tfor\\ttheir\\twork\\ton\\tdeep\\tlearning\\nGeoffrey\\tEverest\\tHinton\\tis\\ta\\tBritish-Canadian\\tcomputer\\tscientist\\tand\\tcognitive\\tpsychologist\\tmost\\tnoted\\tfor\\this\\twork\\ton\\tartificial\\tneural\\tnetworks.\\t…\\nTask.\\tWho\\treceived\\tTuring\\tAward\\twith\\tHinton?Geoffrey\\tEverest\\tHinton\\tis\\ta\\tBritish-Canadian\\tcomputer\\tscientist\\tand\\nTask.\\tHinton’s\\tcollaborators\\tover\\tthe\\tyears?\\n…\\nLong-LLM Answer.\\tAlex\\tKrizhevsky,\\tIlya\\tSutskeverShort-LLM\\nRetrieval\\nAnswer.\\t\\tYoshua\\tBengio,\\tYann\\tLeCunShort-LLM\\n… Hinton\\tinvented\\tBoltzmann\\tmachines\\twith\\tDavid\\tAckley\\nAlexNet\\tdesigned\\tin\\tcollaboration\\twith\\this\\tstudents\\tAlex\\tKrizhevsky\\tand\\tIlya…\\n…\\nAnswer.\\tDavid\\tAckley,\\tAlex\\tKrizhevsky,\\tIlya\\tSutskever\\t…\\nShort-LLM\\n(A)\\tBrute-force\\tSolution(B)\\tNaïve\\tRAG(C)\\tLC-Boost:\\tRAG(D)\\tLC-Boost:\\tDivide-and-Conquer\\nLong\\tContext\\nTaskShort-LLM\\nTask Answer\\nLong\\tContext\\nAccess:\\thow\\tto\\taccess\\tcontext?Utilize:\\thow\\tto\\tutilize\\tcontext?\\nFigure 1: Illustration for LC-Boost. The LLM is prompted to reason for how to access to proper\\ncontext and how to utilize the accessed context to solve the task. Toy Examples.(A) Brute-force\\nsolution. Despite correctness, it is unnecessarily expensive due to the processing of the entire context\\nsimultaneously. (B) Naive RAG. It is hard to handle problems like information aggregation, which\\nleads to the incomplete answer. (C) LC-Boost leverages RAG to tackle the problem, which produces\\nthe correct answer in a small cost. (D) LC-Boost processes the long-context via sequential scan,\\nwhich correctly solves the problem based on the comprehensively collected information.\\nto solve the long-context tasks purely based on short contexts. This is because different tasks call\\nfor distinct ways of accessing and utilizing information from the long context; therefore, there can\\nhardly be any fixed rules to handle all possible situations. To address this challenge, we propose a\\nmethod, called LC-Boost, where short-LLMs are employed to solve general long-context tasks in a\\nbootstrapping manner. LC-Boost operates with two critical reasoning steps. One is the reasoning of\\nAccess, where the LLM prompts itself to plan for how to access the appropriate part of context within\\nthe input. The other one is the reasoning of Utilize, where the LLM figures out how to make effective\\nuse of the accessed context. Thanks to the above design, LC-Boost is able to adaptively handle\\ndiversified long-context tasks according to their unique nature. For example, given a knowledge-\\ngrounded QA problem, the LLM may directly access to the knowledgable context through retrieval,\\nand generate the answer in the form of RAG. Besides, it may sequentially scan the long context\\nchunk-by-chunk if the task calls for the aggregation of specific information from the entire input.\\nThe following toy examples are presented to better illustrate the mechanism of LC-Boost (Figure 1).\\nParticular, there are two common approaches to tackle long-context problems: (A) the brute-force\\nmethod based on long-LLMs, (B) the surrogate methods, like RAG Xu et al. [2023a]. Despite being\\nstraightforward, the brute-force method is likely to incur huge unnecessary costs as the problem\\ncould be directly solved by simple surrogate methods, like RAG. On the other hand, although the\\nsurrogate methods may help in certain cases, they are likely to become useless in other situations.\\nFor instance, the RAG-based methods are inappropriate to handle information aggregation problems,\\nas showcased in (B). In contrast, LC-Boost is able to handle general long-context tasks thanks to the\\nproper reasoning of how to access and utilize the long-context information based on each specific\\ntask. As shown in (C), it can directly access to the needed information via retrieval and generate the\\nanswer based on RAG. Meanwhile, it can also process the entire context in a divide-and-conquer\\nmanner, which will fully collect the information and solve the problem presented in (D).\\nWe perform comprehensive experiments for LC-Boost, including both popular real-world long-\\ncontext problems, like question-answering and summarization of long documents, and a wide variety\\nof synthetic tasks. In our experiments, LC-Boost is able to achieve equivalent performances as the\\nbrute-force methods based on strong long-LLMs, e.g., GPT-4-128K. In many cases, its performances\\ncan even notably surpass the brute-force methods, probably due to the elimination of distracting\\n2\\ncontext. Besides, our experiments also underscore the importance of reasoning and adaptability, as\\nLC-Boost outperforms all short-LLM surrogates with predefined access and utilization of context.\\nTo summarize, our paper makes the following contributions. (1) We identify the research problem of\\nsolving long-context problems with short-LLMs. To the best of our knowledge, it is the first study\\nof its kind, which is important to not only address the problem itself but also meaningful to the\\nsustainability and energy-efficient running of AI industry in a broader sense. (2) We propose a novel\\nframework LC-Boost, which is able to adaptively handle general long-context tasks based on the\\nreasoning of how to access and utilize the long context. (3) We empirically verify the effectiveness of\\nLC-Boost based on its superior performances achieved from low resource-consumption.\\n2 LC-Boost\\n2.1 Preliminaries\\nLLMs can be succinctly defined as Y = γ(q), where γ(·) represents a selected LLM, q denotes a\\nuser query, and Y refers to the answer produced by the LLMs. As highlighted in many previous\\nstudies, e.g., [Ji et al., 2023, Lewis et al., 2020, Shuster et al., 2021], the knowledge embedded in an\\nLLM’s parameters is static and, consequently, often fails to adequately address user queries requiring\\nup-to-date or in-depth knowledge. To address this limitation, we can introduce external knowledge\\n(refer to as context X) into the LLMs. Additionally, tasks involving information aggregation (e.g.,\\nsummarization) also take a context X as input along with task instructions q. Thus, we can generally\\ndefine the model’s generation process w.r.t. a contextX as: Y = γ(q, X).\\nAs discussed in Section 1, in many scenarios, the context X is a long sequence, necessitating that\\nLLMs manage long contexts. However, most existing LLMs were originally introduced with limited\\ncontext sizes (e.g., 4K). Consequently, these models are unable to process inputs that exceed their\\ncapacity without truncation. In this paper, we characterize such scenarios as long-context problem. It\\ninvolves LLMs processing inputs that notably surpass their inherent context limitations, which can be\\nformally described by:\\nY = γ(q, X) s.t.|X| ≫L, (1)\\nwhere L denotes the native context length limit of the LLM. The most straightforward way to address\\nthe long-context problem is to increase the LLMs’ context length L, mitigating the challenges of long\\ncontexts. In this paper, we instead explore solving long-context tasks using short-context LLMs (e.g.,\\n4K) without increasing the model’s context length L.\\n2.2 Pilot Study: Are Most Long-Context Tasks Short-Context Solvable?\\nDespite the potential for fine-tuning LLMs to handle much longer contexts, this approach incurs sub-\\nstantial costs. Additionally, directly processing long contexts during the inference stage exponentially\\nincreases computing resource consumption, which is not environmentally friendly. In the following,\\nwe conduct a pilot study from both theoretical and empirical perspectives to explore the question:\\nAre most long-context tasks solvable with short contexts?\\nTheoretical Analysis Suppose we have an input variable X and an output variable Y, the relevant\\npart of X given Y is denoted by ˜X. An ideal ˜X should capture all relevant features of the original\\ninput variable X in relation to Y. In other words, the optimal ˜X represents the simplest mapping of\\nX that accurately preserves the mutual information I(X; Y). We therefore propose a Markov chain\\nX →˜X → Y. According to the data processing inequality (DPI), we have I(X; ˜X) ≥ I(X; Y),\\nwith equality holding if and only if ˜X constitutes a sufficient statistics [Cover, 1999, Tishby and\\nZaslavsky, 2015]. This suggests that, in an optimal setting, we can always find a subset ˜X ⊆ Xthat\\nprovides information at least as useful for generating the output Y as the full context X.\\nIn practical scenarios, obtaining the optimal ˜X is challenging due to various factors, such as empirical\\nerrors Mohri et al. [2018]. Thus, we can only estimate ˜X. Estimating ˜X directly from X might be\\nchallenging if X defines a large variable space. In this situation, we propose decomposing the original\\ninput variable X into a series of subsets, X = {X1, ··· , Xn} and process each subset variable\\n3\\nSingle-Doc QA Multi-Doc QA Summarization Few-shot Synthetic Code0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\nGPT-4 / Brute-force\\nGPT-4 / LC-Boost\\nGPT-3.5 / LC-Boost\\nFigure 2: Pilot Study Across Various Tasks: In the Brute-force setting, the entire context is processed\\nby GPT-4-128K. In the LC-Boost setting, the maximum context length is restricted to 4K, and\\nLC-Boost is utilized to solve the long-context problem with short context.\\nseparately. Thus, according to the chain rule for mutual information Cover [1999], we have:\\nI(X, ˜X) =I(X1, ··· , Xn; ˜X) =I(X1; ˜X) +\\nnX\\ni=2\\nI(Xi; ˜X|X1, ··· , Xi−1), (2)\\nwhich indicates that the mutual information I(X, ˜X) can be understood as the sum of the mutual\\ninformation of each subset Xi and ˜Xi given all previous subsets.\\nIn the scenario of Eq. 1, the variable X represents a long context and the variable Y is the output\\nanswer produced by a LLM. Thus, ˜X can be interpreted as the minimal necessary context from\\nthe long context X given the output answer Y. Inspired by Eq. 2, we can estimate an optimal ˜X\\nusing decomposed shorter contexts {X1, . . . ,Xn}. Thus, I(X; ˜X) can be computed by processing\\neach subset Xi individually. However, as the number of subsets n increases, accounting for all\\npreceding subsets becomes computationally demanding. To alleviate this burden, we propose\\nreducing the number of conditional subsets considered by replacing the entire sequence of previous\\nsubsets with a compressed surrogate ˆXi, which is iteratively derived using a compression function\\nˆXi = g( ˆXi−1, Xi−1). Consequently, Eq. 2 can be reformulated as follows:\\nI(X, ˜X) =I(X1, ··· , Xn; ˜X) ≃ I(X1; ˜X) +\\nnX\\ni=2\\nI(Xi; ˜X| ˆXi)). (3)\\nThe equality can be upheld under two specific conditions: (1) the decomposed variables{X1, . . . ,Xn}\\nare mutually independent, and (2) the compression function g(·) is optimally designed, ensuring that\\nthe compressed surrogate ˆXi encapsulates all relevant information from the preceding subsets with\\nrespect to ˜X. Otherwise, I(X, ˜X) can only be approximately estimated.\\nEmpirical Analysis To empirically assess the accuracy of estimating the minimal necessary context\\n˜X using decomposed short contexts {X1, . . . ,Xn}, we conduct pilot experiments across various tasks\\nrequiring long contexts. Specifically, we utilize GPT-4-128K to perform these tasks in two settings:\\n(1) feeding the entire long context into GPT-4-128K in a brute-force manner, instructing the model\\nto directly produce the output answer, and (2) decomposing the full context into short contexts and\\napplying the methods defined in Eq. 3 to approximate ˜X, which then guides the model to produce the\\nfinal output (the LC-Boost setting).\\nFigure 2 presents the experiment results, which generally indicate that LC-Boost consistently performs\\nas well as or better than the brute-force setting. In particular, for tasks such as QA, few-shot learning,\\nand synthetic tasks, LC-Boost outperforms the brute-force setting. This is because the decomposed\\nshort contexts for these tasks are more likely to be mutually independent given the input query which\\ncan be adequately supported by a few extracted contexts from the long context. By precisely locating\\nthese supported context, it can filter out irrelevant context of X that might otherwise undermine task\\n4\\nperformance. For tasks like summarization and code completion, the inherent properties of these\\ntasks require considering the mutual dependencies among all decomposed short contexts, making the\\nLC-Boost setting more challenging. However, as discussed in Eq. 3, when the compression function\\ng(·) is optimal, we can achieve the optimal ˜X. GPT-4 serves as such a strong compression function,\\nensuring that the compressed surrogate ˆXi is well-estimated. Consequently, in these tasks, LC-Boost\\nachieves performance that is equal to or slightly better than the brute-force setting.\\nThrough theoretical analysis, we can posit that long-context tasks are short-context solvable if we can\\nestimate a better minimal necessary context ˜X from the decomposed short contexts {X1, . . . ,Xn}\\nthan from the long context X. Empirical analysis supports this assumption, demonstrating that\\nin most cases, the estimation error of deriving ˜X from the long context X is often larger than\\nfrom the decomposed short contexts {X1, . . . ,Xn}. This indicates that using short contexts can\\nbe comparatively more advantageous than using the full context. Therefore, we can validate our\\nargument in Section 1: most long-context tasks, if not all, are short-context solvable.\\n2.3 The Proposed Method: LC-Boost\\nWe propose a method called LC-Boost, which utilizes short LLMs to solve general long-context tasks.\\nLC-Boost begins with an input query q and a long context X, with the goal of producing an output\\nanswer Y. Since the underlying LLM in LC-Boost has a limited context size (we limit LC-Boost\\nworking with 4K context length), directly generating the output answer Y is infeasible for long-\\ncontext tasks. To address this, we propose solving long-context tasks by strategically understanding\\nthe decomposed short contexts X = {X1, ··· , Xn}. From these short contexts, we aim to extract the\\nminimal necessary context ˜X to support the generation of the output answer Y.\\nLC-Boost achieves this goal through a decision-making process involving iterative interactions\\nbetween LC-Boost and the decomposed short contexts {X1, ··· , Xn} with respect to the input query\\nq. In the process, LC-Boost interact with each short context Xi, employing two types of actions:\\ninformation access and information utilization.\\nWe denote an action at time step i by ai and denote the relevant context LC-Boost obtains from the\\ni-th short context Xi by ˜Xi The action ai is predicted by considering the current short context Xi, the\\ninput query q, as well as all previous extracted relevant information ˜X1:i−1: ai = γ(q, Xi| ˜X1:i−1),\\nwhere γ(·) denotes LC-Boost’s underlying LLM.\\nPredicting the action ai in a continuous space is challenging as it requires the underling model to\\nreason about highly implicit relations among the input query, the current context, and the previous\\ncontexts. Therefore, we define a discrete action space A comprising: (1) [Task Understanding]:\\nanalyzing the query and task for initialization; (2)[Retrieve]: accessing text evidence by a retrieval\\nmethod; (3) [Move]: accessing the next short text context directly; These two are information access\\nactions which define the LC-Boost’s trajectory to access short contexts. (4) [Append]: generating\\nrelevant context ˜Xi independently, denoting by ˜Xi = ai(Xi); (5) [Merge]: generating relevant\\ncontext ˜Xi with respect to previous extracted relevant information, denoting by ˜Xi = ai(Xi| ˜X1:i−1);\\n(6) [Answer]: answering the user query and returning; (7) [Aggregation]: aggregating all relevant\\ninformation and returning. We define our LC-Boost frame in Algorithm 1.\\nThough the pre-defined action space A comprises only seven actions, LC-Boost serves as a general\\nframework sufficient for solving most long-context tasks. This effectiveness is based on the following\\nreasons: (1) Flexible accessibility:By utilizing both [Retrieve] and [Move] actions, LC-Boost\\ncan access any short context Xi ∈ Xin a flexible trajectory, avoiding the need to browse the entire\\nlong context. This makes the information accessing process more efficient.(2) Accurate information\\nacquisition: Through the [Append] and [Merge] actions, LC-Boost can either independently extract\\nrelevant information from the current short context, appending it to previously extracted information,\\nor merge the current relevant information into the previous relevant information. This capability\\nallows LC-Boost to acquire relevant information in a compatible manner, making it adaptable to many\\nknowledge-intensive tasks. and (3) Dynamic answering:Using the [Answer] and [Aggregate]\\nactions, LC-Boost can dynamically utilize the acquired relevant information to produce the target\\nform of the answer (e.g., a short answer for QA tasks via the [Answer] action, or a long answer for\\nsummarization tasks via the [Aggregate] action).\\n5\\nAlgorithm 1LC-Boost Framework\\n1: Input: Input query q, long context X\\n2: Output: Answer Y\\n3: Decompose long context X ← {X1, ··· , Xn}\\n4: Initialize extracted relevant context ˜X0 ← None\\n5: Perform [Task Understanding]\\n6: while i ≤ n do\\n7: Select an action ai ← ai = γ(q, Xi| ˜X1:i−1), ai ∈ A\\n8: if ai is [Move] then i ← i + 1, continue\\n9: if ai is [Retrieve] then retrieve evidence from X = {X1, ··· , Xn}\\n10: if ai is [Append] then generate relevant context by ˜Xi = ai(Xi)\\n11: if ai is [Merge] then generate relevant context by ˜Xi = ai(Xi| ˜X1:i−1)\\n12: if ai ∈ {[Answer],[Aggregation]} then generate answer Y = γ(q, ˜X1:i), break\\n13: i ← i + 1\\n14: end while\\n15: return answer Y\\nIn our pilot study depicted in Figure 2, we observe that while GPT-3.5 serves as an inferior foundation\\nmodel compared to GPT-4, it still demonstrates significant effectiveness when incorporated with\\nLC-Boost. Given considerations of efficiency and cost-effectiveness, we employ GPT-3.5 as the\\nfoundation model for LC-Boost in the subsequent experiments. Besides, we show the prompts used\\nin LC-Boost in Appendix B.\\n3 Experiments\\n3.1 Experiment Settings\\nWe evaluate LC-Boost and baseline models on 12 datasets, including: (1) Single-Doc QA: Narra-\\ntiveQA [Koˇciský et al., 2017], Qasper [Dasigi et al., 2021], and MultiFieldQA [Bai et al., 2023]. (2)\\nMulti-Doc QA: HotpotQA [Yang et al., 2018], 2WikiMQA [Ho et al., 2020], and MuSiQue [Trivedi\\net al., 2022]. (3) Summarization: GovReport [Huang et al., 2021] and MultiNews [Fabbri et al., 2019].\\n(4) Few-shot Learning: SAMSum [Gliwa et al., 2019]. (5) Synthetic Task: Passage Count [Bai et al.,\\n2023] and Self-Constructed Dataset. (6) Code Completion: LCC [Guo et al., 2023]. More details\\nabout the evaluation datasets and metrics are introduced in Appendix A.\\nWe compare our LC-Boost with three types of models: (1) Short LLMs (defined as with context length\\n< 32K): Llama2-7B-Chat-4K [Touvron et al., 2023b], Llama3-8B-Instruct-8K and Vicuna-v1.5-7B-\\n16K [Chiang et al., 2023]; (2) Long LLMs (defined as with context length ≥ 32K): LongChat-v1.5-\\n7B-32K [Li et al., 2023b], Mistral-7B-Instruct-v0.2-32K [Jiang et al., 2023a], Llama3-8B-80K Zhang\\net al. [2024b], Phi-3-mini-128K [Abdin et al., 2024] and Yi-9B-200K [AI et al., 2024]; (3) Closed-\\nSource LLMs: DeepSeek-v2 (236B MoE model, ranks top-tier in MT-Bench) [DeepSeek-AI, 2024],\\nClaude-3-Haiku3 and GPT-3.5-turbo-16K4. In the experiments, if the context length exceed the\\nmodel’s length limit, following Bai et al. [2023], we truncate the context from the middle since the\\nfront and end of the context may contain crucial information. We provide further implementation\\ndetails in Appendix B.\\n3.2 Main Results\\nTable 1 shows the overall experimental results for all models across all tasks. From the table, we\\nderive several key findings: First, LC-Boost, with a context length of 4K, outperforms all baseline\\nmodels in all tasks except for the Code Completion task. This result verifies LC-Boost’s capability to\\neffectively solve long-context tasks by strategically processing decomposed short contexts. Second,\\nlong LLMs generally perform better than short LLMs, indicating the effectiveness of fine-tuning\\nLLMs to adapt to long contexts. However, the performance of long LLMs is not consistently stable\\nacross different tasks. For example, Yi-9B-200K excels in the Code Completion task but does not\\n3https://www.anthropic.com/claude\\n4https://platform.openai.com/docs/models\\n6\\nTable 1: Main experiment results. The best results are in bold and the secondary results are marked\\nwith underline. We report the average scores (%) on the main tasks. The detailed scores over all\\ndataset are shown in Table 3.\\nModels Single-Doc Multi-Doc Summ. Few-shot Synthetic Code\\nShort LLMs (Context Length< 32K)\\nLlama2-7B-Chat-4K 24.9 22.5 26.6 40.7 6.3 52.4\\nLlama3-8B-Instruct-8K 37.3 36.0 26.5 42.7 15.0 57.5\\nVicuna-v1.5-7B-16K 28.0 18.6 27.5 40.8 8.9 51.0\\nLong LLMs (Context Length≥ 32K)\\nLongChat-v1.5-7B-32K 28.7 20.6 28.6 34.2 6.8 53.0\\nMistral-7B-Instruct-v0.2-32K 31.9 26.0 29.3 43.0 14.0 55.4\\nLlama3-8B-80K 43.6 43.1 30.2 42.9 19.6 53.6\\nPhi-3-mini-128K 33.5 38.2 28.8 36.0 19.9 60.1\\nYi-9B-200K 29.6 38.7 28.4 14.6 6.5 72.1\\nClosed-Source LLMs\\nDeepSeek-v2 (32K) 37.6 49.1 30.8 39.3 14.5 37.0\\nClaude-3-Haiku (200K) 41.9 45.4 30.1 7.2 25.5 16.9\\nGPT-3.5-turbo-16K 39.8 38.7 28.1 41.7 18.7 54.7\\nLC-Boost (4K) 47.8 56.4 31.8 44.1 27.5 59.0\\nshow consistent performance in other tasks such as single-doc QA, few-shot learning, and synthetic\\ntasks. This inconsistency suggests that adapting LLMs to long contexts may compromise their general\\nabilities. Last, LC-Boost consistently surpasses its underlying LLM, GPT-3.5-turbo-16K, across\\nall tasks by a notable margin. This demonstrates that LC-Boost can achieve improved performance\\nwhile simultaneously reducing resource costs, making LC-Boost an environmentally friendly method.\\n3.3 Ablation Study: Dynamic is Important\\nTo investigate the necessity of LC-Boost’s design, we conduct ablation studies by changing LC-\\nBoost’s action space A, resulting in different information acquisition strategies. We experiment\\nwith the following settings: (1) [Retrieve] only: Directly retrieve the most relevant short context.\\n(2) [Merge] only: Sequentially process all short contexts while considering the previously processed\\ncontext. (3) [Append] only: Sequentially process all short contexts independently. (4) [Merge]\\n& [Move]: Selectively process short contexts while considering the already processed context.\\n(6) [Append] & [Move]: Selectively process short contexts independently. (7): [Retrieve] &\\n[Move]: Retrieve the top-k relevant short contexts and selectively process a few of them. (8): Brute-\\nforce: Directly produce the answer based on the entire long context. (9) Random: For each short\\ncontext, randomly select an action. Based on the acquired information from each strategy, LC-Boost\\nthen selects either the [Answer] or [Aggregation] action to produce the final answer.\\nFigure 3 illustrates the results, from which we find that: (1) Compared to fixed processing strate-\\ngies, LC-Boost customizes the action trajectory for each query, resulting in notable performance\\nimprovements. This finding emphasizes the importance of the dynamic capabilities of LC-Boost.\\n(2) LC-Boost is particularly effective in single-doc QA and multi-doc QA tasks, as it can accurately\\nselect the minimal necessary context required to answer the input query, filtering out irrelevant\\ninformation from the long context. (3) In the few-shot learning task, LC-Boost does not significantly\\noutperform the fixed strategies. This is attributed to the numerous in-context examples provided\\nwithin the task, which offer substantial guidance, thus diminishing the impact of the number of\\nin-context examples on the final performance.\\n3.4 Case Study: Model Behavior Analysis on Self-Construct Dataset\\nIn Table 2, we present two case studies from the self-constructed dataset. These cases are particularly\\nchallenging as they require reasoning across the entire long context. Despite having sufficient context\\nsize, LLMs struggle to generate correct responses. In contrast, LC-Boost dynamically customizes\\nsolutions for each case, thereby effectively solving the problems using a shorter context length.\\n7\\n[Append]\\n[Merge]\\n[Retrieve]\\n[Append] & [Move]\\n[Merge] & [Move]\\n[Retrieve] & [Move]\\nLC-Boost\\nRandom\\nBrute-force\\nNarrative\\n[Append]\\n[Merge]\\n[Retrieve]\\n[Append] & [Move]\\n[Merge] & [Move]\\n[Retrieve] & [Move]\\nLC-Boost\\nRandom\\nBrute-force\\nHotpot\\n[Append]\\n[Merge]\\n[Retrieve]\\n[Append] & [Move]\\n[Merge] & [Move]\\n[Retrieve] & [Move]\\nLC-Boost\\nRandom\\nBrute-force\\nSamSUM\\nFigure 3: Performance comparison on different context processing strategies in the ablation study.\\nNarrativeQA (left) is a single-doc QA task. HotpotQA (middle) is a multi-doc QA task. Sam-\\nSUM (right) is a few-shot learning task.\\nTable 2: Case study on the self-constructed dataset. Correct answers are marked in teal, incorrect\\nanswers in red, and ambiguous answers in orange.\\nQuery: How many papers in ACL 2023 only have one author?\\nContext: Full accepted paper list in ACL 2023 main conference. (Context length: 45K)\\nGround-truth target: 8 papers\\nPhi-3-mini-128K: 11 papers GPT-3.5-turbo-16K: 0 papers Claude-3-Haiku-200K: 1 papers (Acc. Score: 0)\\nLC-Boost’s action trajectory: [Task Reasoning] → [Append]→ ··· →[Append]→ [Aggregation]\\nLC-Boost: 8 papers (Acc. Score: 1)\\nQuery: List all people names that are petrified, separated by comma.\\nContext: Full content of Harry Potter and the Chamber of Secrets. (Context length: 122.6K)\\nGround-truth target: Colin Creevey, Justin Finch-Fletchley, Penelope Clearwater, Hermione Granger\\nPhi-3-mini-128K: Hermione Granger, Ginny Weasley, Mrs Norris (F1-Score: 0.29)\\nGPT-3.5-turbo-16K: Colin Creevey, Mrs Norris (F1-Score: 0.33)\\nClaude-3-Haiku-200K: Nick, Hermione, Ron (F1-Score: 0.18)\\nLC-Boost’s action trajectory: [Task Reasoning] → [Move]→ ··· →[Merge]→ [Aggregation]\\nLC-Boost: Colin Creevey, Penelope Clearwater, Hermione Granger, Nick, Mrs Norris (F1-Score: 0.71)\\nFor the first query, LC-Boost performs [Append] or [Move] actions across all short context along\\nwith a rewritten query, \"Extract paper information in the following list that have only one au-\\nthor,\" derived via [Task Reasoning]. After processing all short contexts, LC-Boost employs the\\n[Aggregation] action to compile the final answer. This approach simplifies the task compared to\\ndirectly extracting a numeric answer from the entire long context, mimicking the human process of\\nreading comprehension and thereby producing accurate results.\\nIn the second case, the query necessitates conditional reasoning on each short context. As highlighted\\nin previous research [Liu et al., 2023], reasoning directly from the entire context risks losing crucial\\ninformation, particularly in the middle of the long context. Thus LLMs tend to miss key details such\\nas people’s names. LC-Boost addresses this issue by processing only one short context at a step where\\nit extracts information from arbitrary position of the long text with equal accuracy. Additionally,\\nanswers marked in orange include non-human names (e.g., cat, ghost) that are misconstrued as people,\\nillustrating a common challenge where models fail to differentiate in-depth entity properties.\\n3.5 Context be Short, Energy be Saved!\\nRecently, we have witnessed the remarkable success of LLMs, which are becoming an indispensable\\npart of our daily lives. We believe that in the near future, LLMs will become as ubiquitous as\\nelectricity or gas supply, serving as fundamental infrastructure in human society. At that point, the\\nenergy consumption of LLMs will emerge as a significant environmental concern. Therefore, it\\nis imperative for the research community to focus on reducing the energy consumption associated\\nwith these models. Figure 4 presents an analysis of energy consumption, comparing the brute-force\\n8\\napproach with our LC-Boost method. The y-axis is measured in Joules. The theoretical energy\\nconsumption is estimated for 7B LLMs across varying context lengths. We roughly estimate the\\nenergy consumption using the formula\\n\\x10\\nTotal Float Operation\\n312 TFLOPS\\n\\x11\\n× 400W, assuming the use of an A100\\nGPU with a compute capability of 312 TFLOPS for BFLOAT16 operations and a maximum TDP of\\n400W5. The practical energy consumption is estimated by recording the GPU time and GPU power\\nduring inference with different context lengths. We use a Llama2-7B-128K [Peng et al., 2023] and a\\nLlama2-7B-chat-4K [Touvron et al., 2023a] for the brute-force setting and LC-Boost, respectively.\\n2K 4K 8K 16K 32K 64K 128K\\nContext Length\\n0\\n5000\\n10000\\n15000\\n20000\\n25000\\nTheoretical Energy Consumption (J)\\nEnergy Consumption Analysis\\n0\\n25000\\n50000\\n75000\\n100000\\n125000\\n150000\\n175000\\n200000\\nPractical Energy Consumption (J)\\nTheoretical Brute-force\\nTheoretical LC-Boost\\nPractical Brute-force\\nPractical LC-Boost\\nFigure 4: Energy consumption analysis.\\nFigure 4 clearly indicates that longer context lengths\\nsignificantly increase energy consumption with the\\nbrute-force method, especially evident in practical\\nmeasurements. This difference is primarily due to\\nthe need to distribute sequence activation tensors\\nacross multiple GPUs in practical experiment, with\\ntensor I/O exacerbating inference latency and thereby\\ninflating energy costs. In contrast, our LC-Boost\\nmethod, working with 4K context lengths, shows only\\na mild increase in energy consumption across con-\\ntexts, thereby confirming its energy efficiency while\\nmaintaining comparable or superior performance on\\nlong-context tasks. We also provide an analysis on\\ntoken consumption in Appendix C.\\n4 Related Works\\nDealing with long contexts is a fundamental research problem for LLMs, as many real-world\\napplications involve long-context inputs [Li et al., 2023a, Fu et al., 2024]. The most direct approach\\nto address long-context tasks is to increase the working context size of LLMs [Abdin et al., 2024,\\nAI et al., 2024, Li et al., 2023a, Cai et al., 2024]. A year ago, significant research efforts focused on\\nextending the working context size of LLMs from 4K to 32K [Jiang et al., 2023a, Li et al., 2023b,\\nChen et al., 2023a, Du et al., 2022]. Currently, many popular open-source and close-source LLMs still\\noperate with a context size under 32K [Touvron et al., 2023a, OpenAI, 2023], such as GPT-3.5-turbo,\\nwhich has a 16K context length. Recently, research has shifted towards extending LLMs’ working\\ncontext to the million-level. Notably, GPT-4 was updated to a 128K context length not long ago, and\\nthe newly released GPT-4o also operates with a 128K context. Moreover, several recent open-source\\nLLMs have been introduced with context lengths exceeding 100K, for example, the Yi series model\\nsupports up to 200K [AI et al., 2024], and the Phi-3 model operates with 128K [Abdin et al., 2024].\\nInstead of merely increasing the context length, another approach to address long-context tasks\\ninvolves extracting a short surrogate context from the full context. This includes techniques like\\nretrieval-augmented generation (RAG) and context refinement methods [Izacard and Grave, 2021a,\\nGao et al., 2024, Wang et al., 2023, Qian et al., 2024]. However, many of these methods utilize\\ntask-specific strategies to manage the long context. For instance, RAG methods often deploy retrievers\\nto select relevant context chunks as supporting evidence [Izacard and Grave, 2021b, Xu et al., 2023b,\\nJiang et al., 2023b]. Recent studies have criticized the chunking process in RAG for undermining\\nthe semantic coherence of the long context and have proposed chunking-free methods to refine the\\nlong context into a concise surrogate context [Qian et al., 2024, Luo et al., 2024]. Furthermore, some\\nstudies have also explored sequential processing strategies, such as Ratner et al. [2022] and Xu et al.\\n[2023a], to sequentially process the context in a manner that preserves its integrity.\\nLastly, reasoning-based methods also show significant potential for addressing long context tasks\\n[Nakano et al., 2022, Yang et al., 2023, Driess et al., 2023]. These methods predominantly employ\\na decision-making process to navigate through the long context sequentially, utilizing reasoning\\ntechniques such as in-context learning [Dong et al., 2022], chain-of-thought [Wei et al., 2022], and\\nself-reflection [Shinn et al., 2023]. In this paper, LC-Boost incorporates a decision-making process\\nthat dynamically customizes the action trajectory for each query, thereby offering considerable\\nflexibility in accessing and leveraging information to produce the final output answer.\\n5The calculation of total float operations is based on the method outlined in https://www.harmdevries.\\ncom/post/context-length/\\n9\\n5 Conclusion\\nIn this paper, we argue that most long-context tasks are short-context solvable, and we validate this\\nclaim through both theoretical and empirical analysis. We propose a method called LC-Boost to\\nsolve long-context tasks by decomposing the long context into short contexts and processing them\\nusing a decision-making process. We conduct experiments on 12 datasets to compare LC-Boost with\\nlong LLMs and other baseline models. Empirical results verify LC-Boost’s effectiveness in solving\\nlong-context tasks. Additionally, we discuss the energy consumption of LC-Boost versus long LLMs,\\ndemonstrating that LC-Boost can achieve comparable performance with significantly less energy\\nconsumption. In Appendix D, we also discuss the limitations and broader impact of this paper.\\n10\\nReferences\\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\\nLiu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context\\nunderstanding. arXiv preprint arXiv:2308.14508, 2023.\\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han,\\nZhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. ∞bench: Extending long context\\nevaluation beyond 100k tokens, 2024a.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023a.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models,\\n2020.\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\\nPercy Liang. Lost in the middle: How language models use long contexts, 2023.\\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica,\\nXuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In\\nNeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023a.\\nRalph Adolphs. Social cognition and the human brain. Trends in cognitive sciences, 3(12):469–479,\\n1999.\\nRandal E Bryant and David Richard O’Hallaron. Computer systems: a programmer’s perspective.\\nPrentice Hall, 2011.\\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian,\\nEvelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets Long Context\\nLarge Language Models. arXiv, 2023a. doi: 10.48550/arxiv.2310.03025. Experimental.\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM\\nComputing Surveys, 55(12):1–38, 2023.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Na-\\nman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian\\nRiedel, and Douwe Kiela. Retrieval-Augmented Generation for knowledge-intensive NLP\\ntasks. In Advances in Neural Information Processing Systems , volume 33, pages 9459–\\n9474, 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/\\n6b493230205f780e1bc26945df7481e5-Paper.pdf.\\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation\\nreduces hallucination in conversation. InFindings of the Association for Computational Linguistics:\\nEMNLP 2021, pages 3784–3803, Punta Cana, Dominican Republic, November 2021. Association\\nfor Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.320. URL https://\\naclanthology.org/2021.findings-emnlp.320.\\nThomas M Cover. Elements of information theory. John Wiley & Sons, 1999.\\nNaftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle, 2015.\\nMehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT\\npress, 2018.\\nTomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis,\\nand Edward Grefenstette. The narrativeqa reading comprehension challenge, 2017.\\n11\\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. A dataset of\\ninformation-seeking questions and answers anchored in research papers. In Proceedings of the\\n2021 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, pages 4599–4610, 2021.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov,\\nand Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\\nanswering, 2018.\\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-\\nhop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel,\\nand Chengqing Zong, editors, Proceedings of the 28th International Conference on Compu-\\ntational Linguistics , pages 6609–6625, Barcelona, Spain (Online), December 2020. Interna-\\ntional Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580. URL\\nhttps://aclanthology.org/2020.coling-main.580.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop\\nquestions via single-hop question composition. Transactions of the Association for Computational\\nLinguistics, 10:539–554, 2022.\\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for\\nlong document summarization. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek\\nHakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou,\\neditors, Proceedings of the 2021 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies, pages 1419–1436, Online, June\\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.112. URL\\nhttps://aclanthology.org/2021.naacl-main.112.\\nAlexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R. Radev. Multi-news: a\\nlarge-scale multi-document summarization dataset and abstractive hierarchical model, 2019.\\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: A human-\\nannotated dialogue dataset for abstractive summarization. In Lu Wang, Jackie Chi Kit Cheung,\\nGiuseppe Carenini, and Fei Liu, editors, Proceedings of the 2nd Workshop on New Frontiers in\\nSummarization, pages 70–79, Hong Kong, China, November 2019. Association for Computational\\nLinguistics. doi: 10.18653/v1/D19-5409. URL https://aclanthology.org/D19-5409.\\nDaya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. Longcoder: A long-range\\npre-trained language model for code completion, 2023.\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\\n//lmsys.org/blog/2023-03-30-vicuna/ .\\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica,\\nXuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?,\\nJune 2023b. URL https://lmsys.org/blog/2023-06-29-longchat .\\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\\nDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.\\nMistral 7b. arXiv preprint arXiv:2310.06825, 2023a.\\nPeitian Zhang, Ninglu Shao, Zheng Liu, Shitao Xiao, Hongjin Qian, Qiwei Ye, and Zhicheng Dou.\\nExtending llama-3’s context ten-fold overnight, 2024b.\\nMarah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany\\nAwadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha\\nBilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu\\nChen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon,\\nRonen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider,\\nJunheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos\\nKarampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee,\\n12\\nYuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik\\nModi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid\\nPryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli\\nSaarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma,\\nXia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael\\nWyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong\\nZhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and\\nXiren Zhou. Phi-3 technical report: A highly capable language model locally on your phone, 2024.\\n01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,\\nJiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin\\nYang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu,\\nPengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and\\nZonghong Dai. Yi: Open foundation models by 01.ai, 2024.\\nDeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model,\\n2024.\\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context win-\\ndow extension of large language models. In The Twelfth International Conference on Learning\\nRepresentations, 2023.\\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng.\\nData engineering for scaling language models to 128k context, 2024.\\nZheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen,\\nZhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024.\\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Lon-\\nglora: Efficient fine-tuning of long-context large language models. In The Twelfth International\\nConference on Learning Representations, 2023a.\\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm:\\nGeneral language model pretraining with autoregressive blank infilling. In Proceedings of the 60th\\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\\n320–335, 2022.\\nOpenAI. Gpt-4 technical report. https://cdn.openai.com/papers/gpt-4.pdf, 2023.\\nGautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open\\ndomain question answering, 2021a.\\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu\\nGuo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models:\\nA survey, 2024.\\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. Learning to filter\\ncontext for retrieval-augmented generation, 2023.\\nHongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, and Zhicheng Dou. Grounding language model\\nwith chunking-free in-context retrieval, 2024.\\nGautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question\\nanswering. In International Conference on Learning Representations , 2021b. URL https:\\n//openreview.net/forum?id=NTEz-6wysdb.\\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian,\\nEvelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large\\nlanguage models. In The Twelfth International Conference on Learning Representations, 2023b.\\nZhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\\nJamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint\\narXiv:2305.06983, 2023b. URL https://arxiv.org/pdf/2305.06983.\\n13\\nKun Luo, Zheng Liu, Shitao Xiao, and Kang Liu. Bge landmark embedding: A chunking-free\\nembedding method for retrieval augmented long-context large language models, 2024.\\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel Context Windows Improve\\nIn-Context Learning of Large Language Models. arXiv, 2022. doi: 10.48550/arxiv.2212.10947.\\nWindow.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\\nGretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:\\nBrowser-assisted question-answering with human feedback, 2022.\\nHui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and\\nadditional opinions. arXiv preprint arXiv:2306.02224, 2023.\\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal\\nlanguage model. arXiv preprint arXiv:2303.03378, 2023.\\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\\nZhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V\\nLe, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models.\\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in\\nNeural Information Processing Systems, 2022. URL https://openreview.net/forum?id=\\n_VjQlMeSB_J.\\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic\\nmemory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding:\\nMulti-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge\\ndistillation, 2023b.\\n14\\nTable 3: Main experiment results. The best results are in bold and the secondary results are marked\\nwith underline. We report the average scores (%) on all tasks.\\nModel Narrative Qasper MultiField Hotpot MuSiQue 2Wiki\\nShort LLMs (Context Length< 32K)\\nLlama2-7B-Chat-4K 18.7 19.2 36.8 25.4 9.4 32.8\\nLlama3-8B-Instruct-8K 21.5 43.0 47.5 47.3 23.3 37.5\\nVicuna-v1.5-7B-16K 19.4 26.1 38.5 25.3 9.8 20.8\\nLong LLMs (Context Length≥ 32K)\\nLongChat-v1.5-7B-32K 16.9 27.7 41.4 31.5 9.7 20.6\\nMistral-7B-Instruct-v0.2-32K 21.6 29.2 47.9 37.7 18.6 21.8\\nLlama3-8B-80K 28.8 47.4 54.5 55.8 27.4 46.0\\nPhi-3-mini-128K 21.0 39.4 51.5 48.1 28.2 38.1\\nYi-9B-200K 15.6 39.3 33.8 51.4 26.6 38.2\\nClosed-Source LLMs\\nDeepSeek-v2 (32K) 18.3 45.7 48.9 57.7 22.6 66.9\\nClaude-3-Haiku (200K) 30.2 44.0 51.5 51.5 32.5 52.1\\nGPT-3.5-turbo-16K 23.6 43.3 52.3 51.6 26.9 37.7\\nLC-Boost (4K) 30.6 50.6 62.1 63.5 42.5 63.1\\nModel GovReport MultiNews SAMSum LCC PCount Self\\nShort LLMs (Context Length< 32K)\\nLlama2-7B-Chat-4K 27.3 25.8 40.7 52.4 2.1 10.5\\nLlama3-8B-Instruct-8K 30.1 27.6 42.7 57.5 8.0 21.9\\nVicuna-v1.5-7B-16K 27.9 27.2 40.8 51.0 6.5 11.3\\nLong LLMs (Context Length≥ 32K)\\nLongChat-v1.5-7B-32K 30.8 26.4 34.2 53.0 1.0 12.5\\nMistral-7B-Instruct-v0.2-32K 31.7 26.9 43.0 55.4 2.6 25.4\\nLlama3-8B-80K 32.3 28.1 42.9 53.6 3.5 35.7\\nPhi-3-mini-128K 32.6 24.9 36.0 60.1 3.2 36.5\\nYi-9B-200K 30.3 26.5 14.6 72.0 4.2 8.7\\nClosed-Source LLMs\\nDeepSeek-v2 (32K) 35.2 26.3 39.3 37.0 12.7 16.2\\nClaude-3-Haiku (200K) 34.1 26.1 7.2 16.9 5.0 46.0\\nGPT-3.5-turbo-16K 29.5 26.7 41.7 54.7 4.5 32.9\\nLC-Boost (4K) 34.4 29.2 44.1 59.0 7.2 47.7\\nA More details of the Datasets\\nWe evaluated all models on 12 datasets, as shown in Table 4. Most of these datasets are provided by\\nthe LongBench benchmark [Bai et al., 2023]. Following LongBench, we used F1-score, accuracy,\\nand edit similarity as the evaluation metrics. Additionally, we manually annotated a self-constructed\\ndataset comprising long contexts from practical scenarios, such as the full schedule of the Olympic\\nGames and the complete list of accepted papers at ACL. The queries in the self-constructed dataset\\ninvolve reasoning over the entire long context. For example, “Who has the most accepted papers at\\nACL 2023?” These queries require the model to accurately understand the long context and perform\\nreasoning, making them highly challenging. The details of the self-constructed dataset are in Table 5.\\nB Implementation Details\\nLC-Boost begins with the [Task Understanding] action after receiving the input query and\\ncontext, using the prompt shown in Table 6. If the task does not include an input query, the two\\ncolumns \"Below is the query\" and \"{input_query}\" are omitted. Besides, for the synthetic task, we\\nuse the prompt shown in Table 7 to reformulate the query for better adaptation to LC-Boost. Based on\\n15\\nTable 4: Statistical information of the datasets utilized in this paper.\\nDataset Narrative Qasper MultiField Hotpot MuSiQue 2Wiki\\nNum of Samples 200 200 150 200 200 200\\nAve. Length 18,409 3,619 4,559 9,151 11,214 4,887\\nMetric F1 F1 F1 F1 F1 F1\\nDataset GovReport MultiNews SAMSum PCount LCC Self\\nNum of Samples 200 200 200 200 500 32\\nAve. Length 8,734 2,113 6,258 11,141 1,235 39,420\\nMetric Rouge-L Rouge-L Rouge-L Accuracy Edit Sim F1&Accuracy\\nTable 5: Data details of the self-constructed dataset.\\nSource Length # Queries Example Query\\nAccepted paper list of ACL\\n2023 Main Conference\\n44,490 7 Who has the most accepted paper in\\nACL 2023?\\nThe Diamond Sutra 19,993 3 How many chapters of the Sutra?\\nSchedule of The 2024\\nOlympic Games\\n15,844 9 Which day has the most gold medal\\nevents?\\nSubtitle of The Big Bang\\nTheory S3E14\\n11,136 6 How long does this episode?\\nThe Little Prince 22,471 4 How many planets does the little prince\\nvisit?\\nHarry Potter and the Cham-\\nber of Secrets\\n122,591 3 How many times has the chamber of se-\\ncret been opened?\\nthe output of the [Task Understanding] action, LC-Boost adopts different strategies to perform\\nthe task. Specifically, “option [1]” directs LC-Boost to utilize a retriever to rank all chunks of the\\nlong context. In this paper, we employ BGE-Reranker-Large as the retriever Chen et al. [2023b].\\nFor “option [2]” and “option [3]”, LC-Boost uses the prompts shown in Table 10 and Table 8 to\\nsequentially process each short context, respectively. After processing each short context, if the\\noutput is not \"null\", the newly summarized context is added to the \"previous summarization\".\\nOnce all short contexts are processed, LC-Boost aggregates all relevant information to produce the\\nfinal answer. At this stage, we use the prompt provided by LongBench, replacing the full context\\nwith the surrogate context produced by LC-Boost. For “option [4]”, LC-Boost utilizes the prompts\\nprovided by LongBench to process each short context and produces the answer as soon as the\\nproper information is found. Table 9 presents an example prompt from LongBench, designed for\\nMultiFieldQA tasks. We modified the prompt by adding the instruction “If no answer can be found in\\nthe text, please output \"null\"”. This allows LC-Boost to skip irrelevant short contexts, performing the\\n[Move] action. Specifically, for the Code Completion task, LC-Boost reversely browses the context\\ncode from near to far as the near context are more useful to predict the code completion. We evaluate\\nall baseline models following the settings provided in LongBench 6. We use a node with 8 A100 80G\\nGPUs to conduct all experiments.\\nC Token Consumption Analysis\\nIn Section 3.5, our analysis confirms that LC-Boost significantly reduces energy consumption\\ncompared to long LLMs. However, most closed-source LLMs, such as the underlying model of\\nLC-Boost, GPT-3.5-turbo, charge based on token consumption, e.g., US$0.50 per 1M tokens for\\ninput and US$1.50 per 1M tokens for output7. Consequently, it is crucial to examine whether the\\n6https://github.com/THUDM/LongBench\\n7https://openai.com/api/pricing/\\n16\\nTable 6: Prompt Template for the [Task Understanding] action.\\nYou need to process a task with a long context that greatly exceeds your context limit.\\nThe only feasible way to handle this is by processing the long context chunk by chunk.\\nBelow is the original task prompt:\\n{task_prompt}\\nBelow is the query:\\n{input_query}\\nYou have the following options to process the long context. Choose one of them:\\n[1]. Retrieve the chunk most relevant to the input query to support answer generation.\\n[2]. Summarize each chunk and then aggregate the summaries after processing all chunks.\\n[3]. Extract key sentences from each chunk and then aggregate them after processing all chunks.\\n[4]. Sequentially scan chunks and produce the answer as soon as the query can be answered.\\nBelow are some examples for reference:\\nThe examples begin as follows:\\n{examples}\\nThe examples conclude here.\\nPlease learn the examples and select one of the options by only outputting the corresponding index number.\\nTable 7: Query Rewritten Prompt Template for the [Task Understanding] action.\\nYou need to process a task with a long context that greatly exceeds your context limit.\\nThe only feasible way to handle this is by processing the long context chunk by chunk.\\nBelow is the original task prompt:\\n{task_prompt}\\nBelow is the query:\\n{input_query}\\nYou will process the long context with the following strategy:\\n{strategy}\\nDo you think the the query is proper for processing context chunk? If not, rewrite the query.\\nBelow are some examples for reference:\\nThe examples begin as follows:\\n{examples}\\nThe examples conclude here.\\nPlease study the examples carefully. If the query needs to be rewritten, directly output the revised query.\\nIf no revision is necessary, output “null”.\\ndecision-making process of LC-Boost increases token consumption compared to the brute-force\\nmethod.\\nTo address this issue, we recorded the end-to-end token consumption for three datasets: NarrativeQA,\\nGovReport, and LCC. After token counting, we conclude that LC-Boost’s token consumption was\\n34.1% of the brute-force method’s consumption in NarrativeQA, 112% in GovReport, and 29.5% in\\nLCC. These results indicate that LC-Boost’s token consumption varies significantly across different\\ntasks. For tasks requiring precise context location, such as QA and code completion, LC-Boost\\ncan respond as soon as the relevant context is identified, thereby avoiding the need to process the\\nfull context. However, for tasks that necessitate information aggregation, such as summarization,\\nLC-Boost may require more tokens for prompts in each iteration. In practice, for token-consumption-\\nsensitive LLMs, there might be a trade-off between performance and cost-efficiency, which also\\nvaries considerably across different tasks.\\nD Limitations and Broad Impact\\nIn this paper, we propose LC-Boost, a method dedicated to solving long-context tasks using short\\ncontexts. However, there are several limitations we would like to address in the future work: (1)\\nAlthough we conduct comprehensive experiments on many tasks and provide theoretical analysis to\\nsupport our major claim that most long-context tasks are short-context solvable, there may be more\\ncomplicated scenarios that require understanding the full context in a brute-force setting. LC-Boost\\nmight not be able to process such tasks effectively. (2) As mentioned in Section 2.3, LC-Boost selects\\nactions from a discrete action space. While we argue that the pre-defined action space is versatile\\n17\\nTable 8: Prompt Template for the [Append] action.\\nYou are given an article and a question. Read the article carefully and follow my instructions to process it.\\nArticle:\\nThe article begins as follows:\\n{article}\\nThe article concludes here.\\nQuestion:\\n{question}\\nInstructions:\\nEach sentence in the article is marked with a sentence identifier [si], for example [s1].\\nSelect up to ten key sentences from the article that are most likely to answer the question.\\nOnly output the selected sentence identifiers, separated by commas.\\nExample: [s39],[s54]\\nIf no sentences are relevant, please output \"null\".\\nTable 9: Prompt Template for the MultiFieldQA Task from the LongBench Benchmark. Additions\\nmade by us are highlighted in blue.\\nRead the following text and answer briefly.\\n{context}\\nNow, answer the following question based on the above text, only give me the answer and do not output any\\nother words. If no answer can be found in the text, please output \"null\".\\nQuestion:{question}\\nAnswer:\\nTable 10: Prompt Template for the [Merge] action.\\nYou are provided with a portion of an article, a question, and summarization of the article’s previous portions.\\nRead the article portion and follow my instructions to process it.\\nArticle:\\nThe article begins as follows:\\n{article}\\nThe article concludes here.\\nPrevious summarization:\\nThe previous summarization is as follows:\\n{previous_sum}\\nThe previous summarization concludes here.\\nQuestion:\\n{question}\\nInstruction:\\nSummarize the partial article to supplement the previous summarization, which can better support the task.\\nIf no content needs to be supplemented, please output \"null\".\\nenough to handle most scenarios, a more elegant solution would be to predict actions in a continuous\\nspace. We conducted preliminary experiments to explore allowing LC-Boost to prompt itself to\\npredict actions without a predefined action space, such as writing prompts or code autonomously.\\nThese experiments resulted in highly unstable performance, particularly for models like GPT-3.5,\\nas such requirements are still challenging. We believe that with a much stronger foundation model,\\nLC-Boost could be expected to predict actions in a continuous space. (3) We choose GPT-3.5 as\\nthe foundation model for LC-Boost, instead of open-source LLMs. The reason is that GPT-3.5 is a\\nstrong, yet efficient model that can generally understand most instructions. However, we found that\\nmost open-source LLMs lack these properties in a zero-shot setting. Fine-tuning these open-source\\nLLMs might be helpful, but constructing such instruction data is infeasible and expensive.\\nAs discussed in Section 3.5, LLMs are likely to become a fundamental infrastructure in the near\\nfuture. At that scale, their energy consumption will pose significant environmental challenges. As\\nshown in Figure 4, LC-Boost avoids processing long contexts directly by decomposing them into\\nshorter contexts. This approach significantly reduces energy consumption as the context length\\nincreases, leading to substantial positive environmental impacts. We believe that in the future, more\\nresearch will focus on green AI initiatives. This paper could serve as an initial spark to inspire further\\nresearch in this direction, potentially resulting in broader social impact.\\n18\\n\\n===== Document: paper2.txt =====\\n\\nPublished as a conference paper at ICLR 2025\\nLONG PO: L ONG CONTEXT SELF -EVOLUTION OF\\nLARGE LANGUAGE MODELS THROUGH SHORT-TO-\\nLONG PREFERENCE OPTIMIZATION\\nGuanzheng Chen1,2,3,∗ Xin Li2,3,† Michael Qizhe Shieh1 Lidong Bing4\\n1National University of Singapore 2DAMO Academy, Alibaba Group\\n3Hupan Lab, 310023, Hangzhou, China\\n4Shanda AI Research Institute\\ngc.chen@u.nus.edu, xinting.lx@alibaba-inc.com\\nmichaelshieh@comp.nus.edu.sg, lidong.bing@shanda.com\\nABSTRACT\\nLarge Language Models (LLMs) have demonstrated remarkable capabilities\\nthrough pretraining and alignment. However, superior short-context LLMs may\\nunderperform in long-context scenarios due to insufficient long-context alignment.\\nThis alignment process remains challenging due to the impracticality of human\\nannotation for extended contexts and the difficulty in balancing short- and long-\\ncontext performance. To address these challenges, we introduce LongPO, that\\nenables short-context LLMs to self-evolve to excel on long-context tasks by in-\\nternally transferring short-context capabilities. LongPO harnesses LLMs to learn\\nfrom self-generated short-to-long preference data, comprising paired responses\\ngenerated for identical instructions with long-context inputs and their compressed\\nshort-context counterparts, respectively. This preference reveals capabilities and\\npotentials of LLMs cultivated during short-context alignment that may be dimin-\\nished in under-aligned long-context scenarios. Additionally, LongPO incorporates\\na short-to-long KL constraint to mitigate short-context performance decline during\\nlong-context alignment. When applied to Mistral-7B-Instruct-v0.2 from 128K to\\n512K context lengths, LongPO fully retains short-context performance and largely\\noutperforms naive SFT and DPO in both long- and short-context tasks. Specifi-\\ncally, LongPO-trained models can achieve results on long-context benchmarks\\ncomparable to, or even surpassing, those of superior LLMs (e.g., GPT-4-128K)\\nthat involve extensive long-context annotation and larger parameter scales. Our\\ncode is available at https://github.com/DAMO-NLP-SG/LongPO.\\n1 I NTRODUCTION\\nRecent advancements in Large Language Models (LLMs) have revealed remarkable capabilities\\nthrough extensive pretraining and subsequent alignment with human intentions. The alignment pro-\\ncess, including methods such as Supervised Fine-Tuning (SFT) (Wei et al., 2022), Direct Preference\\nOptimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback\\n(RLHF) (Christiano et al., 2017; Ouyang et al., 2022; Stiennon et al., 2020), has effectively un-\\nleashed the potential of LLMs acquired during pretraining to achieve desired behaviors.\\nAlthough off-the-shelf alignment methods have made significant strides in short-context settings,\\ntheir application to long-context situations remains challenging (Bai et al., 2024). First, the scarcity\\nof high-quality, long-context annotated data poses a significant hurdle. Human annotation becomes\\nimpractical and less-reliable as context length increases (Dubey et al., 2024), while synthetic data\\ngeneration using advanced LLMs lacks scalability and remains resource-intensive. Moreover, sim-\\nply concatenating existing short-context datasets has been shown to yield unsatisfactory long-context\\nperformance (Liu et al., 2024b). Second, long-context alignment methods grapple with the balance\\n∗This work was done during the internship of Guanzheng Chen at Alibaba DAMO Academy.\\n†Corresponding Author.\\n1\\narXiv:2502.13922v3  [cs.CL]  1 Mar 2025\\nPublished as a conference paper at ICLR 2025\\n60 65 70 75 80 85\\nShort-Context Performance\\n15\\n20\\n25\\n30\\n35\\n40Long-Context Performance\\nGPT-4-128K\\nLLaMA3.1-8B-Instruct-128K\\nGLM-4-9B-Chat-128K\\nMistral-7B-LongPO-128K\\nMistral-7B-Instruct-v0.2\\nGLM-4-9B-Chat-1M\\n+25.45\\nFigure 1: The comparison of long-context ( ∞Bench) and short-context (MMLU) performance\\namong GPT-4-128K and smaller LLMs.\\nbetween preserving short-context proficiency and cultivating long-context capabilities (Liu et al.,\\n2024b). For instance, the LLaMA-3.1 series incorporate merely 0.1% long-context data with over\\n99% short-context data during alignment to maintain the short-context performance (Liu et al.,\\n2024b). This limited exposure to natural long-context data may result in insufficient alignment,\\npotentially blocking the intrinsic long-context capabilities in LLMs.\\nThe challenges of long-context alignment suggest that the full potential of LLMs may remain un-\\ntapped for long-context tasks. As illustrated in Figure 1, even superior models such as GPT-4,\\nwhich excel in short-context tasks, unexpectedly underperform in long-context scenarios. Interest-\\ningly, despite the much stronger short-context capabilities, GPT-4 is still inferior to LLaMA3.1-8B\\non long-context tasks. This disparity underscores the need for more effective long-context alignment\\nmethods to fully unleash the intrinsic power of LLMs across variable context lengths.\\nIn this work, we posit that the capabilities, deeply ingrained during short-context pretraining and\\nalignment, can be effectively transferred to longer contexts without external guidance. To this end,\\nwe introduce Short-to- Long Preference Optimization (LongPO), to steer long-context alignment\\nby injecting internal short-context preferences into long-context scenarios. Specifically, we propose\\nto construct the preference data pairs by prompting the short-context LLM (e.g., Mistral-Instruct)\\nwith two inputs: (1) a long input comprising an instruction over a long document and, (2) a short\\ninput with the identical instruction over the relevant shortened chunk within the same document.\\nWe then designate the responses to short and long inputs as chosen and rejected responses, respec-\\ntively. The short-to-long preference, i.e., the discrepancies between each paired response, reveal\\nthe capabilities and potentials cultivated during short-context alignment that may be diminished in\\nunder-aligned long-context scenarios. In order to bring forward the established capabilities, LongPO\\nis utilized to optimize the model towards short-to-long preferences using DPO-style objectives upon\\nlong contexts. Furthermore, to maintain the short-context performance, we incorporate a short-to-\\nlong constraint in LongPO by applying Kullback-Leibler (KL) divergence between the response\\ndistributions to short and long inputs, respectively. This constraint, inspired by the KL constraint in\\nRLHF (Ouyang et al., 2022; Stiennon et al., 2020), guides the policy model to minimize the deviation\\nfrom its short-context output distribution when giving the long context during training. We found\\nthat this straightforward constraint largely enhances the retention of short-context performance after\\nthe long-context alignment.\\nWe apply LongPO to Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) and Qwen2.5-7B-Instruct, while\\niteratively extending their context lengths up to 512K, with the self-generated short-to-long pref-\\nerence data only. The experimental results demonstrate that LongPO, as a long-context alignment\\nmethod, surpasses naive SFT and DPO by large margins (over 10 points) in both long- and short-\\ncontext tasks. Notably, LongPO fully retains the performance of short-context LLMs after long-\\ncontext alignment, whereas SFT and DPO yield substantial performance degradation (10∼20 points\\non most tasks). In terms of long-context performance, LongPO largely improves the Mistral-7B-\\nInstruct-v0.2 by 25.45 points on ∞Bench. Specifically, as depicted in Figure 1, the resulting model\\n2\\nPublished as a conference paper at ICLR 2025\\nis comparable with superior long-context LLMs at various scales (e.g., Mistral-7B-LongPO-128K of\\n39.27 vs. GPT-4-128K of 34.81 on ∞Bench), despite the latter often involving extensive continual\\ntraining on hundreds of billions of tokens (Dubey et al., 2024) or labor-intensive long-context data\\nannotation (Zeng et al., 2024). These findings underscore the efficacy of our proposed method in\\naddressing the challenges of long-context alignment while simultaneously preserving short-context\\ncapabilities, offering a more efficient and balanced approach to the development of long-context\\nLLMs.\\n2 P RELIMINARIES\\nIn this section, we introduce two key methods for aligning language models with human preferences:\\nReinforcement Learning from Human Feedback (RLHF, §2.1) and Direct Preference Optimization\\n(DPO, §2.2).\\n2.1 RLHF\\nReinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Stiennon et al., 2020)\\naims to optimize the policy model πθ to maximize rewards while maintaining proximity to a refer-\\nence policy πref. Formally, the objective is\\nmax\\nπθ\\nEx∼D,y∼πθ(y∣x)[rϕ(x, y)]−βDKL[πθ(y ∣ x) ∣∣πref(y ∣ x)], (1)\\nwhere rϕ is the reward model that has been trained on ranked responses to reflect human prefer-\\nence, β is a hyper-parameter controlling the deviation from reference policy, and DKL denotes the\\nKullback-Leibler divergence. Typically, bothπθ and πref are initialized with identical model.\\n2.2 DPO\\nConsidering the instability and difficulty of RLHF training, DPO (Rafailov et al., 2023) offers an\\nalternative approach by reparameterizing the reward functionr that incorporates the optimal policy:\\nr(x, y) =β log πθ(y ∣ x)\\nπref(y ∣ x) +β log Z(x), (2)\\nwhere Z(x) is the partition function. DPO assumes access to preference data D, which consists\\nof paired responses (yw, yl) to an instruction x. Specifically, the yw and yl represent the preferred\\n(winning) and dispreferred (losing) responses, respectively, based on human preference. Inspired by\\nthe Bradley-Terry (BT) theory that models the preference distribution p∗ by\\np∗(yw ≻yl ∣ x) =σ(r(x, yw)−r(x, yl)), (3)\\nwhere σ is the sigmoid function. DPO derives the preference optimization objective for the policy\\nmodel πθ as\\nLDPO(πθ; πref) =−E(x,yw,yl)∼D [σ(rθ(x, yw)−rθ(x, yl))]\\n=−E(x,yw,yl)∼D [log σ (β log πθ(yw ∣ x)\\nπref(yw ∣ x) −β log πθ(yl ∣ x)\\nπref(yl ∣ x))]. (4)\\n3 L ONG PO: S HORT-TO-LONG PREFERENCE OPTIMIZATION\\nMotivated by the challenges of data annotation and performance balance during long-context align-\\nment, we introduce the Short-to-Long Preference Optimization (LongPO), to effectively empowers\\na short-context LLM self-evolve to a long-context counterpart while preserving its original short-\\ncontext capabilities. The foundation of LongPO lies in the transfer of capabilities deeply ingrained\\nduring short-context alignment to long-context scenarios by learning from short-to-long preference\\n(§3.1). Additionally, LongPO incorporates a short-to-long constraint based on the KL divergence\\nbetween short- and long-context models during training, to maintain the short-context performance\\nin a simple yet effective way (§3.2). In §3.3, we present the details of curating short-to-long prefer-\\nence data without external guidance and self-evolving long context training process using LongPO.\\n3\\nPublished as a conference paper at ICLR 2025\\n3.1 L EARNING FROM SHORT-TO-LONG PREFERENCE\\nAs outlined in §2, aligning LLMs with human preference typically relies on datasets comprising\\nranked responses to identical prompts or instructions. However, in long-context scenarios, con-\\nstructing such datasets becomes impractical due to the extensive effort required for annotation. To\\ncircumvent the external data annotation, we leverage theshort-to-long preferenceto internally trans-\\nfer capabilities well-established in the short-context alignment of LLMs to long-context counterpart.\\nConcretely, we assume access solely to a short-context LLM πS that has been well aligned. Given\\na long input xL = [CL; IL] where CL is the long context and IL is the instruction, we can acquire\\nthe response yL ∼πS(y ∣ xL) by conditioning on the entire context. Due to the limitations of πS in\\nhandling long contexts, yL is likely to be of lower quality.\\nWe then hypothesize an ideal extractor F that can rigorously identify and extract all essential infor-\\nmation CS within CL relevant to addressing IL:\\nCS =F(CL, IL). (5)\\nBy querying the instruction IL based on CS, we obtain a new answer yS ∼ πS(y ∣ xS), where\\nxS = [CS; IL]. As CS is a shortened context for IL, the well-aligned short-context model πS should\\nbe capable of producing a high-quality answer that aligns with human preferences.\\nIntuitively, yS can serve as a high-quality answer even when giving the whole long context, as\\nits conditioned context is self-contained for instruction IL. Hence, we definite the short-to-long\\npreference distribution pSL based on Bradley-Terry (BT) model following Eq. (3):\\npSL(yS ≻yL ∣ xL) =σ(r(xL, yS)−r(xL, yL)). (6)\\nWe now steer a policy model πθ (initialized with πS) to follow the preference distribution pSL,\\nforming the LongPO objective:\\nLLongPO(πθ; πref) =−E(xS,xL,yS,yL)∼DSL [σ(rθ(xL, yS)−rθ(xL, yL))], (7)\\nwhere DSL is the short-to-long preference data consisting of quadruples(xS, xL, yS, yL). This objec-\\ntive encourages the policy model to consistently accommodate the well-aligned short-context pref-\\nerence while deviating the under-aligned long-context preference. Therefore, LongPO internally\\ntransfers preferences from short to long contexts without requiring external supervision, effectively\\naddressing the challenge of long-context data annotation.\\n3.2 S HORT-TO-LONG CONSTRAINT\\nLong-context alignment often leads to an imbalance between long- and short-context performance.\\nWhile this issue can be mitigated by carefully calibrating the scale and mixing proportion of long\\nand short data across various context lengths, such an approach is resource-intensive and time-\\nconsuming. Moreover, an excessive incorporation of short-context data may inadvertently lead to\\ninsufficient long-context alignment. In LongPO, we recognize that the degradation in short-context\\nperformance during long-context alignment may be attributed to an improper (or missing) constraint\\nin current alignment methods.\\nSpecifically, the RLHF and DPO objectives (implicitly) include a KL divergence term,βDKL[πθ(y ∣\\nx) ∣∣ πref(y ∣ x)], which serves as a constraint to prevent excessive deviation from the reference\\nmodel in Eq. (1). For a long input xL, this constraint C is expressed as:\\nC =βDKL[πθ(y ∣ xL) ∣∣πref(y ∣ xL)]. (8)\\nHowever, the reference model is typically the short-context model πS itself, which is not adept at\\nhandling long contexts. This results in a problematic reference distribution πref(y ∣ xL), leading to\\nundesired deviation from the short-context model distribution.\\nTo address this issue, we propose a short-to-long constraint leveraging the quadruples introduced\\nin Eq. (7). Recall that xS contains all the essential information from xL required to generate a\\nsatisfactory response, πS can serve as a proficient reference model conditioned on xS. While for an\\nideal reference model π∗\\nref capable of handling context lengths from short to long, we should have:\\nDKL[π∗\\nref(y ∣ xL) ∣∣π∗\\nref(y ∣ xS)] =DKL[π∗\\nref(y ∣ xS) ∣∣πS(y ∣ xS)] =0, (9)\\n4\\nPublished as a conference paper at ICLR 2025\\nFigure 2: The procedure of generating short-to-long preference data from step 1 to 7.\\nnamely π∗\\nref(y ∣ xL) and πS(y ∣ xS) are identical distribution following Gibbs’ inequality. We\\nhence derive an adjusted short-to-long constraint between short-context reference model and “long-\\ncontext” policy model given contexts of different lengths:\\nC′ =βDKL[πθ(y ∣ xL) ∣∣πS(y ∣ xS)]. (10)\\nThis refined constraint ensures that the policy model πθ operating on long contexts does not deviate\\nsignificantly from the short-context model πS when provided with the essential information. By\\nenforcing this constraint, we aim to preserve the short-context performance during long-context\\nalignment, thereby addressing the imbalance issue in a more principled manner.\\nBy incorporating the short-to-long constraint in Eq. (2), we have a refined reward function for long\\ninput xL (following derivation in Appx. §A.1):\\nrLongPO\\nθ (xL, y) =β log πθ(y ∣ xL)\\nπS (y ∣ xS) +β log Z(xL, xS), (11)\\nwhere xS is extracted from xL as illustrated in Eq. (5). Hence we access the LongPO objective:\\nLLongPO(πθ; πS) =−E(xS,xL,yS,yL)∼DSL [σ(rLongPO\\nθ (xL, yS)−rLongPO\\nθ (xL, yL))]\\n=−E(xS,xL,yS,yL)∼DSL [log σ (β log πθ(yS ∣ xL)\\nπS(yS ∣ xS) −β log πθ(yL ∣ xL)\\nπS(yL ∣ xS))].\\n(12)\\n3.3 S ELF -EVOLVING\\nInitialization. LongPO relies solely on access to a well-aligned short-context LLM, i.e., πS, in\\nconjunction with a long-context plain corpus. Note that the long-context corpus need not be metic-\\nulously crafted, as it can be sampled and extracted from existing pretraining corpora of LLMs.\\nConstruction of short-to-long preference data. The construction of short-to-long preference\\ndata DSL introduced in §3.1 assumes an impractical extractor capable of retrieving essential in-\\nformation from long contexts for each instruction. To satisfy this hypothesis, we reversely prompt\\nπS to generate instructions for shortened chunks within long documents. This ensures that the short\\ncontext information is self-contained for instructions. Concretely, our data construction process\\ninvolves two steps as displayed in Figure 2:\\n1. Instruction Generation. For each long document CL, we randomly sample a shortened\\nchunk CS and prompt the πS to generate an instruction via the Self-Instruct (Wang et al.).\\nTo ensure the diversity of instructions, the model is prompted to generate an instruction\\npool first and then we randomly sample an instruction IL from this pool.\\n2. Response Generation. Using the generated instruction IL, we prompt πS to produce two\\nresponses: a chosen response yS ∼πS(y ∣ xS) based on the short context xS, and a rejected\\nresponse yL ∼πS(y ∣ xL) derived from the long context xL.\\n5\\nPublished as a conference paper at ICLR 2025\\nIterative self-evolving training with LongPO. LongPO employs an iterative process to extend\\nLLM context length. Initially, a short-context LLM πS generates short-to-long preference data for\\ndocuments of length L1. The resulting model after LongPO training, now capable of handling L1\\ncontexts, then serves as the new “short-context LLM” for the next iteration, generating data for an\\nextended length L2. This process repeats, progressively increasing context length capacity.\\nAs there are multiple short chunks CS = {Ci\\nS}n\\ni=1 within a long document CL, we collect the\\ninstruction-response triples (Ii\\nL, yi\\nS, yi\\nL) for each chunk within identical long document, to form a\\nmulti-turn dataset ˆDSL. We then aggregate the probabilities across all turns to produce a multi-turn\\nLongPO objective:\\nLMT\\nLongPO(πθ; πS) =−E(xS,xL,yS,yL)∼ ˆDSL [log σ (β log ∑n\\ni=1 πθ(yi\\nS ∣ xi\\nL)\\n∑n\\ni=1 πS(yi\\nS ∣ Ci\\nS) −β log ∑n\\ni=1 πθ(yi\\nL ∣ xi\\nL)\\n∑n\\ni=1 πS(yi\\nL ∣ Ci\\nS))],\\n(13)\\nwhere xS ={[Ci\\nS; Ii\\nL]}n\\ni=1, xL ={[CL; Ii\\nL]}n\\ni=1, yS ={yi\\nS}n\\ni=1, and yL ={yi\\nL}n\\ni=1. LLMs trained with\\nLongPO do not necessarily involve continual training before, which may lead to instability when\\nprocessing long contexts. To address this issue and stabilize the training process, we incorporate\\na continual training objective following Pang et al. (2024b). Specifically, we add the negative log-\\nlikelihood (NLL) loss over entire long chosen sequencesSL =[xL; {Ii\\nL; yi\\nS}n\\ni=1] to LongPO objective.\\nThus, our final training objective is:\\nLθ =λ ⋅LMT\\nLongPO(πθ; πS)+LNLL(πθ; SL) =λ ⋅LMT\\nLongPO(πθ; πS)+ πθ(SL)\\n∣SL∣ . (14)\\n4 E XPERIMENTAL SETUP\\n4.1 T RAINING SETUP\\nData Curation Details. We curate the short-to-long preference data based on a long-context cor-\\npus sampled from the Book and ArXiv subsets of Long-Data-Collection1, and the GitHub subset of\\nRedPajama (Computer, 2023). For a specific target length (e.g., 128K tokens), we filter the corpus\\nto include only documents that are shorter than this length but longer than 64K tokens. Each long\\ndocument is then segmented into chunks of up to 32K tokens, with a maximum of 4 randomly-\\nsampled chunks retained per document. For instruction generation, we prompt short-context models\\nto generate 4 instructions per document, from which we randomly select one for further use. Af-\\nter filtering undesired (censored and repetitive) responses, we collect 45K, 16K, 2.5K multi-turn\\ninstruction-response samples of 128K, 256K, and 512K tokens for Mistral-7B, and 32K samples of\\n128K tokens for Qwen2.5-7B. More details are listed in Appx. §B.1.\\nTraining Details. We extend the context length of Mistral-7B and Qwen2.5-7B using our LongPO\\non short-to-long preference data specifically generated by models themselves. The training pro-\\ncess begins with utilizing Mistral-7B/Qwen2.5-7B to generate data with a length of 128K and ex-\\ntend the context length to 128K. To investigate the scalability, we utilize the resulting Mistral-7B-\\nLongPO-128K to generate data with lengths of 256K/512K and further extend the context length\\nto 256K/512K. We leverage Deepspeed-Ulysses (Jacobs et al., 2023) for sequence parallelism and\\nemploy Flash Attention (Dao et al., 2022; Dao, 2023) for efficient computation. All models are\\noptimized using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 5e-7. We set the\\nmargin β in Eq. (13) to 0.1 and the weighting factor λ in Eq. (14) to 0.01. RoPE θ is set as 1e7 and\\nthe batch size is set as 8. More details are listed in Appx. §B.3.\\n4.2 E VALUATION BENCHMARKS\\nWe assess both the long- and short-context capabilities of our models against baselines. The long-\\ncontext evaluation utilizes the following benchmarks:\\n• ∞Bench (Zhang et al.). We evaluate all models on three tasks in this benchmark: sum-\\nmarization (En.Sum), long-book question answering (En.QA), and multi-choice question-\\nanswering (En.MC). The evaluation length is beyond 100K.\\n1https://huggingface.co/datasets/togethercomputer/Long-Data-Collections\\n6\\nPublished as a conference paper at ICLR 2025\\n• RULER (Hsieh et al., 2024). This benchmark comprises four types of synthetic tasks\\nacross variable sequence lengths (4K to 128K): Needle-in-a-haystack (NIAH) retrieval,\\nMulti-hop Tracing with Variable Tracking (VT), Aggregation, and Question Answering\\n(QA). We exclude the Aggregation tasks, which involve word frequency counting within\\nthe context, since they present challenges in word counting beyond mere long-context ca-\\npabilities that current LLMs still struggle in.\\n• LongBench-Chat (Bai et al., 2024). This benchmark assesses instruction-following abil-\\nities over long contexts (10K to 100K tokens), employing GPT-4-128K as an impartial\\njudge to evaluate model-generated responses. We filter out the English samples for fair\\ncomparison across different models.\\nFor short-context evaluation, we employ MMLU (Hendrycks et al., 2021), ARC-C (Clark et al.,\\n2018), Hellaswag (Zellers et al., 2019) and Winogrande (Sakaguchi et al., 2019) for assessing the\\ngeneral language understanding and reasoning capabilities, and MT-Bench (Zheng et al., 2023) for\\nassessing instruction-following capability. More details are listed in Appx. §B.2.\\n4.3 B ASELINES\\nWe train our LongPO on Mistral-7B-Instruct-v0.2 (denoted as Mistral-7B) and Qwen2.5-7B-\\nInstruct (denoted as Qwen2.5-7B), comparing them against a range of powerful LLMs including\\nGPT-4-128K, Qwen2-72B-Instruct (Yang et al., 2024), LLaMA-3.1-70B, LLaMA-3.1-8B, GLM-\\n4-9B-Chat, GLM-4-9B-Chat-1M, LWM-Text-Chat-1M (Liu et al., 2024b), and Yarn-Mistral-7b-\\n128k (Peng et al., 2023). Additionally, we establish baselines using Mistral-7B trained with conven-\\ntional SFT and DPO on the same dataset used by LongPO.\\nFor short-context evaluation, we primarily compare the performance of naive LLMs against their\\ncounterparts post-trained with SFT, DPO, and LongPO on our synthetic data. To provide a more\\ncomprehensive comparison, we also include two series of open-source long-context language mod-\\nels: GLM-4-9B-Chat versus GLM-4-9B-Chat-1M, and LWM-Text-Chat-128k versus LWM-Text-\\nChat-1M. This allows us to assess the effectiveness of our LongPO to maintain the short-context\\nperformance during long-context alignment, comparing with baselines utilizing various strategies.\\n5 R ESULTS AND ANALYSES\\nIn this section, we demonstrate the exceptional effectiveness of LongPO through two types of com-\\nparisons: (1) comparison with naive SFT and DPO trained on identical models and datasets; (2)\\ncomparison with SOTA long-context LLMs.\\n5.1 C OMPARISON WITH SFT AND DPO\\nWe first compare LongPO with conventional SFT and DPO using identical LLM (Mistral-7B). All\\nmodels are trained on equivalent self-generated datasets, as detailed in §4.1. Given the inability of\\nSFT to leverage preference data, we apply it to the instructions paired with chosen responses.\\nLongPO exhibits superior performance over SFT and DPO. The experimental results, illus-\\ntrated in Table 1, reveal consistent and substantial performance gains (10 to 20+ points) of LongPO\\nover SFT and DPO across a diverse range of long-context tasks. Crucially, as depicted in Figure 3,\\nLongPO maintains robust short-context performance compared with original short-context LLMs\\n(59.99 vs 59.15 on MMLU), whereas SFT and DPO exhibit notable degradation in short-context\\nscenarios after long-context alignment process.\\nThe performance disparity between LongPO and SFT can be attributed to the explicit integration\\nof short-to-long preference in LongPO, which is either absent or merely implicit in the chosen re-\\nsponses utilized by SFT. While both LongPO and DPO leverage the proposed short-to-long prefer-\\nence data, the pivotal difference lies in the short-to-long constraint introduced in §3.2. The marked\\nperformance gaps between LongPO and DPO, observed across both long- and short-context tasks,\\nhighlight the effectiveness of the proposed constraint for successfully mitigating the problematic\\nlimitations in DPO and retaining the short-context performance during long-context training. More\\nablations are detailed in §5.3.\\n7\\nPublished as a conference paper at ICLR 2025\\nTable 1: Long-Context Performance of our LongPO compared with baselines. Higher is better for\\nall metrics. Results marked with ♭ are evaluated by ourselves, while other results of baselines are\\nsourced from the original benchmarks. Full results on RULER are listed in Table 2.\\nModel Train/Claimed∞Bench RULER LongBench-Length En.Sum En.QA En.MC A VG. NIAH VT QA A VG. Chat (EN)GPT-4-128K 128K 14.73 22.44 67.25 34.81 95.4 99.9 70.3 88.53 8.40Qwen2-72B 128K 24.32♭ 7.03♭72.05♭34.47♭ 88.6 95.7 66.7 83.67 7.72♭LLaMA 3.1-70B 128K 33.55♭36.08♭69.00♭46.21♭ 96.1 93.2 67.8 85.7 6.67♭LLaMA 3.1-8B 128K 28.06♭30.47♭58.08♭38.87♭ 97.93 91.4 64.7 84.68 6.22♭GLM-4-9B 128K 14.84♭ 9.51♭67.25♭30.53♭96.51♭97.3♭64.8♭0 86.20♭ 5.67♭GLM-4-9B-1M 1M 28.3 9.7 68.6 35.53 98.2 99.4 69.4 89.0 5.03♭LWM-7B-1M 1M 4.33♭ 0.0♭ 3.06♭ 2.46♭ 87.20 57.5 56.4 67.03 1.25♭YaRN-Mistral-7B 128K 9.09 9.55 27.95 15.53 63.4 36.1 25.9 41.8 -Mistral-7B 32K 22.13 4.93 14.41 13.82 72.60 74.40 52.2 66.4 4.10- SFT 128K 23.44 13.45 53.21 30.03 88.73 79.64 51.08 73.15 4.25- DPO 128K 15.21 10.34 48.14 25.56 74.25 72.36 50.24 65.62 4.08- LongPO (iter1)128K27.0523.5167.2539.2796.8896.4964.8186.065.42- LongPO (iter2)256K28.1624.4366.3539.6596.8097.064.8786.225.48- LongPO (iter3)512K29.1027.8566.6741.2197.2897.4864.9286.565.80Qwen2.5-7B 128K 22.89 6.08 52.4 27.12 82.1 80.09 54.30 72.16 5.80- LongPO (iter1)128K32.0617.3272.0540.4895.8189.7159.481.645.75\\nMMLU ARC-C Hellaswag Winogrande MT-Bench\\n-10.0\\n-1.0\\n0.0\\n1.0\\nMargin\\n0.84\\n0.08\\n-0.21\\n0.13 0.10\\n0.32\\n1.02\\n-0.06\\n-0.26\\n0.40\\n-25.74\\n-14.80\\n-10.77 -14.11\\n-19.90\\n-14.04\\n-9.73 -9.07 -12.15\\n-22.00\\n-0.42\\n-1.11 -1.12\\n-0.55\\n-0.40\\n-1.03\\n-0.51\\n-0.25\\n-1.02\\n-2.40\\nLongPO (iter1) LongPO (iter2) SFT DPO GLM LWM\\nFigure 3: The margins of the short-context performance of Mistral-7B-LongPO and baselines rela-\\ntive to corresponding base model. GLM and LWM refer to the margins of GLM-9B-1M and LWM-\\n7B-1M over GLM-9B-128K and LWM-7B-128K, respectively. MT-Bench metrics ( ∈[0, 10]) are\\nlinearly scaled to [0, 100] for better comparability across tasks. See numerical results in Table 3.\\n5.2 C OMPARISON WITH SOTA LONG -CONTEXT LLM S\\nTo further substantiate the efficacy of LongPO, we conducted an extensive comparison between our\\nLongPO-trained Mistral-7B and leading long-context LLMs across varying model scales.\\nLongPO demonstrates exceptional competitiveness at similar scale. As detailed in Table 1,\\nLongPO demonstrates formidable competitiveness in terms of models at similar scales. For exam-\\nple, Mistral-7B-LongPO significantly outperforms some established long-context models, including\\nLWM-7B and YaRN-Mistral, across all long-context tasks in ∞Bench and RULER. Remarkably,\\nMistral-7B-LongPO-128K surpasses GLM-4-9B (39.27 vs. 30.53 on ∞Bench and 86.06 vs. 86.20\\non RULER), although the latter is training on manually annotated long-context data spanning up\\nto 128K sequence length. Moreover, GLM-4-9B-1M, an extension of GLM-4-9B trained on con-\\ntexts up to 1M tokens, demonstrates slightly superior performance than LongPO on the RULER\\nbenchmark. However, these performance gains come at the costs of degenerated short-context per-\\nformance (0.41 on MMLU) and long-context instruction-following capability(0.64 on LongBench-\\nChat (EN)) as illustrated in Figure 3. Notably, our models still outperform GLM-4-9B-1M on\\n∞Bench even trained with substantially shorter sequences. These results underscore the exceptional\\nefficiency of LongPO in transferring performance from short to long contexts through self-evolution,\\nthereby circumventing the need for extensive manual annotation.\\n8\\nPublished as a conference paper at ICLR 2025\\n0 1000 2000 3000 4000 5000\\nTraining Steps\\n20\\n40\\n60\\n80RULER-/glyph1197IAH\\n0 1000 2000 3000 4000 5000\\nTraining Steps\\n40\\n50\\n60MMLU\\nLongPO LongPO (w/o NLL) SFT-Chosen SFT-Rejected DPO SFT-Chosen-Constraint\\nFigure 4: Long- and short-context performance comparison among LongPO, SFT on chosen re-\\nsponses (SFT-Chosen), SFT on rejected responses ( SFT-Rejected), DPO, and SFT on chosen re-\\nsponses with short-to-long constraint (SFT-Chosen-Constraint).\\nLong-context annotation is not sufficient. The superiority of our approach is particularly evident\\nin the En.QA task within ∞Bench, which involves complex free-form question answering over\\nextensive book-length contexts. In this challenging task, our models surpass both GLM-4-9B and\\nGLM-4-9B-1M by substantial margins (10+ points). The inherent difficulty of such task, which\\nposes challenges even for human annotators, highlights the limitations of relying solely on manually\\nannotated long-context data. By effectively transferring short-context capabilities to long-context\\nscenarios, LongPO demonstrates superior scalability and efficacy across diverse and intricate tasks.\\nSuperior LLMs Yet to Dominate Long-Context Scenarios When benchmarked against leading\\nmodels such as GPT-4-128K, our LongPO-trained models still exhibit comparable or even superior\\nlong-context performance (e.g., Mistral-7B-LongPO-128K of 39.27 vs. GPT-4-128K of 34.81 on\\n∞Bench), despite being based on significantly smaller Mistral-7B. This observation reveals that\\neven the most advanced LLMs have not yet achieved the same level of dominance in long-context\\nscenarios as they have in short-context tasks. This performance gap can be attributed primarily to the\\nscarcity of high-quality, large-scale long-context training data. The dearth of such data is particularly\\nimpactful for larger LLMs, given the established scaling laws in language model training. This\\nfinding underscores the potential of LongPO for enhanced performance without the requirement for\\nexternally annotated long-context datasets.\\n5.3 A BLATION STUDIES\\nWe conduct comprehensive ablation studies to investigate the efficacy of components in LongPO:\\nEffectiveness of short-to-long preference. The core of LongPO is learning the short-to-long pref-\\nerence between chosen and rejected responses given short and long contexts, respectively. To evalu-\\nate this component’s effectiveness, we compare LongPO with two baseline methods: SFT on chosen\\nresponses (SFT-Chosen) and on rejected responses (SFT-Rejected). SFT-Chosen implicitly incor-\\nporates short-context preference, while SFT-Rejected entirely omits it. As illustrated in Figure 4,\\nLongPO consistently outperforms both SFT variants in long-context performance (RULER-NIAH)\\nthroughout the training process. This substantial improvement underscores the efficacy of our short-\\nto-long preference approach in enhancing long-context capabilities.\\nEffectiveness of short-to-long constraint. To assess the impact of our short-to-long constraint,\\nwe compare LongPO with DPO upon short-to-long preference that removes this constraint. As evi-\\ndent in Figure 4, the unconstrained DPO demonstrates markedly inferior performance throughout the\\ntraining process, both in long- and short-context tasks. Notably, short-context capabilities degrade\\nrapidly in DPO during the initial training. Conversely, when we apply our short-to-long constraint to\\nnaive SFT without explicit short-to-long preference, the model maintains short-context performance\\non par with the original LLMs, even after long-context alignment. These results demonstrate the\\ncrucial role of our short-to-long constraint in preserving short-context capabilities.\\nImpact of NLL loss. We investigate the effect of incorporating a negative log-likelihood (NLL)\\nloss over long context input and chosen response in Eq. (14) during LongPO training. As shown\\nin Figure 4, removing the NLL loss significantly degrades the long-context performance of LongPO\\n9\\nPublished as a conference paper at ICLR 2025\\nacross the training procedure. Specifically, the convergence of training for long-context performance\\nbecomes slower. This demonstrates the crucial role of NLL loss in enhancing long-context capabil-\\nities without resorting to continual training on long data.\\n6 R ELATED WORK\\nAlignment of LLMs. Aligning Large Language Models (LLMs) with human preferences and val-\\nues has been crucial to unlocking their full potential from large-scale pretraining. The typical align-\\nment process begins with Supervised Finetuning (SFT) on annotated instruction-response pairs. This\\nis followed by Reinforcement Learning from Human Feedback (RLHF), which aligns LLMs more\\nclosely with human intentions through reward model training and policy optimization (Christiano\\net al., 2017; Ouyang et al., 2022; Bai et al., 2022; Stiennon et al., 2020). To streamline RLHF train-\\ning, Direct Preference Optimization (DPO) (Rafailov et al., 2023) and its variants (Ethayarajh et al.,\\n2024; Azar et al., 2023; Pang et al., 2024a; Hong et al., 2024; Meng et al., 2024) have been pro-\\nposed, eliminating the need for explicit reward model training by learning preferences directly from\\nhuman-ranked response pairs. While these alignment methods have shown significant success, they\\nheavily rely on human-annotated data. This reliance becomes problematic for long-context data,\\nwhere human annotation is both challenging and potentially less reliable.\\nLong-context extending of LLMs. Extending the context length of LLMs has been approached\\nthrough various methods. Some techniques involve scaling the rotary position embedding (Su et al.,\\n2022) followed by continual training on a small corpus of long documents (Chen et al., 2023b; Peng\\net al., 2023; Rozière et al., 2023; Chen et al., 2023a). Alternative approaches, such as those proposed\\nby Jin et al. (2024); An et al. (2024), introduce hierarchical or chunked attention mechanisms to ex-\\ntend context length without additional training. However, these methods often involve limitations in\\npractical applications. Recent advancements include the work of Dubey et al. (2024), who proposed\\ncontinual pretraining on a massive long-context corpus (800B tokens) and incorporating a small\\nfraction (0.1%) of long-context data during SFT to enhance long-context capabilities. Zeng et al.\\n(2024) utilizes human-annotated long-context data for SFT and DPO to align long-context LLMs.\\nDespite their effectiveness, these methods require either extensive training or human annotation of\\nlong-context data, making them prohibitively expensive and lack scalability.\\nSelf-Evolving LLMs. Recent works (Yuan et al., 2024; Liu et al., 2024a; Li et al., 2024) have\\nunveiled the remarkable capability of Large Language Models (LLMs) to evolve from relatively\\nweak to significantly stronger performance through self-augmented data. Yuan et al. (2024);\\nLiu et al. (2024a) leverage iterative training on model-generated responses, ranked by LLM-as-\\na-Judge (Zheng et al., 2023) prompting, to enhance model itself. Li et al. (2024) introduces the\\ninstruction backtranslation to produce self-augmenting data that further enhances model capabili-\\nties. Our work first extends the self-evolution property to the context length, to develop long-context\\nLLMs without relying on external annotations.\\n7 C ONCLUSION AND DISCUSSION\\nIn this work, we propose LongPO, a novel long-context alignment method that enables LLMs to ef-\\nfectively transfer their short-context capabilities to long-context scenarios. Our approach addresses\\nkey challenges in long-context alignment by leveraging intrinsic model knowledge, eliminating the\\nneed for external long-context annotated data. LongPO is built on short-to-long preference data,\\ncomprising paired responses for the same instruction given a long context and relevant shortened\\nchunk, respectively. By steering the policy model to learn from the discrepancies within these paired\\nresponses, LongPO facilitates the transfer of established capabilities from short to long contexts. In\\naddition, LongPO incorporates a short-to-long constraint using KL divergence, that effectively pre-\\nserve short-context performance during training. Experimental results demonstrate that LongPO\\nsignificantly improves long-context performance across various tasks, outperforming existing align-\\nment methods and even surpassing more sophisticated models. Importantly, this improvement is\\nachieved without sacrificing short-context proficiency. The success of LongPO highlights the poten-\\ntial of leveraging internal model knowledge for alignment tasks, opening new avenues for efficient\\nadaptation of LLMs to diverse context lengths.\\n10\\nPublished as a conference paper at ICLR 2025\\nACKNOWLEDGMENTS\\nThis work was supported by DAMO Academy through DAMO Academy Research Intern Program.\\nREFERENCES\\nChenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong.\\nTraining-free long-context scaling of large language models, 2024.\\nMohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal\\nValko, and Rémi Munos. A general theoretical paradigm to understand learning from human\\npreferences. ArXiv, abs/2310.12036, 2023. URL https://api.semanticscholar.org/\\nCorpusID:264288854.\\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Ols-\\nson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-\\nJohnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse,\\nKamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mer-\\ncado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna\\nKravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Con-\\nerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario\\nAmodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai:\\nHarmlessness from ai feedback. 2022. URL https://arxiv.org/abs/2212.08073.\\nYushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi\\nLi. Longalign: A recipe for long context alignment of large language models. 2024. URL\\nhttps://arxiv.org/abs/2401.18058.\\nEdward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen\\nRajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard\\n(2023-2024). https://huggingface.co/spaces/open-llm-leaderboard-old/\\nopen_llm_leaderboard, 2023.\\nGuanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Li Bing. Clex: Continuous length\\nextrapolation for large language models. ArXiv, abs/2310.16450, 2023a. URL https://api.\\nsemanticscholar.org/CorpusID:264451707.\\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window\\nof large language models via positional interpolation. 2023b. URL https://arxiv.org/\\nabs/2306.15595.\\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\\nreinforcement learning from human preferences. Advances in neural information processing sys-\\ntems, 30, 2017.\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\\nchallenge. ArXiv, abs/1803.05457, 2018. URL https://api.semanticscholar.org/\\nCorpusID:3922816.\\nTogether Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\\nURL https://github.com/togethercomputer/RedPajama-Data.\\nTri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.\\nURL https://arxiv.org/abs/2307.08691.\\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and\\nmemory-efficient exact attention with IO-awareness. InAdvances in Neural Information Process-\\ning Systems, 2022. URL https://arxiv.org/abs/2205.14135.\\n11\\nPublished as a conference paper at ICLR 2025\\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\\narXiv preprint arXiv:2407.21783, 2024.\\nKawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model\\nalignment as prospect theoretic optimization. ArXiv, abs/2402.01306, 2024. URL https://\\napi.semanticscholar.org/CorpusID:267406810.\\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Fos-\\nter, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muen-\\nnighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lin-\\ntang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework\\nfor few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/\\n12608602.\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Ja-\\ncob Steinhardt. Measuring massive multitask language understanding. In International Confer-\\nence on Learning Representations, 2021. URL https://openreview.net/forum?id=\\nd7KBjmI3GmQ.\\nJiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without\\nreference model. ArXiv, abs/2403.07691, 2024. URL https://api.semanticscholar.\\norg/CorpusID:268363309.\\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang\\nZhang, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language\\nmodels? arXiv preprint arXiv:2404.06654, 2024.\\nSam Adé Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song, Samyam Ra-\\njbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training\\nof extreme long sequence transformer models. ArXiv, abs/2309.14509, 2023. URL https:\\n//api.semanticscholar.org/CorpusID:262826014.\\nAlbert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\\nChaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lu-\\ncile Saulnier, L’elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,\\nThibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. ArXiv,\\nabs/2310.06825, 2023. URL https://api.semanticscholar.org/CorpusID:\\n263830494.\\nHongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan\\nChen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning, 2024.\\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Yoshua\\nBengio and Yann LeCun (eds.),3rd International Conference on Learning Representations, ICLR\\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:\\n//arxiv.org/abs/1412.6980.\\nXian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and\\nMike Lewis. Self-alignment with instruction backtranslation, 2024. URL https://arxiv.\\norg/abs/2308.06259.\\nAiwei Liu, Haoping Bai, Zhiyun Lu, Xiang Kong, Simon Wang, Jiulong Shan, Meng Cao, and\\nLijie Wen. Direct large language model alignment through self-rewarding contrastive prompt\\ndistillation, 2024a. URL https://arxiv.org/abs/2402.11907.\\nHao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video\\nand language with ringattention. arXiv preprint, 2024b. URL https://arxiv.org/abs/\\n2402.08268.\\nYu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization\\nwith a reference-free reward. ArXiv, abs/2405.14734, 2024. URL https://api.\\nsemanticscholar.org/CorpusID:269983560.\\n12\\nPublished as a conference paper at ICLR 2025\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel-\\nton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano,\\nJan Leike, and Ryan J. Lowe. Training language models to follow instructions with human\\nfeedback. ArXiv, abs/2203.02155, 2022. URL https://api.semanticscholar.org/\\nCorpusID:246426909.\\nRichard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason\\nWeston. Iterative reasoning preference optimization. ArXiv, abs/2404.19733, 2024a. URL\\nhttps://api.semanticscholar.org/CorpusID:269457506.\\nRichard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason\\nWeston. Iterative reasoning preference optimization, 2024b. URL https://arxiv.org/\\nabs/2404.19733.\\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\\nextension of large language models. 2023. URL https://arxiv.org/abs/2309.00071.\\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and\\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\\nIn NeurIPS, 2023.\\nBaptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\\nAdi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton,\\nManish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez,\\nJade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and\\nGabriel Synnaeve. Code llama: Open foundation models for code. 2023. URL https://\\narxiv.org/abs/2308.12950.\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver-\\nsarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019.\\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,\\nDario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances\\nin Neural Information Processing Systems, 33:3008–3021, 2020.\\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: En-\\nhanced transformer with rotary position embedding. 2022. URLhttps://arxiv.org/abs/\\n2104.09864.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and\\nHannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In\\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume\\n1: Long Papers). URL https://aclanthology.org/2023.acl-long.754.\\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\\nAndrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Interna-\\ntional Conference on Learning Representations, 2022. URL https://openreview.net/\\nforum?id=gEZrGCozdqR.\\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\\nChengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang,\\nJialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jin-\\ngren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin\\nYang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao,\\nRunji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wen-\\nbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng\\nRen, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu,\\nZeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL\\nhttps://arxiv.org/abs/2407.10671.\\n13\\nPublished as a conference paper at ICLR 2025\\nWeizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason\\nWeston. Self-rewarding language models. ArXiv, abs/2401.10020, 2024. URL https://api.\\nsemanticscholar.org/CorpusID:267035293.\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma-\\nchine really finish your sentence? In Annual Meeting of the Association for Computational Lin-\\nguistics, 2019. URL https://api.semanticscholar.org/CorpusID:159041722.\\nTeam Glm Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu\\nFeng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng,\\nJiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Ming yue Liu,\\nMinlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao,\\nShuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiaoyu Xia, Xiaohan Zhang, Xiaotao\\nGu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yi An, Yifan\\nXu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang,\\nZhenyi Yang, Zhengxiao Du, Zhen-Ping Hou, and Zihan Wang. Chatglm: A family of large\\nlanguage models from glm-130b to glm-4 all tools. ArXiv, abs/2406.12793, 2024. URL https:\\n//api.semanticscholar.org/CorpusID:270562306.\\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen\\nThai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. ∞Bench: Extending long context evaluation\\nbeyond 100K tokens. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings\\nof the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers). URL https://aclanthology.org/2024.acl-long.814.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\\nZi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph Gonzalez, and Ion Stoica.\\nJudging llm-as-a-judge with mt-bench and chatbot arena. ArXiv, abs/2306.05685, 2023. URL\\nhttps://api.semanticscholar.org/CorpusID:259129398.\\n14\\nPublished as a conference paper at ICLR 2025\\nA M ATHEMATICAL DERIVATIONS\\nA.1 D ERIVING THE LONG PO O BJECTIVE\\nIn this section, we will derive the reward function of our LongPO objective in Eq. (11) by incorporat-\\ning short-to-long constraint in Eq. (10). Starting from RLHF objective in Eq. (1) with short-to-long\\nconstraint, we have\\nmax\\nπ\\nExL∼D,y∼π[r(xL, y)]−βDKL[π(y∣xL)∣∣πS(y∣xS)] (15)\\nFollowing the DPO derivation process (Rafailov et al., 2023), we have:\\nmax\\nπ\\nE(xL,xS)∼ ˆDSL,y∼π(y∣xL)[r(xL, y)]−βDKL[π(y∣xL) ∣∣πS(y∣xS)]\\n=max\\nπ\\nE(xL,xS)∼ ˆDSL Ey∼π(y∣xL) [r(xL, y)−β log π(y∣xL)\\nπS(y∣xS)]\\n=min\\nπ\\nE(xL,xS)∼ ˆDSL Ey∼π(y∣xL) [log π(y∣xL)\\nπS(y∣xS) − 1\\nβ r(xL, y)]\\n=min\\nπ\\nE(xL,xS)∼ ˆDSL Ey∼π(y∣xL)\\n⎡⎢⎢⎢⎢⎢⎣\\nlog π(y∣xL)\\n1\\nZ(xL,xS) πS(y∣xS)exp (1\\nβ r(xL, y))\\n−log Z(xL, xS)\\n⎤⎥⎥⎥⎥⎥⎦\\n,\\n(16)\\nwhere we have partition function:\\nZ(xL, xS) =∑\\ny\\nπS(y∣xS)exp (1\\nβ r(xL, y)).\\nThe partition function is only related to xL, xS, and original short-context LLM πS. Hence we have\\nthe optimal solution following Rafailov et al. (2023):\\nπ∗(y∣xL) = 1\\nZ(xL, xS)πS(y∣xS)exp (1\\nβ r(xL, y)). (17)\\nThe optimal reward function would be derived:\\nr∗(xL, y) =β log π∗(y∣xL)\\nπS(y∣xS) +β log Z(xL, xS). (18)\\nWe thus have:\\np∗(y1 ≻y2∣xL) = exp (r∗(xL, y1))\\nexp (r∗(xL, y1))+exp (r∗(xL, y2))\\n=\\nexp (β log π∗(y1∣xL)\\nπS(y1∣xS) +β log Z(xL, xS))\\nexp (β log π∗(y1∣xL)\\nπS(y1∣xS) +β log Z(xL, xS))+exp (β log π∗(y2∣xL)\\nπS(y2∣xS) +β log Z(xL, xS))\\n= 1\\n1 +exp (β log π∗(y2∣xL)\\nπS(y2∣xS) −β log π∗(y1∣xL)\\nπS(y1∣xS) )\\n=σ (β log π∗(y1∣xL)\\nπS(y1∣xS) −β log π∗(y2∣xL)\\nπS(y2∣xS)).\\nBy optimizing the πθ towards the optimal policy π∗, we finally access the objective of LongPO\\nin Eq. (12).\\nB E XPERIMENTAL DETAILS\\nB.1 D ATA CONSTRUCTION DETAILS\\nWe prompt the Mistral-7B-Instruct-v0.2 to generate instructions with decode parameters of temper-\\nature T = 0.7 and p = 0.9. The prompt of Self-Instruct to generate an instruction pool is shown\\n15\\nPublished as a conference paper at ICLR 2025\\nFigure 5: The prompt for generating instruction pool.\\nin Figure 5. For generating the corresponding responses, we directly concatenate the short or long\\ncontext with corresponding instructions and adopt the greedy decoding to maintain the deterministic\\nbehaviour of LLMs. As shown in Figure 6, the model would tend to prefer the high-quality chosen\\nresponse and deviate from the low-quality rejected response over long context, hence improve the\\nlong-context capabilities.\\nB.2 E VALUATION DETAILS\\nOn long-context benchmarks InfiniteBench and RULER, we evaluate our models and all baselines\\nfollowing the settings in the original benchmarks. For short-context evaluation, we utilize the lm-\\nevaluaton-harness framework (Gao et al., 2024) and following the evaluation settings in (Beeching\\net al., 2023): 5-shots for MMLU, 25-shots for ARC-C, 10-shots for Hellaswag, and 5-shots for\\nWinogrande. We use GPT-4-Turbo-1106-Preview as the judge for MT-Bench and LongBench-Chat\\nevaluation.\\nB.3 M ORE TRAINING DETAILS\\nLeveraging the DeepSpeed-Ulysses sequence parallel framework, we train the Mistral-7B/Qwen2.5-\\n7B with a sequence length of 128K on an 8 ×A800 80GB, achieving a throughput of 4,401 tokens\\nper second. For sequence lengths of 256K and 512K, the models are trained on a 16 ×A800 80GB,\\nyielding throughputs of 4,120 tokens per second and 2,744 tokens per second, respectively. To\\nfacilitate a comparison with standard LLM alignment methods, we train Mistral-7B using SFT and\\nDPO utilizing the same short-to-long preference data of LongPO. For DPO training, we apply the\\nsame settings as LongPO outlined in §4.1, but excluding the short-to-long constraint of LongPO\\nintroduced in §3.2. Since SFT cannot utilize paired responses within preference data, we train it\\nusing only the chosen responses provided alongside long context inputs. The hyperparameters for\\nSFT remain unchanged, except for an increase in the learning rate to 2e-5.\\n16\\nPublished as a conference paper at ICLR 2025\\n(a) The rewards for chosen response during training.\\n (b) The rewards for rejected response during training.\\nFigure 6: The chosen and rejected rewards during the training of Mistral-7B-LongPO-128K.\\nTable 2: Full results on 13 tasks of RULER benchmark. The bold values denote the average score\\nof 13 tasks in RULER over various context lengths.\\nModel Category 4k 8k 16k 32k 64k 128k A VG\\nQwen2.5-7B-Instruct\\nNIAH 99.69 98.45 97.82 95.24 74.56 26.86 82.10\\nVT 99.88 99.72 96.24 96.44 81.44 6.84 80.09\\nAGG 92.52 89.78 92.08 81.93 62.48 28.23 74.50\\nQA 71.00 65.30 64.00 58.70 46.80 19.99 54.30\\nA VG (13 tasks) 94.19 92.11 91.61 87.66 68.96 24.47 76.50\\nQwen2.5-7B-LongPO-128K\\nNIAH 99.64 98.97 97.80 95.54 94.80 88.15 95.82\\nVT 99.96 99.92 96.12 86.24 78.20 77.80 89.71\\nAGG 95.50 86.12 91.75 82.56 66.31 49.81 78.67\\nQA 70.00 64.00 62.70 57.70 53.00 49.00 59.40\\nA VG (13 tasks) 94.47 91.69 91.34 87.00 82.71 75.43 87.11\\nMistral-7B-LongPO-128K\\nNIAH 99.43 98.64 98.09 97.84 95.82 91.44 96.88\\nVT 99.40 99.16 98.08 96.36 92.80 93.12 96.49\\nAGG 88.31 82.91 92.23 72.775 46.305 46.79 71.55\\nQA 71.10 70.15 66.60 65.80 61.00 54.20 64.81\\nA VG (13 tasks) 93.36 91.88 92.35 88.94 82.61 78.97 88.02\\nMistral-7B-LongPO-256K\\nNIAH 99.16 97.79 98.02 97.76 96.53 91.54 96.80\\nVT 99.40 99.20 97.96 97.72 94.21 93.52 97.00\\nAGG 87.40 76.59 89.03 72.20 45.17 44.47 69.14\\nQA 71.50 69.50 66.70 64.30 60.80 56.40 64.87\\nA VG (13 tasks) 93.11 90.28 91.81 88.68 82.95 79.04 87.65\\nMistral-7B-LongPO-512K\\nNIAH 99.19 97.78 98.06 97.69 96.62 94.36 97.28\\nVT 99.44 99.16 98.04 97.80 95.92 94.52 97.48\\nAGG 87.56 76.71 88.95 72.70 44.93 44.51 69.22\\nQA 71.40 69.50 66.40 64.50 60.60 57.10 64.92\\nA VG (13 tasks) 93.14 90.29 91.78 88.75 83.07 80.97 88.00\\nTable 3: Performance on short-context tasks.\\nModel MMLU ARC-C Hellaswag Winogrande MT-Bench\\nMistral-7B-Instruct-v0.2 59.15 59.26 83.2 78.4 6.34\\nMistral-7B-LongPO-128K 59.99 59.34 82.99 78.53 6.35\\nMistral-7B-LongPO-256K 59.47 60.28 83.14 78.14 6.38\\nMistral-7B-LongPO-512K 59.51 60.58 82.87 77.66 6.34\\nQwen2.5-7B-Instruct 74.28 67.15 81.41 74.66 7.30\\nQwen2.5-7B-LongPO-128K 73.64 65.70 80.82 74.98 7.62\\n17'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_documents(folder_path: str) -> List[Tuple[str, str]]:\n",
    "    docs = []\n",
    "    for file in sorted(Path(folder_path).glob(\"*.txt\")):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            docs.append((file.name, f.read()))\n",
    "    return docs\n",
    "    \n",
    "def concat_documents(docs: List[Tuple[str, str]]) -> Tuple[str, int]:\n",
    "    combined_text = \"\"\n",
    "    for name, text in docs:\n",
    "        header = f\"\\n\\n===== Document: {name} =====\\n\\n\"\n",
    "        combined_text += header + text\n",
    "    return combined_text\n",
    "    \n",
    "long_context = concat_documents(load_documents(\"dummy_files/hard\"))\n",
    "long_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48463710-0a23-49b4-b03f-acf1de3e5245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>golden</th>\n",
       "      <th>thoughts</th>\n",
       "      <th>answers</th>\n",
       "      <th>hallucination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What does the first clue say?</td>\n",
       "      <td>People Passion Innovation from article1.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The first clue says: \"People Passion Innovation\"</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the 2nd clue?</td>\n",
       "      <td>DSO53 from article5.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The second clue is: \"People Passion Innovation.\"</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What is Clue #3?</td>\n",
       "      <td>I love IEL! From article3.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Clue #3 is hidden within the description of DS...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                       question  \\\n",
       "0   1  What does the first clue say?   \n",
       "1   2          What is the 2nd clue?   \n",
       "2   3               What is Clue #3?   \n",
       "\n",
       "                                        golden  thoughts  \\\n",
       "0  People Passion Innovation from article1.txt       NaN   \n",
       "1                      DSO53 from article5.txt       NaN   \n",
       "2                I love IEL! From article3.txt       NaN   \n",
       "\n",
       "                                             answers hallucination  \n",
       "0   The first clue says: \"People Passion Innovation\"            No  \n",
       "1   The second clue is: \"People Passion Innovation.\"           Yes  \n",
       "2  Clue #3 is hidden within the description of DS...           Yes  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = pd.read_csv(\"dummy_testset.csv\")\n",
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26875b7b-c39d-476c-bb32-eec2a03138fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution here! Please feel free to be creative! ###\n",
    "def hallu_det(long_context, testset):\n",
    "    ### Assume your solution returns some classification \n",
    "    return [\"Yes\", \"Yes\", \"Yes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa04dc-94f9-4450-87af-095f8bf4d313",
   "metadata": {},
   "source": [
    "The accuracy of your solution can be determined by running this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53ffe934-d76b-469c-9c9d-651d467da994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations, Your accuracy is 66.66666666666666% !\n"
     ]
    }
   ],
   "source": [
    "a = hallu_det(long_context, testset)\n",
    "b = testset['hallucination']\n",
    "print(f\"Congratulations, Your accuracy is {sum(1 for x,y in zip(a,b) if x == y) / len(a)*100}% !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35582262-c6e7-454c-a261-327a781ccf5e",
   "metadata": {},
   "source": [
    "Please do approach the H.O.T guys to demonstrate your solution when you run this notebook! We are very interested to learn how to combat hallucinations :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c523b1c4-2f64-42d6-a5df-07fe990cc884",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Here is a hint for you:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef57666-0d8d-4097-a7fc-b53cddc588e3",
   "metadata": {},
   "source": [
    "The model we will use for testing will be: <br> 1) A small model that can support 128k context length <br> 2) From one of the following model families: Llama, Qwen or Phi <br> Note: Recall that your solution should be model agnostic. Remeber to demonstrate that in your presentation! (Overfitting is no good)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
