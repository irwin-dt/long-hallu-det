# ğŸ“œ Long Context Hallucination Detection
## ğŸ¤” Problem
To determine if a LLM is hallucinating in terms of faithfulness to a given long context. ğ“‚ƒâœï¸
<br>
The team considers questions containing a long context that may include multiple documents ğŸ“–ğŸ“‘ğŸ““ğŸ“•ğŸ“‹ğŸ“, and demonstrate across models that their solution can detect any hallucination in the answer that deviates from the context. ğŸ¤–ğŸ‡¦ğŸ‡®ğŸ§ 

_* Please read `generation.ipynb` for more details._

âš ï¸ Important Information for scoring
* Your solution should be model agnostic. *- Practicality/Scalability (30%)* ğŸ”§
* Input to your solution is *question+context+answer*; output is *yes/no*; explanation will be a **bonus** *- Innovation/Novelty (30%)* ğŸ’¡
* For *Performance (30%)* evaluation, we will choose a **small, competetive model** that can handle context length of **at least 128k tokens**. âš¡
* Prepare a 5 minutes presentation + 5 minutes Q&A to show the judges the fruits of your labour!  *- Presentation (10%)* ğŸ”

## ğŸ“¤ Submission
1) Download the model to be used for final testing (to be announced on Day 2). â˜‘ï¸
* In `evaluation.ipynb`
2) Concatenate the `test_documents` and read `test_set.csv` (to be provided on Day 2). â˜‘ï¸
3) Fill in your preferred detection solution. â˜‘ï¸
4) Show us your `accuracy score:` generated by the `evaluation.ipynb` script. â˜‘ï¸

Happy Hacking! ğŸ’»
