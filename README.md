# 📜 Long Context Hallucination Detection
## 🤔 Problem
To determine if a LLM is hallucinating in terms of faithfulness to a given long context. 𓂃✍︎
<br>
The team considers questions containing a long context that may include multiple documents 📖📑📓📕📋📝, and demonstrate across models that their solution can detect any hallucination in the answer that deviates from the context. 🤖🇦🇮🧠

_* Please read `generation.ipynb` for more details._

⚠️ Important Information for scoring
* Your solution should be model agnostic. *- Practicality/Scalability (30%)* 🔧
* Input to your solution is *question+context+answer*; output is *yes/no*; explanation will be a **bonus** *- Innovation/Novelty (30%)* 💡
* For *Performance (30%)* evaluation, we will choose a **small, competetive model** that can handle context length of **at least 128k tokens**. ⚡
* Prepare a 5 minutes presentation + 5 minutes Q&A to show the judges the fruits of your labour!  *- Presentation (10%)* 🔎

## 📤 Submission
1) Download the model to be used for final testing (to be announced on Day 2). ☑️
* In `evaluation.ipynb`
2) Concatenate the `test_documents` and read `test_set.csv` (to be provided on Day 2). ☑️
3) Fill in your preferred detection solution. ☑️
4) Show us your `accuracy score:` generated by the `evaluation.ipynb` script. ☑️

Happy Hacking! 💻
