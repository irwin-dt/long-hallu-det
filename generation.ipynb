{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f1bed7-2d94-40f4-865e-cede69d4f4ef",
   "metadata": {},
   "source": [
    "### Does a long context lead to increased hallucinations in LLM's?\n",
    "In this hackathon, we will investigate if it is possible to detect hallucinations in LLM's given a long context.\n",
    "<br>In this case, we define an LLM to be hallucinating if the output is unfaithful to the given information.\n",
    "<br>Your task is to: <br>1) Create a model agnostic solution that can detect any hallucination(s) in the answer that deviate from the context. <br>2) Provide research insights on the problem and lesson's learnt from this hackathon. <br>This is a starter kit for you to start tinkering with the models and to observe it's outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce18f17-581d-4225-b02c-1c75decef078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-15 11:19:29 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoProcessor\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581b822e-e905-4693-8001-18f07ad2dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "max_length = 128000 # can only be as long as your VRAM allows\n",
    "model_path = \"dummy_model\"\n",
    "docs_folder = \"dummy_files/easy\"\n",
    "system_prompt = \"You are a helpful assistant. You will be given a long context of concatenated documents with clues hidden in them.\"\n",
    "question_list = \"dummy_questions.csv\"\n",
    "output_file = \"dummy_model.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ab4f46-5232-46cb-99aa-0877b9bf8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supports thinking models\n",
    "def extract_thinking_and_summary(text: str, bot: str = \"<think>\", eot: str = \"</think>\") -> Tuple[str, str]:\n",
    "    if bot in text and eot not in text:\n",
    "        return \"\", text\n",
    "    if eot in text:\n",
    "        if bot in text:\n",
    "            return (\n",
    "                text[text.index(bot) + len(bot):text.index(eot)].strip(),\n",
    "                text[text.index(eot) + len(eot):].strip(),\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                text[:text.index(eot)].strip(),\n",
    "                text[text.index(eot) + len(eot):].strip(),\n",
    "            )\n",
    "    return \"\", text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f9ef0d-eb05-4a11-be07-78080cfa3fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load paths of documents (.txt files) in docs_folder\n",
    "def load_documents(folder_path: str) -> List[Tuple[str, str]]:\n",
    "    docs = []\n",
    "    for file in sorted(Path(folder_path).glob(\"*.txt\")):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            docs.append((file.name, f.read()))\n",
    "            # print(docs[-1]) # for debugging\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac7ad112-be8f-415f-b3d2-e9cd047699ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine documents into one long context\n",
    "def concat_documents(docs: List[Tuple[str, str]], tokenizer, max_tokens=max_length) -> Tuple[str, int]:\n",
    "    combined_text = \"\"\n",
    "    token_count = 0\n",
    "    for name, text in docs:\n",
    "        header = f\"\\n\\n===== Document: {name} =====\\n\\n\"\n",
    "        combined_text += header + text\n",
    "        tokens = tokenizer.encode(combined_text, add_special_tokens=False)\n",
    "        token_count = len(tokens)\n",
    "        print(f\"Added {name}: cumulative tokens = {token_count}\") # provides sensing of token contribution of each document\n",
    "        if token_count > max_tokens:\n",
    "            print(f\"⚠️ Warning: Context size exceeded {max_tokens} tokens!\") \n",
    "    return combined_text, token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1dddcf7-616c-4a3c-aa71-0a745945e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model_path: str, docs_folder: str, system_prompt: str, question_list: str, output_file: str):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load documents\n",
    "    docs = load_documents(docs_folder)\n",
    "    if not docs:\n",
    "        print(f\"No text files found in {docs_folder}.\")\n",
    "        return\n",
    "\n",
    "    # Initialize tokenizer & processor\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "    # Concatenate documents with headers\n",
    "    combined_text, total_tokens = concat_documents(docs, tokenizer)\n",
    "    print(f\"Concatenated context: {combined_text}\")\n",
    "    print(f\"Total tokens in concatenated context: {total_tokens}\")\n",
    "\n",
    "    # Prepare questions from .csv file\n",
    "    df = pd.read_csv(question_list)\n",
    "    \n",
    "    thoughts = []\n",
    "    answers = []\n",
    "    \n",
    "    # Initialize vLLM model\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        trust_remote_code=True,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization=0.9,\n",
    "        max_model_len=max_length\n",
    "    )\n",
    "\n",
    "    # Set sampling parameters\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        top_p=1,\n",
    "        repetition_penalty=1.05,\n",
    "        max_tokens=8196,\n",
    "        stop_token_ids=[]\n",
    "    )\n",
    "    \n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Inference\"):\n",
    "        try:\n",
    "            # Safe parsing of messages\n",
    "            qn = row['question']\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": combined_text + qn}]\n",
    "            \n",
    "            prompt = processor.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "           \n",
    "            llm_inputs = {\"prompt\": prompt}\n",
    "\n",
    "            outputs = llm.generate([llm_inputs], sampling_params=sampling_params)\n",
    "            generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "            thought, answer = extract_thinking_and_summary(generated_text)\n",
    "            print(\"thought: \", thought)\n",
    "            print(\"answer: \", answer)\n",
    "            thoughts.append(thought)\n",
    "            answers.append(answer)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Error on row {i}: {e}\")\n",
    "            thoughts.append(\"\")\n",
    "            answers.append(\"\")\n",
    "\n",
    "    df[\"thoughts\"] = thoughts\n",
    "    df[\"answers\"] = answers\n",
    "\n",
    "    # Save result as .csv\n",
    "    model_name = os.path.basename(model_path).replace(\"/\", \"_\")\n",
    "    output_path = Path(output_file or f\"{model_name}_results.csv\")\n",
    "    df.to_csv(output_path)\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"⏱️ Time taken: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69764fc-8213-4fc0-a080-d961886b7f2b",
   "metadata": {},
   "source": [
    "Let's see if the model can extract 3 hidden clues in the concatenated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b654d258-e8fc-4846-8459-3714908d40ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added article1.txt: cumulative tokens = 1575\n",
      "Added article2.txt: cumulative tokens = 2347\n",
      "Added article3.txt: cumulative tokens = 3420\n",
      "Added article4.txt: cumulative tokens = 3858\n",
      "Added article5.txt: cumulative tokens = 4621\n",
      "Added article6.txt: cumulative tokens = 5461\n",
      "Added article7.txt: cumulative tokens = 6666\n",
      "Added article8.txt: cumulative tokens = 7472\n",
      "Added article9.txt: cumulative tokens = 8299\n",
      "Concatenated context: \n",
      "\n",
      "===== Document: article1.txt =====\n",
      "\n",
      "Pushing Boundaries in Flight: The 16th Singapore Amazing Flying Machine Competition Honours Innovation with Thrilling Demonstrations and a Celebration of Creativity\n",
      "\n",
      "Singapore, 5 April 2025 – The Awards Ceremony for the 16th Singapore Amazing Flying Machine Competition (SAFMC) 2025 took place today at the Singapore University of Technology and Design (SUTD). Co-organised by DSO National Laboratories (DSO) and Science Centre Singapore (SCS), this year’s competition brought together 1,865 participants across 584 teams, competing in five distinct categories.\n",
      "\n",
      "\n",
      "The ceremony's Guest of Honour, Mr Heng Chee How, Senior Minister of State for Defence, experienced the thrill of the teams' innovative flying machines through live demonstrations and presented awards to the winners.\n",
      "\n",
      " \n",
      "\n",
      "Speaking at the ceremony, Mr Heng highlighted, “This ceremony reminds us of the journey our participants have undertaken – a journey fuelled by creativity, the spirit of discovery, and an unwavering pursuit of excellence in innovation. You have gone beyond book learning to embrace the challenge of tackling complex problems, iterating on designs, and ultimately, turning abstract concepts and aerodynamics knowledge into amazing flying machines. Your participation is testament to the power of youth-driven innovation.” \n",
      "\n",
      "\n",
      "Mr Cheong Chee Hoo, Chief Executive Officer of DSO said, “Innovation takes flight when we dare to discover, learn, and adapt. This competition is more than just building a flying machine—it’s about embracing challenges, pushing boundaries, and igniting a passion for STEM. Every idea you test and every obstacle you overcome brings you closer to shaping the future of science and technology.”\n",
      "\n",
      "\n",
      "Associate Professor Lim Tit Meng, Chief Executive of the Science Centre Board remarked, “SAFMC is a powerful platform that brings STEM learning to life, encouraging students to think critically, experiment fearlessly, and innovate with purpose. Each year, we witness incredible ingenuity and determination, proving that such opportunities are key to inspiring a promising new generation of STEM leaders.”\n",
      "\n",
      "\n",
      "SAFMC extends learning beyond the classroom, providing a unique, hands-on experience. Over three months, participants engage in workshops, refine their problem-solving skills, and design and construct their own flying machines—ranging from paper planes to drones. The competition culminates in Challenge Week, where they present their creations to a panel of judges.\n",
      "\n",
      "\n",
      "Entries were assessed based on various criteria, including creativity, design, functionality, application of aerodynamics principles, and participants’ presentation skills.\n",
      "\n",
      "Difficulty raised for challenges\n",
      "\n",
      "\n",
      "This year's advanced category challenges pushed participants to tackle greater complexity and explore new avenues for innovation. In Category D1 (Man-Machine Teaming), teams are allowed to customise their payload which allows them to focus on the human-machine interaction, rather than the pickup. For Category D2, no mechanical connection is required between drones. This encourages innovative collaborative strategies through simultaneous task execution. Lastly, Category E (Swarm) will feature a section of the playfield which will be unknown to the participants throughout the competition, simulating real-world conditions where situational information is scarce. Teams that were not able to complete the mission will continue to be able to try.\n",
      "\n",
      "\n",
      "Perseverance pays off for Category D2 and E winners\n",
      "\n",
      "\n",
      "ICG@NTU clinched the championship title in Category E with a well-coordinated drone swarm strategy. The team divided their swarm into three specialised groups: one navigating the Known Search Area (KSA) using planned paths, another tackling the Unknown Search Area (USA) with monocular depth for obstacle avoidance, and a third operating in the pillar cluster (PC), where navigation aids helped \"trap\" drones within the cluster to locate victims. A central server managed offboard computations, while Ultra-Wideband (UWB) anchors ensured precise localisation, enabling most drones to land accurately on victim markers. Their strategic approach and effective use of UWB localisation secured their victory, making them the first team to achieve a perfect score in the category.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "The team previously placed fourth last year and finally clinched the pole position this year, showing perseverance does pay off.\n",
      "\n",
      " \n",
      "\n",
      "Falcon Duo from Nanyang Polytechnic emerged as the champion in Category D2, as they utilised unique navigational aids made of cardboard boxes scattered around the playfield, providing a reliable localisation solution without requiring elaborate infrastructure. Their machine featured a hook and carrier mechanism for payload pick-up and drop-off, ensuring precise control and structural integrity. They secured the win by autonomously piloting two drones to the drop zone and executing a successful simultaneous payload drop. \n",
      "\n",
      " \n",
      "\n",
      "The team is a recurring participant in Category D2, having placed second last year, and competed in Category E this year. \n",
      "\n",
      "Since 2009, SAFMC has served as a launchpad for Singapore’s defence research and development talent, providing opportunities beyond the classroom for students to further their knowledge and passion in Science, Technology, Engineering, and Math.\n",
      "\n",
      "About Singapore Amazing Flying Competition\n",
      "\n",
      " \n",
      "\n",
      "The Singapore Amazing Flying Machine Competition (SAFMC) is Singapore’s largest flying machine competition, jointly organised by DSO National Laboratories and Science Centre Singapore, and supported by the Ministry of Defence. SAFMC is one of the first competitions in Singapore that challenges participants to push the boundaries of innovation with wearable, collaborative, and swarming technologies for their flying machines.\n",
      "\n",
      " \n",
      "\n",
      "Since 2009, this nation-wide competition has attracted over 22,000 aviation enthusiasts. It will once again challenge young bright minds to come up with innovative creations like no other. SAFMC aims to inspire and nurture youths in Science and Technology. Open to all students and public, this annual competition serves as an expedient platform to those who want to get one step closer to their aviation dreams.\n",
      "\n",
      " \n",
      "\n",
      "About DSO National Laboratories\n",
      "\n",
      " \n",
      "\n",
      "DSO National Laboratories (DSO) is Singapore’s largest defence research and development organisation. For the past 5 decades, DSO has been undertaking indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\n",
      "\n",
      " \n",
      "\n",
      "With more than 1,800 defence engineers and scientists, DSO develops cutting edge technologies and solutions to enhance Singapore’s defence and national security capabilities. For more information, please visit www.dso.org.sg. (the first clue is: 'People Passion Innovation')\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "About Science Centre Singapore\n",
      "\n",
      "\n",
      "Science Centre Singapore, a non-formal educational institution and leading regional Science Centre, along with its group of attractions, brings out the wonders of science, technology, engineering and mathematics through its unique blend of exhibitions, educational programmes and events. A custodian of creativity and innovation, Science Centre Singapore has captured the evolution of scientific developments for nearly four decades.\n",
      "\n",
      " \n",
      "\n",
      "The Centre and its partners have played a pivotal role in transforming the way students and the public interact with and learn about science, technology, engineering and mathematics. Since 1977, the Centre has welcomed over 30 million visitors and inspired them with more than 1,000 exhibits spread across 14 exhibition galleries and outdoor exhibition spaces.\n",
      "\n",
      " \n",
      "\n",
      "The Centre’s group of attractions include Omni-Theatre, Snow City and KidsSTOP™. The Omni-Theatre, is Southeast Asia’s first 8K 3D digital theatre with a 23m wide seamless dome screen, is an immersive destination like no other. Snow City is Singapore’s only permanent indoor snow centre offering an Arctic inspired experience at Singapore’s first ice gallery and snow chamber. KidsSTOP™️ - Where every child gets to Imagine, Experience, Discover and Dream - is Singapore’s first children’s science centre offering an enriching experience through purposeful play for children aged 18 months to 8 years old.\n",
      "\n",
      " \n",
      "\n",
      "For more information, please visit www.science.edu.sg.\n",
      "\n",
      "===== Document: article2.txt =====\n",
      "\n",
      "20 Mar 2025\n",
      "\n",
      "MINDEF, DSTA and DSO partner Mistral AI to advance generative AI for defence applications\n",
      "\n",
      "Singapore, 20 March 2025 – Singapore’s Ministry of Defence (MINDEF), Defence Science and Technology Agency (DSTA) and DSO National Laboratories (DSO) will partner France’s Mistral AI to co-develop generative AI models to augment the SAF’s sensemaking and decision support capabilities in areas such as mission planning.\n",
      "\n",
      "\n",
      "The collaboration will focus on fine-tuning Mistral AI’s Large Language Models (LLMs) and developing a mixture-of-experts (MoE) model, with support from AI Singapore, for the local operating context. Leveraging these models, users can effectively retrieve relevant information, empowering commanders by significantly improving AI-decision support. The flexibility offered by Mistral AI to deploy and manage these models on-premise within internet separated environments is critical for defence.\n",
      "\n",
      "\n",
      "DSTA’s Deputy Chief Executive (Information) Ms Gayle Chan said, “Effective mission planning requires analysing vast amounts of data, a process that is highly demanding, resource-intensive and constrained by significant time pressure. In an increasingly complex environment, leveraging AI-enabled tools will support strategic decision-making of our commanders and enhance the agility of the SAF. By combining our expertise with Mistral AI’s capabilities, we aim to push the boundaries of what’s possible and drive meaningful impact.”\n",
      "\n",
      "\n",
      "Chieu Hai Leong, Distinguished Member of Technical Staff at DSO, said “We’re excited to collaborate with our partners to push the boundaries of AI and create an LLM that truly understands local contexts. This isn’t just about enhancing performance — it’s about empowering better decision-making.”\n",
      "\n",
      "\n",
      "Ms Majorie Janiewicz, Global Head of Revenue at Mistral AI said, “We are pleased to announce this partnership, which comes just a few weeks after the establishment of our office in Singapore. Our collaboration with leading technology organisations such as DSTA and DSO confirms our ability to provide secure and highly customisable AI solutions across strategic industries worldwide.”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About Defence Science and Technology Agency\n",
      "\n",
      "\n",
      "The Defence Science and Technology Agency (DSTA) is a top-notch technology organisation that drives innovation and delivers state-of-the-art capabilities to make the Singapore Armed Forces a formidable fighting force. Harnessing and exploiting science and technology, our engineers and IT professionals leverage multidisciplinary expertise to equip our soldiers with advanced systems to defend Singapore. DSTA also contributes its technological expertise to support national-level developments. To achieve our mission, DSTA excels in systems engineering, digitalised platforms, cyber, software development and more.\n",
      "\n",
      "\n",
      "Visit www.dsta.gov.sg for more information.\n",
      "\n",
      "\n",
      "About DSO National Laboratories\n",
      "\n",
      "\n",
      "DSO National Laboratories (DSO) is Singapore’s largest defence research and development organisation. For the past 5 decades, DSO has been undertaking indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\n",
      "\n",
      "\n",
      "With more than 1,800 defence engineers and scientists, DSO develops cutting edge technologies and solutions to enhance Singapore’s defence and national security capabilities. For more information, please visit www.dso.org.sg. \n",
      "\n",
      "\n",
      "About Mistral AI\n",
      "\n",
      "\n",
      "Mistral AI is a pioneer company in generative artificial intelligence, empowering the world with the tools to build and benefit from the most transformative technology of our time. The company democratizes AI through high-performance, optimized, and cutting-edge open-source models, products and solutions. Headquartered in France and independent, Mistral AI defends a decentralized and transparent approach to technology, with a strong global presence in the United States, United Kingdom, and Singapore.\n",
      "\n",
      "===== Document: article3.txt =====\n",
      "\n",
      "DSO National Laboratories Collaborates with Red Hat to Advance DSO’s Defense Research and Development Efforts\n",
      "\n",
      "DSO to explore DevSecOps practices and edge computing deployments built on Red Hat’s open hybrid cloud portfolio\n",
      "\n",
      " \n",
      "\n",
      "RALEIGH, N.C. – JUNE 13, 2023 – DSO National Laboratories (DSO), Singapore’s national  defense research and development (R&D) organization, and open source leader Red Hat, today announced a collaboration to develop new DevSecOps capabilities. The joint work between Red Hat and DSO shows the value of collaboration to facilitate knowledge exchange in Singapore’s defense R&D efforts.\n",
      "\n",
      " \n",
      "\n",
      "DevSecOps, an IT approach that combines development, operations and system security practices, encompasses culture, automation, and platform design, integrating security considerations as a shared responsibility throughout the entire IT lifecycle. Red Hat aims to collaborate with and support DSO’s DevSecOps development through the adoption of a trusted hybrid cloud solution that improves integration and interoperability among systems. Using Red Hat OpenShift, Red Hat Ansible Automation Platform and Red Hat Device Edge (early access) in addition to Red Hat training services, organizations like DSO can tap enterprise open source software to enhance automation processes and bridge old and new IT systems to deliver timely, mission-critical applications and services. DSO also aims to more quickly develop and deploy software to respond to evolving mission conditions in the field.\n",
      "\n",
      " \n",
      "\n",
      "Red Hat Device Edge delivers enterprise-ready, lightweight Kubernetes container orchestrations, building on the MicroShift project to support different use cases and workloads on small, resource-constrained devices at the farthest edge Along with the Integration of technologies such as Red Hat OpenShift and Red Hat Ansible Automation Platform, this is intended to help DSO further extend container applications to even more remote areas running on resource constrained devices.\n",
      "\n",
      " \n",
      "\n",
      "Supporting Quotes\n",
      "\n",
      "Daniel Aw, vice president, enterprise sales, Red Hat APAC\n",
      "\n",
      "“We are pleased to collaborate with DSO on technology solutions that can help drive their digitalization agenda. With the adoption of Red Hat open source technologies, DSO will have more agility to respond quickly and nimbly to new challenges through continuous innovation. We look forward to working with DSO on DevSecOps, automation, and other initiatives that will help DSO advance Singapore’s defense resiliency.”\n",
      "\n",
      " \n",
      "\n",
      "Cheong Chee Hoo, Chief Executive Officer, DSO National Laboratories\n",
      "\n",
      "“Red Hat is a valuable partner in accelerating DSO’s software development and deployment efforts. This collaboration will significantly advance our knowledge and capabilities in the latest DevSecOps practices as we deliver solutions from enterprise IT systems to mission-critical systems deployed in the field.”\n",
      "\n",
      " \n",
      "\n",
      "Additional Resources\n",
      "\n",
      "Learn more about Red Hat Device Edge\n",
      "Red Hat OpenShift\n",
      "Red Hat Ansible Automation Platform\n",
      "Red Hat training services\n",
      " \n",
      "\n",
      "Connect with Red Hat\n",
      "\n",
      "Learn more about Red Hat\n",
      "Get more news in the Red Hat newsroom\n",
      "Read the Red Hat blog\n",
      "Follow Red Hat on Twitter\n",
      "Join Red Hat on Facebook\n",
      "Watch Red Hat videos on YouTube\n",
      "Follow Red Hat on LinkedIn\n",
      "The 3rd clue is \"I love IEL!\"\n",
      " \n",
      "\n",
      "About Red Hat, Inc.\n",
      "\n",
      "Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500. As a strategic partner to cloud providers, system integrators, application vendors, customers, and open source communities, Red Hat can help organizations prepare for the digital future.\n",
      "\n",
      " \n",
      "\n",
      "About DSO National Laboratories\n",
      "\n",
      "DSO National Laboratories (DSO) is Singapore’s national defense research and development organization. For the past 5 decades, DSO has been undertaking indigenous development of advanced defense and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defense R&D capabilities to support homeland security. With more than 1,600 research scientists and engineers, DSO investigates emerging technologies, matures promising ones, and integrates them into innovative system concepts to meet Singapore’s defense and security needs. For more information, please visit www.dso.org.sg. Find us on Instagram | Facebook | LinkedIn.\n",
      "\n",
      " \n",
      "\n",
      "Forward-Looking Statements\n",
      "\n",
      "Except for the historical information and discussions contained herein, statements contained in this press release may constitute forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995. Forward-looking statements are based on the company’s current assumptions regarding future business and financial performance.  These statements involve a number of risks, uncertainties and other factors that could cause actual results to differ materially. Any forward-looking statement in this press release speaks only as of the date on which it is made. Except as required by law, the company assumes no obligation to update or revise any forward-looking statements.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "###\n",
      "\n",
      " \n",
      "\n",
      "Red Hat, the Red Hat logo, Ansible, and OpenShift are trademarks or registered trademarks of Red Hat, Inc. or its subsidiaries in the U.S. and other countries.\n",
      "\n",
      "===== Document: article4.txt =====\n",
      "\n",
      "DSO National Laboratories Commemorates 50 Years of Defence Science and Technology\n",
      "\n",
      "Singapore, 14 October 2022 – Prime Minister, Lee Hsien Loong graced DSO’s Golden Jubilee Dinner as its Guest-of-Honour at Shangri-La Hotel today. The event marks DSO’s 50-year journey in defence science and technology, and the relentless pursuit of its critical mission in developing technological surprises to enhance Singapore’s defence and national security capabilities. Prime Minister Lee and guests toured a special exhibition which unveiled never-been-seen archived documents, artefacts and photos.\n",
      "\n",
      "In his keynote address, PM Lee said, “I am very happy that DSO is celebrating its Golden Jubilee. You have delivered many generations of impressive capabilities to the SAF, and now it is up to the current and future generations of DSO scientists to drive defence R&D for many years to come. And I am confident that you will continue to surprise us, as well as others, and provide the technology edge for our nation’s defence.”\n",
      "\n",
      "As part of its Golden Jubilee celebrations, DSO also launched its DSO50 commemorative website, titled, “The Relentless Pursuit.” The website aims to provide an interactive and immersive experience in discovering the DSO story and spirit. Comprising six chapters, the website features interviews with past and present leaders, as well as stories of DSO’s capability build-up in key defence science and technology areas.\n",
      "\n",
      " \n",
      "\n",
      "###\n",
      "\n",
      " \n",
      "\n",
      "About DSO National Laboratories\n",
      "\n",
      "DSO National Laboratories (DSO) is Singapore’s national defence research and development organisation. For the past 5 decades, DSO has been undertaking indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\n",
      "\n",
      " \n",
      "\n",
      "With more than 1,600 research scientists and engineers, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg\n",
      "\n",
      "===== Document: article5.txt =====\n",
      "\n",
      "DSO Marks 50 Years of Defence Science and Technology with the DSO50 Technology Showcase\n",
      "\n",
      "One of the key events of DSO National Laboratories’ (DSO) Golden Jubilee, the closed-door event is the largest and most extensive technology showcase extended to guests from the Ministry of Defence (MINDEF), the Singapore Armed Forces (SAF), key public agencies and industry partners.\n",
      "\n",
      "19 July 2022 – Minister for Defence, Dr Ng Eng Hen, visited the DSO50 Technology Showcase (TSC) at the DSO Complex today, as part of DSO’s 50th anniversary celebrations. The TSC is a unique exhibition showcasing DSO’s history, capability built-up over the past five decades and future R&D endeavours for MINDEF, the SAF, and Whole-of-Government efforts.\n",
      "\n",
      " \n",
      "\n",
      "The media was given a glimpse of DSO’s innovations in five key technology domains – Cryptography, Cybersecurity, Miniaturised Radio Frequency and Electronics, Artificial Intelligence / Data Analytics and Unmanned Systems. Many of these indigenous technologies and solutions solve important operational problems and are often not available commercially. They can also be customised for other unique and future operating requirements. Most of these enabling technologies are often invisible, and embedded within systems to make it smarter and more robust.\n",
      "\n",
      " \n",
      "\n",
      "One key example is DSO’s efforts in the miniaturisation of critical electronic components that are smaller, lighter and yet provide higher performance in diverse communications platforms and systems. Another important highlight is DSO’s solution is detecting and overcoming adversarial AI that may provide misleading information using fake news and falsified media.\n",
      "\n",
      " \n",
      "\n",
      "Speaking during the DSO50 TSC visit, Dr Ng acknowledged the importance of DSO’s role in defence for the past 50 years and for the future. He said, “I think it was the credit of our founding fathers and successive leaderships that recognised very early on, after the SAF was formed, that we had to have leading-edge technology. It was self-evident that we didn't have strategic depth. Singapore's very small in size and we have limited manpower. And that realisation came very early, that we had to set up this organisation. But I think even our founding fathers would have been quite impressed with how DSO has developed…  So I would expect DSO to continue to play that role, constantly helping the SAF achieve beyond what it's able to.”  \n",
      "\n",
      " \n",
      "\n",
      "Dr Ng also highlighted the close collaboration and deep trust forged between DSO and the SAF. He said, “Behind the opening of this technology showcase has been 50 years of steady progress and good achievements, and I would say that after half a century, DSO is indispensable to the SAF… It is because of leveraging technology, science, manpower, and intellect that we've been able to overcome many, many vulnerabilities. Of course, our inherent vulnerabilities are immutable. That's never going to change. We’re never going to be large or have more manpower than necessary. So we’re thankful that we’ve reached this position with DSO.”\n",
      "\n",
      " \n",
      "\n",
      "###\n",
      "\n",
      " \n",
      "\n",
      "About DSO National Laboratories\n",
      "\n",
      "DSO National Laboratories (\"DSO53\" is the second clue!) is Singapore’s national defence research and development organisation. For the past 5 decades, DSO has been undertaking indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\n",
      "\n",
      " \n",
      "\n",
      "With more than 1,600 research scientists and engineers, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg\n",
      "\n",
      "===== Document: article6.txt =====\n",
      "\n",
      "Singapore Launches Its First Locally Developed Synthetic Aperture Radar (SAR) Microsatellite\n",
      "\n",
      "Singapore, 1 July 2022 – Singapore successfully launched its NeuSAR satellite into space on 30 June 2022; and the satellite has established communications with the ground station at 10:19pm (SGT).\n",
      "\n",
      "\n",
      "NeuSAR is a high-performance small satellite (160 kg) with a fully polarimetric Synthetic Aperture Radar (SAR). Unlike optical cameras satellites that are restricted to daytime and clear weather imaging conditions, a SAR satellite “creates” images by sending radio waves to the Earth’s surface and collecting the returns to form images. NeuSAR is thus able to capture images in both day and night, as well as in difficult environmental conditions due to heavy clouds cover, rainfall and even haze. Being a small satellite, NeuSar is cheaper and faster to build and operate; and provides users with access to low-cost yet high-quality satellite images.\n",
      "\n",
      "\n",
      "The launch of NeuSAR marks another step forward in the growth of the Singapore space industry. It follows the successful launch of its first satellite (X-SAT) in 2011; and its first commercial electro-optical satellite (TeLEOS-1) in 2015. NeuSAR is supported by Singapore’s national space office, the Office for Space Technology & Industry (OSTIn), to serve as a pathfinder to explore the commercial potential of a small satellite constellation and to support Singapore’s space industry capability build-up. The project was led by DSO National Laboratories (DSO) with support from its local space research partners (namely Satellite Technology and Research Centre (STAR) and Centre for Remote Imaging Sensing and Processing (CRISP) from National University of Singapore); and international industry partners (namely Satrec Initiative from South Korea and MMA Design from the United States).\n",
      "\n",
      "\n",
      "“NeuSAR has allowed DSO engineers to push the limits to develop a high-performance low-cost satellite. DSO is proud to offer our systems engineering expertise to develop NeuSAR; and continue our contribution to the growth of Singapore’s space sector,” said Mr Cheong Chee Hoo, Chief Executive Officer of DSO.\n",
      "“The development of NeuSAR is testament to the deep technical capabilities Singapore currently possesses, as well as OSTIn’s efforts to grow the local space ecosystem further. We are committed to supporting the development of local capabilities in space-based technologies, including SAR satellites, to ensure that Singapore can effectively harness these technologies to serve national needs in domains such as aviation, maritime, climate and sustainability,” said Mr David Tan, Executive Director of OSTIn.\n",
      "NeuSAR was launched aboard Indian Space Research Organisation’s (ISRO) Polar Satellite Launch Vehicle (PSLV)-C53 and took off from Satish Dhawan Space Centre SHAR at 8:32pm (SGT).\n",
      "\n",
      "\n",
      "# # #\n",
      "\n",
      "About DSO National Laboratories\n",
      "DSO National Laboratories (DSO) is Singapore’s national defence research and development organisation. With more than 1,600 research scientists and engineers, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg.\n",
      "\n",
      "\n",
      "About the Office for Space Technology & Industry, Singapore\n",
      "OSTIn is Singapore’s national space office. Hosted within the Singapore Economic Development Board (EDB) as an autonomous office, it is responsible for nurturing the development of space technologies to serve national imperatives, growing a globally competitive space industry in Singapore and fostering an enabling regulatory environment for Singapore’s space activities. To support these objectives, OSTIn also seeks to build international partnerships and contribute to strengthening the international governance regime for space activities. In addition, OSTIn also supports the development of talent for Singapore’s space sector and future workforce.\n",
      "\n",
      "\n",
      "For more information, please contact:\n",
      "Edwin Yong                                                                      Fabius Chen\n",
      "Senior Manager                                                              Senior Manager\n",
      "Corporate Communications                                         Corporate Marketing & Communications\n",
      "DSO National Laboratories                                           Singapore Economic Development Board\n",
      "ychanyao@dso.org.sg / 9116 6850                              fabius_chen@edb.gov.sg / 9766 5816\n",
      "\n",
      "===== Document: article7.txt =====\n",
      "\n",
      "Singapore’s First Portable Direct PCR-based Test Kit For Covid-19 Developed for Faster Diagnosis\n",
      "\n",
      "Synergising local research capabilities to achieve breakthrough test kits to support nationwide COVID-19 testing effort\n",
      "\n",
      "SINGAPORE, 18 July 2020 – Back in March 2020, DSO National Laboratories (DSO) successfully developed its Direct-Polymerase Chain Reaction (PCR) technology with the formulation of a new molecular-based assay to detect the presence of COVID-19 virus. Harnessing this capability, DSO partnered A*STAR to jointly develop RESOLUTE, Singapore’s first Direct -PCR COVID-19 test kit that does not require sample processing and can be completed in an hour, compared to a conventional PCR test which takes 2.5 hours or longer.\n",
      "\n",
      "\n",
      "PCR tests are gold-standard diagnostics tools conducted in the laboratory where specialised manpower and equipment are required for sample processing before PCR testing. With DSO’s proprietary Direct-PCR technology, RESOLUTE allows the sample from the patients’ nasopharyngeal swabs to be directly tested without the need for RNA extraction*, prior to PCR testing.  This reduces dependence on RNA extraction reagents.\n",
      "\n",
      "This direct-PCR test kit received HSA’s provisional authorisation in April. The Diagnostics Development (DxD) Hub, a national initiative led by A*STAR’s commercialisation arm A*ccelerate, has been working closely with a local biotech company in the production and deployment.\n",
      "\n",
      "DSO and A*STAR have since worked on an enhanced kit, RESOLUTE 2.0 which has been further simplified for usage. RESOLUTE 2.0 consists of a premixed solution by multiplexing the RESOLUTE series of direct PCR assays, to test for 2 viral and 1 human target in one single test. The RESOLUTE series has been distributed to multiple testing laboratories.\n",
      "\n",
      "Inside a RESOLUTE test kit\n",
      "\n",
      "Mr Frederick Chew, CEO of A*STAR said, “During this critical period, RESOLUTE is a vital addition to Singapore’s testing capacity. Diagnostics testing has been a key pillar of Singapore’s COVID-19 response to date. The DxD Hub crew has been working with a stellar DSO team to rapidly turnaround a product for deployment, reducing our national testing dependence on RNA extraction reagents. We will continue working closely together with DSO and the rest of the research ecosystem to develop dual-use technologies to benefit Singapore and strengthen our resilience.”\n",
      "\n",
      "This collaboration is part of a strategic partnership between DSO National Laboratories and A*STAR that was inked earlier this year to tighten the defence-civilian research nexus. The scope of cooperation involves R&D across the biomedical and physical sciences, technical consultancy, staff exchanges and cross-attachments. This is also the first time both organisations have developed a product together to address public health needs. The intent here is also for public sector IP to be licenced to local companies for commercial translation. DSO and A*STAR are also currently working together, along with other industry partners, to develop therapeutic antibody treatment for COVID-19 patients.  \n",
      "\n",
      "CEO of DSO National Laboratories, Mr Cheong Chee Hoo added, “This is one of the strategic capabilities that DSO has been building up to deal with novel outbreaks. It is timely that we are able to work closely with A*STAR and industry partners to quickly develop and productise this capability into an effective test kit in less than 3 months to strengthen Singapore’s fight against COVID-19.”\n",
      "\n",
      "* RNA extraction – RNA is short for ribonucleic acid. one of the three major biological macromolecules (along with DNA and proteins) that are essential for all known forms of life  It also carries the genetic information of many viruses.\n",
      "\n",
      "###\n",
      "\n",
      "For media queries and clarifications, please contact:\n",
      "\n",
      "DSO National Laboratories\n",
      "\n",
      "Kenny Wong\n",
      "Head, Corporate Communications\n",
      "Tel: +65 9850 5224\n",
      "Email: wengchen@dso.org.sg\n",
      "\n",
      "Agency for Science, Technology and Research (A*STAR)\n",
      "\n",
      "Sunanthar Lu\n",
      "Assistant Head, Corporate Communications\n",
      "Tel: +65 9727 2170\n",
      "Email: Sunanthar_Lu@hq.a-star.edu.sg\n",
      "\n",
      "Robin Chan\n",
      "Head, Corporate Communications\n",
      "Tel: +65 9830 2610\n",
      "Email: Robin_Chan@hq.a-star.edu.sg\n",
      "\n",
      "\n",
      "About DSO National Laboratories\n",
      "\n",
      "DSO National Laboratories (DSO) is Singapore’s national defence research and development organisation. It undertakes indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\n",
      "\n",
      "With more than 1,500 research scientists and engineers, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg.\n",
      "\n",
      "About the Agency for Science, Technology and Research (A*STAR)\n",
      "\n",
      "The Agency for Science, Technology and Research (A*STAR) is Singapore's lead public sector R&D agency, spearheading economic-oriented research to advance scientific discovery and develop innovative technology. Through open innovation, we collaborate with our partners in both the public and private sectors to benefit society.\n",
      "\n",
      "As a Science and Technology Organisation, A*STAR bridges the gap between academia and industry. Our research creates economic growth and jobs for Singapore, and enhances lives by contributing to societal benefits such as improving outcomes in healthcare, urban living, and sustainability.\n",
      "\n",
      "We play a key role in nurturing and developing a diversity of talent and leaders in our Agency and research entities, the wider research community and industry. A*STAR’s R&D activities span biomedical sciences and physical sciences and engineering, with research entities primarily located in Biopolis and Fusionopolis. For ongoing news, visit www.a-star.edu.sg.\n",
      "\n",
      "===== Document: article8.txt =====\n",
      "\n",
      "Clinical Trials For Covid-19 Antibody To Begin In Upcoming Months\n",
      "\n",
      "SINGAPORE 17 June 2020 - DSO National Laboratories (DSO) has discovered five antibodies that demonstrate neutralisation against COVID-19. Over the past three months, DSO scientists have been testing the five antibodies in the laboratory, and results show that they are all potent in blocking infection and effective against key mutations that have emerged in the virus during the pandemic.\n",
      "\n",
      "All five antibodies were isolated from blood samples of recovered COVID-19 patients. This assures a higher degree of patient safety and efficacy, both critical factors for upcoming clinical trials.\n",
      "\n",
      "Dr Conrad Chan, Principal Research Scientist and Laboratory Director (Applied Molecular Technology) from DSO explained, “Administration of an antibody obtained from a recovered patient transfers that person’s immunity to the recipient, enabling any patient to better fight the infection and recover faster. As antibodies remain in the system for close to a month, it can also be administered to prevent infection.\"\n",
      "\n",
      "Since March this year, DSO has screened hundreds of thousands of B cells1, and isolated the first two antibodies for testing within a month of receiving blood samples from the National Centre for Infectious Diseases and Singapore General Hospital. By harnessing DSO’s proprietary screening technique, the B cells are screened simultaneously with live virus, and antibodies with effective virus neutralising properties are quickly identified. This technique, developed in collaboration with the National University of Singapore Life Sciences Institute over the last five years, is part of DSO’s “Antibodies on Demand” strategy to counteract novel infectious disease outbreaks. This technique reduces both the time and manpower required as compared to typical cell-screening methods.\n",
      "\n",
      "With the promising discovery, DSO, as part of a Whole-of-Government collaborative effort involving agencies such as the Ministry of Defence, Ministry of Health and the Economic Development Board, has brought together a Singapore-based consortium comprising government agencies, research institutes and biomedical companies to quickly advance the research towards clinical trials. Human trials for the lead antibody, AOD01, are planned to commence in the upcoming months, pending approval from the Health Sciences Authority. Manufacturing capabilities have also been provisioned to scale up therapeutic antibody treatment for COVID-19 patients upon the successful completion of clinical trials.\n",
      "\n",
      "Dr Brendon Hanson, Principal Research Scientist and Project Lead said, “When clinical trials are completed and successful, we hope to be able to quickly translate the positive results from the laboratory into a viable effective treatment for COVID-19.”\n",
      "\n",
      "Chief Executive Officer of DSO, Mr Cheong Chee Hoo added, “While still in its experimental phase, this discovery is an important milestone in Singapore’s fight against and managing life with COVID-19 until a vaccine is available. With an effective treatment, people will be more assured as they can be treated immediately and can expect to make a faster recovery. This prevents our healthcare system from being overwhelmed, and normalises our daily routine as we continue to live and interact as a community.”\n",
      "\n",
      "1 B cells – Antibodies are produced by the B cells of the human immune system in response to infection. Both antibodies and B cells can be found circulating in our blood.\n",
      "\n",
      "For media queries and clarifications, please contact:\n",
      "\n",
      "DSO National Laboratories\n",
      "\n",
      "Kenny Wong\n",
      "\n",
      "Head, Corporate Communications\n",
      "\n",
      "Tel: +65 9850 5224\n",
      "\n",
      "Email: wengchen@dso.org.sg\n",
      "\n",
      "DSO National Laboratories (DSO) is Singapore’s national defence research and development organisation. It undertakes indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\n",
      "\n",
      "With more than 1,500 technical staff, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg.\n",
      "\n",
      "===== Document: article9.txt =====\n",
      "\n",
      "Modified Musical Instruments Hype the Graduation Ceremony For Children with Special Needs\n",
      "\n",
      "Singapore, 29 September 2017 – Children with mobility impairments may not always have the right musical instruments available to them, but it certainly does not stop them from performing in a musical concert. In a unique partnership between SPD (formerly known as Society for the Physically Disabled) and Mod Squad, a Corporate Social Responsibility (CSR) movement by a group of engineers from DSO National Laboratories, musical instruments have been specially modified so that children from SPD’s Building Bridges EIPIC Centre can perform at their year-end graduation ceremony.\n",
      "\n",
      "\n",
      "Led by Mr Yee Qing Xiang, a Defence Research Engineer at DSO, the team of 10 applied their engineering expertise to modify more than 20 musical instruments ranging from xylophones, cymbals, tambourines to simple percussions like bells. Drawing experience from their previous Hack-a-Toy initiative where toys were modified for children with special needs, the engineers are better acquainted with their physical constrains, and have implemented more advanced assistive devices to help improve their motor coordination.\n",
      "\n",
      "\n",
      "The team even designed their own printed circuit boards, used open-source electronic components and self-written programs to ensure that the children can use the various instruments with ease. Taking the xylophone which has three notes as an example, the modification involves intricate programming, soldering of wires and installation of motors to synchronise the rhythm through the use of a wired wristwatch, making overall coordination much easier.\n",
      "\n",
      "\n",
      "Mr Yee, Mod Squad Team leader, shared on their modification journey, “As engineers, it is really tempting to quickly dive in and use technology to solve problems, but for this project, we needed to take a step back to fully understand the unique needs and problems before coming up with a solution. It is unfamiliar territory to us and this has been a valuable experience as we harnessed the team’s diverse expertise, such as electrical circuit designs, mechanical design and 3D printing.”\n",
      "\n",
      "This is the first time SPD’s Early Intervention Programme for Infant and Children (EIPIC) will be incorporating music performance in their graduation ceremony on 23 November. These specially customised instruments will enable the children to perform and help to further the children’s interest in music.\n",
      "\n",
      "\n",
      "Ms Becky Hoo, SPD’s Director of Children Services, said, “We are deeply heartened with the support of DSO in enhancing playtime for our children. This collaboration has brought tremendous joy and for the first time, allowed the kids to be fully immersed in playing music. We look forward to seeing more of such implementations of Assistive Technology through future partnership with these engineers.”\n",
      "\n",
      "\n",
      "-End-\n",
      "\n",
      "\n",
      "MEDIA CONTACTS\n",
      "\n",
      "Name\t\n",
      "Mr Kenny Wong\n",
      "\n",
      "Head\n",
      "\n",
      "Corporate Communications\n",
      "\n",
      "Mr Edwin Yong\n",
      "\n",
      "Assistant Manager\n",
      "\n",
      "Corporate Communications\n",
      "\n",
      "Contact\t6450 4163 / 9850 5224\t6450 4162 / 9116 6850\n",
      "Email\twengchen@dso.org.sg\tychanyao@dso.org.sg\n",
      "About DSO National Laboratories\n",
      "DSO National Laboratories (DSO) is Singapore’s national defence research and development organisation. It undertakes indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\n",
      "\n",
      "With more than 1,500 technical staff, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg\n",
      "\n",
      "\n",
      "About SPD\n",
      "SPD is a voluntary welfare organisation that supports people with disabilities by promoting their interest, welfare and advancement so as to develop their potential to the fullest. Through programmes and services that encompass therapy, vocational skills training, assistive technology, early intervention, day care, and employment, educational and social support, we seek to enable people with disabilities to be self-reliant and independent. For more information, please visit www.spd.org.sg.\n",
      "Total tokens in concatenated context: 8299\n",
      "INFO 09-15 11:19:31 [config.py:243] Replacing legacy 'type' key with 'rope_type'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 11:19:31,973\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-15 11:19:38 [config.py:1604] Using max model len 128000\n",
      "INFO 09-15 11:19:38 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-15 11:19:43 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 09-15 11:19:45 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-15 11:19:45 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/home/jovyan/dummy_model', speculative_config=None, tokenizer='/home/jovyan/dummy_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/jovyan/dummy_model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 09-15 11:19:47 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 09-15 11:19:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-15 11:19:47 [gpu_model_runner.py:1843] Starting to load model /home/jovyan/dummy_model...\n",
      "INFO 09-15 11:19:47 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 09-15 11:19:47 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.79s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.45s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.50s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-15 11:19:51 [default_loader.py:262] Loading weights took 3.08 seconds\n",
      "INFO 09-15 11:19:51 [gpu_model_runner.py:1892] Model loading took 7.1694 GiB and 3.266711 seconds\n",
      "INFO 09-15 11:19:56 [backends.py:530] Using cache directory: /home/jovyan/.cache/vllm/torch_compile_cache/60cf5f0370/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-15 11:19:56 [backends.py:541] Dynamo bytecode transform time: 4.91 s\n",
      "INFO 09-15 11:20:03 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.583 s\n",
      "INFO 09-15 11:20:04 [monitor.py:34] torch.compile takes 4.91 s in total\n",
      "INFO 09-15 11:20:05 [gpu_worker.py:255] Available KV cache memory: 30.86 GiB\n",
      "INFO 09-15 11:20:05 [kv_cache_utils.py:833] GPU KV cache size: 252,784 tokens\n",
      "INFO 09-15 11:20:05 [kv_cache_utils.py:837] Maximum concurrency for 128,000 tokens per request: 1.97x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:03<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-15 11:20:09 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.57 GiB\n",
      "INFO 09-15 11:20:09 [core.py:193] init engine (profile, create kv cache, warmup model) took 17.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 37.38it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s, est. speed input: 9126.94 toks/s, output: 10.95 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s, est. speed input: 9126.94 toks/s, output: 10.95 toks/s]\u001b[A\n",
      "Inference:  33%|███▎      | 1/3 [00:00<00:01,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thought:  \n",
      "answer:  The first clue says \"People Passion Innovation.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 46.03it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s, est. speed input: 24540.34 toks/s, output: 41.22 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.91it/s, est. speed input: 24540.34 toks/s, output: 41.22 toks/s]\u001b[A\n",
      "Inference:  67%|██████▋   | 2/3 [00:01<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thought:  \n",
      "answer:  The second clue provided in the text is \"DSO53.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 45.44it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s, est. speed input: 31363.84 toks/s, output: 41.40 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s, est. speed input: 31363.84 toks/s, output: 41.40 toks/s]\u001b[A\n",
      "Inference: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thought:  \n",
      "answer:  Clue #3 is \"I love IEL!\"\n",
      "Results saved to dummy_model.csv\n",
      "⏱️ Time taken: 41.11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W915 11:20:12.246120972 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "run_inference(model_path, docs_folder, system_prompt, question_list, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf498940-6904-4c5d-86b1-a07296d99e36",
   "metadata": {},
   "source": [
    "It seems like a context length of 8k isn't that big of a problem for the model. What if we extend the context length with more documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0398c382-4051-4ec6-9b46-25a594bc72d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does the first clue say?</td>\n",
       "      <td>The first clue says \"People Passion Innovation.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the 2nd clue?</td>\n",
       "      <td>The second clue provided in the text is \"DSO53.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Clue #3?</td>\n",
       "      <td>Clue #3 is \"I love IEL!\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question  \\\n",
       "0  What does the first clue say?   \n",
       "1          What is the 2nd clue?   \n",
       "2               What is Clue #3?   \n",
       "\n",
       "                                            answers  \n",
       "0  The first clue says \"People Passion Innovation.\"  \n",
       "1  The second clue provided in the text is \"DSO53.\"  \n",
       "2                          Clue #3 is \"I love IEL!\"  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"dummy_model.csv\")[[\"question\", \"answers\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "835f70c4-5eea-46a6-8d62-24f086bc2f31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added article1.txt: cumulative tokens = 1575\n",
      "Added article2.txt: cumulative tokens = 2347\n",
      "Added article3.txt: cumulative tokens = 3420\n",
      "Added article4.txt: cumulative tokens = 3858\n",
      "Added article5.txt: cumulative tokens = 4621\n",
      "Added article6.txt: cumulative tokens = 5461\n",
      "Added article7.txt: cumulative tokens = 6666\n",
      "Added article8.txt: cumulative tokens = 7472\n",
      "Added article9.txt: cumulative tokens = 8299\n",
      "Added paper1.txt: cumulative tokens = 25430\n",
      "Added paper2.txt: cumulative tokens = 42257\n",
      "Concatenated context: \n",
      "\n",
      "===== Document: article1.txt =====\n",
      "\n",
      "Pushing Boundaries in Flight: The 16th Singapore Amazing Flying Machine Competition Honours Innovation with Thrilling Demonstrations and a Celebration of Creativity\n",
      "\n",
      "Singapore, 5 April 2025 – The Awards Ceremony for the 16th Singapore Amazing Flying Machine Competition (SAFMC) 2025 took place today at the Singapore University of Technology and Design (SUTD). Co-organised by DSO National Laboratories (DSO) and Science Centre Singapore (SCS), this year’s competition brought together 1,865 participants across 584 teams, competing in five distinct categories.\n",
      "\n",
      "\n",
      "The ceremony's Guest of Honour, Mr Heng Chee How, Senior Minister of State for Defence, experienced the thrill of the teams' innovative flying machines through live demonstrations and presented awards to the winners.\n",
      "\n",
      " \n",
      "\n",
      "Speaking at the ceremony, Mr Heng highlighted, “This ceremony reminds us of the journey our participants have undertaken – a journey fuelled by creativity, the spirit of discovery, and an unwavering pursuit of excellence in innovation. You have gone beyond book learning to embrace the challenge of tackling complex problems, iterating on designs, and ultimately, turning abstract concepts and aerodynamics knowledge into amazing flying machines. Your participation is testament to the power of youth-driven innovation.” \n",
      "\n",
      "\n",
      "Mr Cheong Chee Hoo, Chief Executive Officer of DSO said, “Innovation takes flight when we dare to discover, learn, and adapt. This competition is more than just building a flying machine—it’s about embracing challenges, pushing boundaries, and igniting a passion for STEM. Every idea you test and every obstacle you overcome brings you closer to shaping the future of science and technology.”\n",
      "\n",
      "\n",
      "Associate Professor Lim Tit Meng, Chief Executive of the Science Centre Board remarked, “SAFMC is a powerful platform that brings STEM learning to life, encouraging students to think critically, experiment fearlessly, and innovate with purpose. Each year, we witness incredible ingenuity and determination, proving that such opportunities are key to inspiring a promising new generation of STEM leaders.”\n",
      "\n",
      "\n",
      "SAFMC extends learning beyond the classroom, providing a unique, hands-on experience. Over three months, participants engage in workshops, refine their problem-solving skills, and design and construct their own flying machines—ranging from paper planes to drones. The competition culminates in Challenge Week, where they present their creations to a panel of judges.\n",
      "\n",
      "\n",
      "Entries were assessed based on various criteria, including creativity, design, functionality, application of aerodynamics principles, and participants’ presentation skills.\n",
      "\n",
      "Difficulty raised for challenges\n",
      "\n",
      "\n",
      "This year's advanced category challenges pushed participants to tackle greater complexity and explore new avenues for innovation. In Category D1 (Man-Machine Teaming), teams are allowed to customise their payload which allows them to focus on the human-machine interaction, rather than the pickup. For Category D2, no mechanical connection is required between drones. This encourages innovative collaborative strategies through simultaneous task execution. Lastly, Category E (Swarm) will feature a section of the playfield which will be unknown to the participants throughout the competition, simulating real-world conditions where situational information is scarce. Teams that were not able to complete the mission will continue to be able to try.\n",
      "\n",
      "\n",
      "Perseverance pays off for Category D2 and E winners\n",
      "\n",
      "\n",
      "ICG@NTU clinched the championship title in Category E with a well-coordinated drone swarm strategy. The team divided their swarm into three specialised groups: one navigating the Known Search Area (KSA) using planned paths, another tackling the Unknown Search Area (USA) with monocular depth for obstacle avoidance, and a third operating in the pillar cluster (PC), where navigation aids helped \"trap\" drones within the cluster to locate victims. A central server managed offboard computations, while Ultra-Wideband (UWB) anchors ensured precise localisation, enabling most drones to land accurately on victim markers. Their strategic approach and effective use of UWB localisation secured their victory, making them the first team to achieve a perfect score in the category.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "The team previously placed fourth last year and finally clinched the pole position this year, showing perseverance does pay off.\n",
      "\n",
      " \n",
      "\n",
      "Falcon Duo from Nanyang Polytechnic emerged as the champion in Category D2, as they utilised unique navigational aids made of cardboard boxes scattered around the playfield, providing a reliable localisation solution without requiring elaborate infrastructure. Their machine featured a hook and carrier mechanism for payload pick-up and drop-off, ensuring precise control and structural integrity. They secured the win by autonomously piloting two drones to the drop zone and executing a successful simultaneous payload drop. \n",
      "\n",
      " \n",
      "\n",
      "The team is a recurring participant in Category D2, having placed second last year, and competed in Category E this year. \n",
      "\n",
      "Since 2009, SAFMC has served as a launchpad for Singapore’s defence research and development talent, providing opportunities beyond the classroom for students to further their knowledge and passion in Science, Technology, Engineering, and Math.\n",
      "\n",
      "About Singapore Amazing Flying Competition\n",
      "\n",
      " \n",
      "\n",
      "The Singapore Amazing Flying Machine Competition (SAFMC) is Singapore’s largest flying machine competition, jointly organised by DSO National Laboratories and Science Centre Singapore, and supported by the Ministry of Defence. SAFMC is one of the first competitions in Singapore that challenges participants to push the boundaries of innovation with wearable, collaborative, and swarming technologies for their flying machines.\n",
      "\n",
      " \n",
      "\n",
      "Since 2009, this nation-wide competition has attracted over 22,000 aviation enthusiasts. It will once again challenge young bright minds to come up with innovative creations like no other. SAFMC aims to inspire and nurture youths in Science and Technology. Open to all students and public, this annual competition serves as an expedient platform to those who want to get one step closer to their aviation dreams.\n",
      "\n",
      " \n",
      "\n",
      "About DSO National Laboratories\n",
      "\n",
      " \n",
      "\n",
      "DSO National Laboratories (DSO) is Singapore’s largest defence research and development organisation. For the past 5 decades, DSO has been undertaking indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\n",
      "\n",
      " \n",
      "\n",
      "With more than 1,800 defence engineers and scientists, DSO develops cutting edge technologies and solutions to enhance Singapore’s defence and national security capabilities. For more information, please visit www.dso.org.sg. (the first clue is: 'People Passion Innovation')\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "About Science Centre Singapore\n",
      "\n",
      "\n",
      "Science Centre Singapore, a non-formal educational institution and leading regional Science Centre, along with its group of attractions, brings out the wonders of science, technology, engineering and mathematics through its unique blend of exhibitions, educational programmes and events. A custodian of creativity and innovation, Science Centre Singapore has captured the evolution of scientific developments for nearly four decades.\n",
      "\n",
      " \n",
      "\n",
      "The Centre and its partners have played a pivotal role in transforming the way students and the public interact with and learn about science, technology, engineering and mathematics. Since 1977, the Centre has welcomed over 30 million visitors and inspired them with more than 1,000 exhibits spread across 14 exhibition galleries and outdoor exhibition spaces.\n",
      "\n",
      " \n",
      "\n",
      "The Centre’s group of attractions include Omni-Theatre, Snow City and KidsSTOP™. The Omni-Theatre, is Southeast Asia’s first 8K 3D digital theatre with a 23m wide seamless dome screen, is an immersive destination like no other. Snow City is Singapore’s only permanent indoor snow centre offering an Arctic inspired experience at Singapore’s first ice gallery and snow chamber. KidsSTOP™️ - Where every child gets to Imagine, Experience, Discover and Dream - is Singapore’s first children’s science centre offering an enriching experience through purposeful play for children aged 18 months to 8 years old.\n",
      "\n",
      " \n",
      "\n",
      "For more information, please visit www.science.edu.sg.\n",
      "\n",
      "===== Document: article2.txt =====\n",
      "\n",
      "20 Mar 2025\n",
      "\n",
      "MINDEF, DSTA and DSO partner Mistral AI to advance generative AI for defence applications\n",
      "\n",
      "Singapore, 20 March 2025 – Singapore’s Ministry of Defence (MINDEF), Defence Science and Technology Agency (DSTA) and DSO National Laboratories (DSO) will partner France’s Mistral AI to co-develop generative AI models to augment the SAF’s sensemaking and decision support capabilities in areas such as mission planning.\n",
      "\n",
      "\n",
      "The collaboration will focus on fine-tuning Mistral AI’s Large Language Models (LLMs) and developing a mixture-of-experts (MoE) model, with support from AI Singapore, for the local operating context. Leveraging these models, users can effectively retrieve relevant information, empowering commanders by significantly improving AI-decision support. The flexibility offered by Mistral AI to deploy and manage these models on-premise within internet separated environments is critical for defence.\n",
      "\n",
      "\n",
      "DSTA’s Deputy Chief Executive (Information) Ms Gayle Chan said, “Effective mission planning requires analysing vast amounts of data, a process that is highly demanding, resource-intensive and constrained by significant time pressure. In an increasingly complex environment, leveraging AI-enabled tools will support strategic decision-making of our commanders and enhance the agility of the SAF. By combining our expertise with Mistral AI’s capabilities, we aim to push the boundaries of what’s possible and drive meaningful impact.”\n",
      "\n",
      "\n",
      "Chieu Hai Leong, Distinguished Member of Technical Staff at DSO, said “We’re excited to collaborate with our partners to push the boundaries of AI and create an LLM that truly understands local contexts. This isn’t just about enhancing performance — it’s about empowering better decision-making.”\n",
      "\n",
      "\n",
      "Ms Majorie Janiewicz, Global Head of Revenue at Mistral AI said, “We are pleased to announce this partnership, which comes just a few weeks after the establishment of our office in Singapore. Our collaboration with leading technology organisations such as DSTA and DSO confirms our ability to provide secure and highly customisable AI solutions across strategic industries worldwide.”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About Defence Science and Technology Agency\n",
      "\n",
      "\n",
      "The Defence Science and Technology Agency (DSTA) is a top-notch technology organisation that drives innovation and delivers state-of-the-art capabilities to make the Singapore Armed Forces a formidable fighting force. Harnessing and exploiting science and technology, our engineers and IT professionals leverage multidisciplinary expertise to equip our soldiers with advanced systems to defend Singapore. DSTA also contributes its technological expertise to support national-level developments. To achieve our mission, DSTA excels in systems engineering, digitalised platforms, cyber, software development and more.\n",
      "\n",
      "\n",
      "Visit www.dsta.gov.sg for more information.\n",
      "\n",
      "\n",
      "About DSO National Laboratories\n",
      "\n",
      "\n",
      "DSO National Laboratories (DSO) is Singapore’s largest defence research and development organisation. For the past 5 decades, DSO has been undertaking indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\n",
      "\n",
      "\n",
      "With more than 1,800 defence engineers and scientists, DSO develops cutting edge technologies and solutions to enhance Singapore’s defence and national security capabilities. For more information, please visit www.dso.org.sg. \n",
      "\n",
      "\n",
      "About Mistral AI\n",
      "\n",
      "\n",
      "Mistral AI is a pioneer company in generative artificial intelligence, empowering the world with the tools to build and benefit from the most transformative technology of our time. The company democratizes AI through high-performance, optimized, and cutting-edge open-source models, products and solutions. Headquartered in France and independent, Mistral AI defends a decentralized and transparent approach to technology, with a strong global presence in the United States, United Kingdom, and Singapore.\n",
      "\n",
      "===== Document: article3.txt =====\n",
      "\n",
      "DSO National Laboratories Collaborates with Red Hat to Advance DSO’s Defense Research and Development Efforts\n",
      "\n",
      "DSO to explore DevSecOps practices and edge computing deployments built on Red Hat’s open hybrid cloud portfolio\n",
      "\n",
      " \n",
      "\n",
      "RALEIGH, N.C. – JUNE 13, 2023 – DSO National Laboratories (DSO), Singapore’s national  defense research and development (R&D) organization, and open source leader Red Hat, today announced a collaboration to develop new DevSecOps capabilities. The joint work between Red Hat and DSO shows the value of collaboration to facilitate knowledge exchange in Singapore’s defense R&D efforts.\n",
      "\n",
      " \n",
      "\n",
      "DevSecOps, an IT approach that combines development, operations and system security practices, encompasses culture, automation, and platform design, integrating security considerations as a shared responsibility throughout the entire IT lifecycle. Red Hat aims to collaborate with and support DSO’s DevSecOps development through the adoption of a trusted hybrid cloud solution that improves integration and interoperability among systems. Using Red Hat OpenShift, Red Hat Ansible Automation Platform and Red Hat Device Edge (early access) in addition to Red Hat training services, organizations like DSO can tap enterprise open source software to enhance automation processes and bridge old and new IT systems to deliver timely, mission-critical applications and services. DSO also aims to more quickly develop and deploy software to respond to evolving mission conditions in the field.\n",
      "\n",
      " \n",
      "\n",
      "Red Hat Device Edge delivers enterprise-ready, lightweight Kubernetes container orchestrations, building on the MicroShift project to support different use cases and workloads on small, resource-constrained devices at the farthest edge Along with the Integration of technologies such as Red Hat OpenShift and Red Hat Ansible Automation Platform, this is intended to help DSO further extend container applications to even more remote areas running on resource constrained devices.\n",
      "\n",
      " \n",
      "\n",
      "Supporting Quotes\n",
      "\n",
      "Daniel Aw, vice president, enterprise sales, Red Hat APAC\n",
      "\n",
      "“We are pleased to collaborate with DSO on technology solutions that can help drive their digitalization agenda. With the adoption of Red Hat open source technologies, DSO will have more agility to respond quickly and nimbly to new challenges through continuous innovation. We look forward to working with DSO on DevSecOps, automation, and other initiatives that will help DSO advance Singapore’s defense resiliency.”\n",
      "\n",
      " \n",
      "\n",
      "Cheong Chee Hoo, Chief Executive Officer, DSO National Laboratories\n",
      "\n",
      "“Red Hat is a valuable partner in accelerating DSO’s software development and deployment efforts. This collaboration will significantly advance our knowledge and capabilities in the latest DevSecOps practices as we deliver solutions from enterprise IT systems to mission-critical systems deployed in the field.”\n",
      "\n",
      " \n",
      "\n",
      "Additional Resources\n",
      "\n",
      "Learn more about Red Hat Device Edge\n",
      "Red Hat OpenShift\n",
      "Red Hat Ansible Automation Platform\n",
      "Red Hat training services\n",
      " \n",
      "\n",
      "Connect with Red Hat\n",
      "\n",
      "Learn more about Red Hat\n",
      "Get more news in the Red Hat newsroom\n",
      "Read the Red Hat blog\n",
      "Follow Red Hat on Twitter\n",
      "Join Red Hat on Facebook\n",
      "Watch Red Hat videos on YouTube\n",
      "Follow Red Hat on LinkedIn\n",
      "The 3rd clue is \"I love IEL!\"\n",
      " \n",
      "\n",
      "About Red Hat, Inc.\n",
      "\n",
      "Red Hat is the world’s leading provider of enterprise open source software solutions, using a community-powered approach to deliver reliable and high-performing Linux, hybrid cloud, container, and Kubernetes technologies. Red Hat helps customers integrate new and existing IT applications, develop cloud-native applications, standardize on our industry-leading operating system, and automate, secure, and manage complex environments. Award-winning support, training, and consulting services make Red Hat a trusted adviser to the Fortune 500. As a strategic partner to cloud providers, system integrators, application vendors, customers, and open source communities, Red Hat can help organizations prepare for the digital future.\n",
      "\n",
      " \n",
      "\n",
      "About DSO National Laboratories\n",
      "\n",
      "DSO National Laboratories (DSO) is Singapore’s national defense research and development organization. For the past 5 decades, DSO has been undertaking indigenous development of advanced defense and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defense R&D capabilities to support homeland security. With more than 1,600 research scientists and engineers, DSO investigates emerging technologies, matures promising ones, and integrates them into innovative system concepts to meet Singapore’s defense and security needs. For more information, please visit www.dso.org.sg. Find us on Instagram | Facebook | LinkedIn.\n",
      "\n",
      " \n",
      "\n",
      "Forward-Looking Statements\n",
      "\n",
      "Except for the historical information and discussions contained herein, statements contained in this press release may constitute forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995. Forward-looking statements are based on the company’s current assumptions regarding future business and financial performance.  These statements involve a number of risks, uncertainties and other factors that could cause actual results to differ materially. Any forward-looking statement in this press release speaks only as of the date on which it is made. Except as required by law, the company assumes no obligation to update or revise any forward-looking statements.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "###\n",
      "\n",
      " \n",
      "\n",
      "Red Hat, the Red Hat logo, Ansible, and OpenShift are trademarks or registered trademarks of Red Hat, Inc. or its subsidiaries in the U.S. and other countries.\n",
      "\n",
      "===== Document: article4.txt =====\n",
      "\n",
      "DSO National Laboratories Commemorates 50 Years of Defence Science and Technology\n",
      "\n",
      "Singapore, 14 October 2022 – Prime Minister, Lee Hsien Loong graced DSO’s Golden Jubilee Dinner as its Guest-of-Honour at Shangri-La Hotel today. The event marks DSO’s 50-year journey in defence science and technology, and the relentless pursuit of its critical mission in developing technological surprises to enhance Singapore’s defence and national security capabilities. Prime Minister Lee and guests toured a special exhibition which unveiled never-been-seen archived documents, artefacts and photos.\n",
      "\n",
      "In his keynote address, PM Lee said, “I am very happy that DSO is celebrating its Golden Jubilee. You have delivered many generations of impressive capabilities to the SAF, and now it is up to the current and future generations of DSO scientists to drive defence R&D for many years to come. And I am confident that you will continue to surprise us, as well as others, and provide the technology edge for our nation’s defence.”\n",
      "\n",
      "As part of its Golden Jubilee celebrations, DSO also launched its DSO50 commemorative website, titled, “The Relentless Pursuit.” The website aims to provide an interactive and immersive experience in discovering the DSO story and spirit. Comprising six chapters, the website features interviews with past and present leaders, as well as stories of DSO’s capability build-up in key defence science and technology areas.\n",
      "\n",
      " \n",
      "\n",
      "###\n",
      "\n",
      " \n",
      "\n",
      "About DSO National Laboratories\n",
      "\n",
      "DSO National Laboratories (DSO) is Singapore’s national defence research and development organisation. For the past 5 decades, DSO has been undertaking indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\n",
      "\n",
      " \n",
      "\n",
      "With more than 1,600 research scientists and engineers, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg\n",
      "\n",
      "===== Document: article5.txt =====\n",
      "\n",
      "DSO Marks 50 Years of Defence Science and Technology with the DSO50 Technology Showcase\n",
      "\n",
      "One of the key events of DSO National Laboratories’ (DSO) Golden Jubilee, the closed-door event is the largest and most extensive technology showcase extended to guests from the Ministry of Defence (MINDEF), the Singapore Armed Forces (SAF), key public agencies and industry partners.\n",
      "\n",
      "19 July 2022 – Minister for Defence, Dr Ng Eng Hen, visited the DSO50 Technology Showcase (TSC) at the DSO Complex today, as part of DSO’s 50th anniversary celebrations. The TSC is a unique exhibition showcasing DSO’s history, capability built-up over the past five decades and future R&D endeavours for MINDEF, the SAF, and Whole-of-Government efforts.\n",
      "\n",
      " \n",
      "\n",
      "The media was given a glimpse of DSO’s innovations in five key technology domains – Cryptography, Cybersecurity, Miniaturised Radio Frequency and Electronics, Artificial Intelligence / Data Analytics and Unmanned Systems. Many of these indigenous technologies and solutions solve important operational problems and are often not available commercially. They can also be customised for other unique and future operating requirements. Most of these enabling technologies are often invisible, and embedded within systems to make it smarter and more robust.\n",
      "\n",
      " \n",
      "\n",
      "One key example is DSO’s efforts in the miniaturisation of critical electronic components that are smaller, lighter and yet provide higher performance in diverse communications platforms and systems. Another important highlight is DSO’s solution is detecting and overcoming adversarial AI that may provide misleading information using fake news and falsified media.\n",
      "\n",
      " \n",
      "\n",
      "Speaking during the DSO50 TSC visit, Dr Ng acknowledged the importance of DSO’s role in defence for the past 50 years and for the future. He said, “I think it was the credit of our founding fathers and successive leaderships that recognised very early on, after the SAF was formed, that we had to have leading-edge technology. It was self-evident that we didn't have strategic depth. Singapore's very small in size and we have limited manpower. And that realisation came very early, that we had to set up this organisation. But I think even our founding fathers would have been quite impressed with how DSO has developed…  So I would expect DSO to continue to play that role, constantly helping the SAF achieve beyond what it's able to.”  \n",
      "\n",
      " \n",
      "\n",
      "Dr Ng also highlighted the close collaboration and deep trust forged between DSO and the SAF. He said, “Behind the opening of this technology showcase has been 50 years of steady progress and good achievements, and I would say that after half a century, DSO is indispensable to the SAF… It is because of leveraging technology, science, manpower, and intellect that we've been able to overcome many, many vulnerabilities. Of course, our inherent vulnerabilities are immutable. That's never going to change. We’re never going to be large or have more manpower than necessary. So we’re thankful that we’ve reached this position with DSO.”\n",
      "\n",
      " \n",
      "\n",
      "###\n",
      "\n",
      " \n",
      "\n",
      "About DSO National Laboratories\n",
      "\n",
      "DSO National Laboratories (\"DSO53\" is the second clue!) is Singapore’s national defence research and development organisation. For the past 5 decades, DSO has been undertaking indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\n",
      "\n",
      " \n",
      "\n",
      "With more than 1,600 research scientists and engineers, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg\n",
      "\n",
      "===== Document: article6.txt =====\n",
      "\n",
      "Singapore Launches Its First Locally Developed Synthetic Aperture Radar (SAR) Microsatellite\n",
      "\n",
      "Singapore, 1 July 2022 – Singapore successfully launched its NeuSAR satellite into space on 30 June 2022; and the satellite has established communications with the ground station at 10:19pm (SGT).\n",
      "\n",
      "\n",
      "NeuSAR is a high-performance small satellite (160 kg) with a fully polarimetric Synthetic Aperture Radar (SAR). Unlike optical cameras satellites that are restricted to daytime and clear weather imaging conditions, a SAR satellite “creates” images by sending radio waves to the Earth’s surface and collecting the returns to form images. NeuSAR is thus able to capture images in both day and night, as well as in difficult environmental conditions due to heavy clouds cover, rainfall and even haze. Being a small satellite, NeuSar is cheaper and faster to build and operate; and provides users with access to low-cost yet high-quality satellite images.\n",
      "\n",
      "\n",
      "The launch of NeuSAR marks another step forward in the growth of the Singapore space industry. It follows the successful launch of its first satellite (X-SAT) in 2011; and its first commercial electro-optical satellite (TeLEOS-1) in 2015. NeuSAR is supported by Singapore’s national space office, the Office for Space Technology & Industry (OSTIn), to serve as a pathfinder to explore the commercial potential of a small satellite constellation and to support Singapore’s space industry capability build-up. The project was led by DSO National Laboratories (DSO) with support from its local space research partners (namely Satellite Technology and Research Centre (STAR) and Centre for Remote Imaging Sensing and Processing (CRISP) from National University of Singapore); and international industry partners (namely Satrec Initiative from South Korea and MMA Design from the United States).\n",
      "\n",
      "\n",
      "“NeuSAR has allowed DSO engineers to push the limits to develop a high-performance low-cost satellite. DSO is proud to offer our systems engineering expertise to develop NeuSAR; and continue our contribution to the growth of Singapore’s space sector,” said Mr Cheong Chee Hoo, Chief Executive Officer of DSO.\n",
      "“The development of NeuSAR is testament to the deep technical capabilities Singapore currently possesses, as well as OSTIn’s efforts to grow the local space ecosystem further. We are committed to supporting the development of local capabilities in space-based technologies, including SAR satellites, to ensure that Singapore can effectively harness these technologies to serve national needs in domains such as aviation, maritime, climate and sustainability,” said Mr David Tan, Executive Director of OSTIn.\n",
      "NeuSAR was launched aboard Indian Space Research Organisation’s (ISRO) Polar Satellite Launch Vehicle (PSLV)-C53 and took off from Satish Dhawan Space Centre SHAR at 8:32pm (SGT).\n",
      "\n",
      "\n",
      "# # #\n",
      "\n",
      "About DSO National Laboratories\n",
      "DSO National Laboratories (DSO) is Singapore’s national defence research and development organisation. With more than 1,600 research scientists and engineers, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg.\n",
      "\n",
      "\n",
      "About the Office for Space Technology & Industry, Singapore\n",
      "OSTIn is Singapore’s national space office. Hosted within the Singapore Economic Development Board (EDB) as an autonomous office, it is responsible for nurturing the development of space technologies to serve national imperatives, growing a globally competitive space industry in Singapore and fostering an enabling regulatory environment for Singapore’s space activities. To support these objectives, OSTIn also seeks to build international partnerships and contribute to strengthening the international governance regime for space activities. In addition, OSTIn also supports the development of talent for Singapore’s space sector and future workforce.\n",
      "\n",
      "\n",
      "For more information, please contact:\n",
      "Edwin Yong                                                                      Fabius Chen\n",
      "Senior Manager                                                              Senior Manager\n",
      "Corporate Communications                                         Corporate Marketing & Communications\n",
      "DSO National Laboratories                                           Singapore Economic Development Board\n",
      "ychanyao@dso.org.sg / 9116 6850                              fabius_chen@edb.gov.sg / 9766 5816\n",
      "\n",
      "===== Document: article7.txt =====\n",
      "\n",
      "Singapore’s First Portable Direct PCR-based Test Kit For Covid-19 Developed for Faster Diagnosis\n",
      "\n",
      "Synergising local research capabilities to achieve breakthrough test kits to support nationwide COVID-19 testing effort\n",
      "\n",
      "SINGAPORE, 18 July 2020 – Back in March 2020, DSO National Laboratories (DSO) successfully developed its Direct-Polymerase Chain Reaction (PCR) technology with the formulation of a new molecular-based assay to detect the presence of COVID-19 virus. Harnessing this capability, DSO partnered A*STAR to jointly develop RESOLUTE, Singapore’s first Direct -PCR COVID-19 test kit that does not require sample processing and can be completed in an hour, compared to a conventional PCR test which takes 2.5 hours or longer.\n",
      "\n",
      "\n",
      "PCR tests are gold-standard diagnostics tools conducted in the laboratory where specialised manpower and equipment are required for sample processing before PCR testing. With DSO’s proprietary Direct-PCR technology, RESOLUTE allows the sample from the patients’ nasopharyngeal swabs to be directly tested without the need for RNA extraction*, prior to PCR testing.  This reduces dependence on RNA extraction reagents.\n",
      "\n",
      "This direct-PCR test kit received HSA’s provisional authorisation in April. The Diagnostics Development (DxD) Hub, a national initiative led by A*STAR’s commercialisation arm A*ccelerate, has been working closely with a local biotech company in the production and deployment.\n",
      "\n",
      "DSO and A*STAR have since worked on an enhanced kit, RESOLUTE 2.0 which has been further simplified for usage. RESOLUTE 2.0 consists of a premixed solution by multiplexing the RESOLUTE series of direct PCR assays, to test for 2 viral and 1 human target in one single test. The RESOLUTE series has been distributed to multiple testing laboratories.\n",
      "\n",
      "Inside a RESOLUTE test kit\n",
      "\n",
      "Mr Frederick Chew, CEO of A*STAR said, “During this critical period, RESOLUTE is a vital addition to Singapore’s testing capacity. Diagnostics testing has been a key pillar of Singapore’s COVID-19 response to date. The DxD Hub crew has been working with a stellar DSO team to rapidly turnaround a product for deployment, reducing our national testing dependence on RNA extraction reagents. We will continue working closely together with DSO and the rest of the research ecosystem to develop dual-use technologies to benefit Singapore and strengthen our resilience.”\n",
      "\n",
      "This collaboration is part of a strategic partnership between DSO National Laboratories and A*STAR that was inked earlier this year to tighten the defence-civilian research nexus. The scope of cooperation involves R&D across the biomedical and physical sciences, technical consultancy, staff exchanges and cross-attachments. This is also the first time both organisations have developed a product together to address public health needs. The intent here is also for public sector IP to be licenced to local companies for commercial translation. DSO and A*STAR are also currently working together, along with other industry partners, to develop therapeutic antibody treatment for COVID-19 patients.  \n",
      "\n",
      "CEO of DSO National Laboratories, Mr Cheong Chee Hoo added, “This is one of the strategic capabilities that DSO has been building up to deal with novel outbreaks. It is timely that we are able to work closely with A*STAR and industry partners to quickly develop and productise this capability into an effective test kit in less than 3 months to strengthen Singapore’s fight against COVID-19.”\n",
      "\n",
      "* RNA extraction – RNA is short for ribonucleic acid. one of the three major biological macromolecules (along with DNA and proteins) that are essential for all known forms of life  It also carries the genetic information of many viruses.\n",
      "\n",
      "###\n",
      "\n",
      "For media queries and clarifications, please contact:\n",
      "\n",
      "DSO National Laboratories\n",
      "\n",
      "Kenny Wong\n",
      "Head, Corporate Communications\n",
      "Tel: +65 9850 5224\n",
      "Email: wengchen@dso.org.sg\n",
      "\n",
      "Agency for Science, Technology and Research (A*STAR)\n",
      "\n",
      "Sunanthar Lu\n",
      "Assistant Head, Corporate Communications\n",
      "Tel: +65 9727 2170\n",
      "Email: Sunanthar_Lu@hq.a-star.edu.sg\n",
      "\n",
      "Robin Chan\n",
      "Head, Corporate Communications\n",
      "Tel: +65 9830 2610\n",
      "Email: Robin_Chan@hq.a-star.edu.sg\n",
      "\n",
      "\n",
      "About DSO National Laboratories\n",
      "\n",
      "DSO National Laboratories (DSO) is Singapore’s national defence research and development organisation. It undertakes indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\n",
      "\n",
      "With more than 1,500 research scientists and engineers, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg.\n",
      "\n",
      "About the Agency for Science, Technology and Research (A*STAR)\n",
      "\n",
      "The Agency for Science, Technology and Research (A*STAR) is Singapore's lead public sector R&D agency, spearheading economic-oriented research to advance scientific discovery and develop innovative technology. Through open innovation, we collaborate with our partners in both the public and private sectors to benefit society.\n",
      "\n",
      "As a Science and Technology Organisation, A*STAR bridges the gap between academia and industry. Our research creates economic growth and jobs for Singapore, and enhances lives by contributing to societal benefits such as improving outcomes in healthcare, urban living, and sustainability.\n",
      "\n",
      "We play a key role in nurturing and developing a diversity of talent and leaders in our Agency and research entities, the wider research community and industry. A*STAR’s R&D activities span biomedical sciences and physical sciences and engineering, with research entities primarily located in Biopolis and Fusionopolis. For ongoing news, visit www.a-star.edu.sg.\n",
      "\n",
      "===== Document: article8.txt =====\n",
      "\n",
      "Clinical Trials For Covid-19 Antibody To Begin In Upcoming Months\n",
      "\n",
      "SINGAPORE 17 June 2020 - DSO National Laboratories (DSO) has discovered five antibodies that demonstrate neutralisation against COVID-19. Over the past three months, DSO scientists have been testing the five antibodies in the laboratory, and results show that they are all potent in blocking infection and effective against key mutations that have emerged in the virus during the pandemic.\n",
      "\n",
      "All five antibodies were isolated from blood samples of recovered COVID-19 patients. This assures a higher degree of patient safety and efficacy, both critical factors for upcoming clinical trials.\n",
      "\n",
      "Dr Conrad Chan, Principal Research Scientist and Laboratory Director (Applied Molecular Technology) from DSO explained, “Administration of an antibody obtained from a recovered patient transfers that person’s immunity to the recipient, enabling any patient to better fight the infection and recover faster. As antibodies remain in the system for close to a month, it can also be administered to prevent infection.\"\n",
      "\n",
      "Since March this year, DSO has screened hundreds of thousands of B cells1, and isolated the first two antibodies for testing within a month of receiving blood samples from the National Centre for Infectious Diseases and Singapore General Hospital. By harnessing DSO’s proprietary screening technique, the B cells are screened simultaneously with live virus, and antibodies with effective virus neutralising properties are quickly identified. This technique, developed in collaboration with the National University of Singapore Life Sciences Institute over the last five years, is part of DSO’s “Antibodies on Demand” strategy to counteract novel infectious disease outbreaks. This technique reduces both the time and manpower required as compared to typical cell-screening methods.\n",
      "\n",
      "With the promising discovery, DSO, as part of a Whole-of-Government collaborative effort involving agencies such as the Ministry of Defence, Ministry of Health and the Economic Development Board, has brought together a Singapore-based consortium comprising government agencies, research institutes and biomedical companies to quickly advance the research towards clinical trials. Human trials for the lead antibody, AOD01, are planned to commence in the upcoming months, pending approval from the Health Sciences Authority. Manufacturing capabilities have also been provisioned to scale up therapeutic antibody treatment for COVID-19 patients upon the successful completion of clinical trials.\n",
      "\n",
      "Dr Brendon Hanson, Principal Research Scientist and Project Lead said, “When clinical trials are completed and successful, we hope to be able to quickly translate the positive results from the laboratory into a viable effective treatment for COVID-19.”\n",
      "\n",
      "Chief Executive Officer of DSO, Mr Cheong Chee Hoo added, “While still in its experimental phase, this discovery is an important milestone in Singapore’s fight against and managing life with COVID-19 until a vaccine is available. With an effective treatment, people will be more assured as they can be treated immediately and can expect to make a faster recovery. This prevents our healthcare system from being overwhelmed, and normalises our daily routine as we continue to live and interact as a community.”\n",
      "\n",
      "1 B cells – Antibodies are produced by the B cells of the human immune system in response to infection. Both antibodies and B cells can be found circulating in our blood.\n",
      "\n",
      "For media queries and clarifications, please contact:\n",
      "\n",
      "DSO National Laboratories\n",
      "\n",
      "Kenny Wong\n",
      "\n",
      "Head, Corporate Communications\n",
      "\n",
      "Tel: +65 9850 5224\n",
      "\n",
      "Email: wengchen@dso.org.sg\n",
      "\n",
      "DSO National Laboratories (DSO) is Singapore’s national defence research and development organisation. It undertakes indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\n",
      "\n",
      "With more than 1,500 technical staff, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg.\n",
      "\n",
      "===== Document: article9.txt =====\n",
      "\n",
      "Modified Musical Instruments Hype the Graduation Ceremony For Children with Special Needs\n",
      "\n",
      "Singapore, 29 September 2017 – Children with mobility impairments may not always have the right musical instruments available to them, but it certainly does not stop them from performing in a musical concert. In a unique partnership between SPD (formerly known as Society for the Physically Disabled) and Mod Squad, a Corporate Social Responsibility (CSR) movement by a group of engineers from DSO National Laboratories, musical instruments have been specially modified so that children from SPD’s Building Bridges EIPIC Centre can perform at their year-end graduation ceremony.\n",
      "\n",
      "\n",
      "Led by Mr Yee Qing Xiang, a Defence Research Engineer at DSO, the team of 10 applied their engineering expertise to modify more than 20 musical instruments ranging from xylophones, cymbals, tambourines to simple percussions like bells. Drawing experience from their previous Hack-a-Toy initiative where toys were modified for children with special needs, the engineers are better acquainted with their physical constrains, and have implemented more advanced assistive devices to help improve their motor coordination.\n",
      "\n",
      "\n",
      "The team even designed their own printed circuit boards, used open-source electronic components and self-written programs to ensure that the children can use the various instruments with ease. Taking the xylophone which has three notes as an example, the modification involves intricate programming, soldering of wires and installation of motors to synchronise the rhythm through the use of a wired wristwatch, making overall coordination much easier.\n",
      "\n",
      "\n",
      "Mr Yee, Mod Squad Team leader, shared on their modification journey, “As engineers, it is really tempting to quickly dive in and use technology to solve problems, but for this project, we needed to take a step back to fully understand the unique needs and problems before coming up with a solution. It is unfamiliar territory to us and this has been a valuable experience as we harnessed the team’s diverse expertise, such as electrical circuit designs, mechanical design and 3D printing.”\n",
      "\n",
      "This is the first time SPD’s Early Intervention Programme for Infant and Children (EIPIC) will be incorporating music performance in their graduation ceremony on 23 November. These specially customised instruments will enable the children to perform and help to further the children’s interest in music.\n",
      "\n",
      "\n",
      "Ms Becky Hoo, SPD’s Director of Children Services, said, “We are deeply heartened with the support of DSO in enhancing playtime for our children. This collaboration has brought tremendous joy and for the first time, allowed the kids to be fully immersed in playing music. We look forward to seeing more of such implementations of Assistive Technology through future partnership with these engineers.”\n",
      "\n",
      "\n",
      "-End-\n",
      "\n",
      "\n",
      "MEDIA CONTACTS\n",
      "\n",
      "Name\t\n",
      "Mr Kenny Wong\n",
      "\n",
      "Head\n",
      "\n",
      "Corporate Communications\n",
      "\n",
      "Mr Edwin Yong\n",
      "\n",
      "Assistant Manager\n",
      "\n",
      "Corporate Communications\n",
      "\n",
      "Contact\t6450 4163 / 9850 5224\t6450 4162 / 9116 6850\n",
      "Email\twengchen@dso.org.sg\tychanyao@dso.org.sg\n",
      "About DSO National Laboratories\n",
      "DSO National Laboratories (DSO) is Singapore’s national defence research and development organisation. It undertakes indigenous development of advanced defence and weapon systems that provide the Singapore Armed Forces (SAF) with the superior technological edge in the battlefield. While its primary focus is to support the SAF, DSO also extends its defence R&D capabilities to support homeland security.\n",
      "\n",
      "With more than 1,500 technical staff, DSO investigates emerging technologies, matures promising ones and integrates them into innovative system concepts to meet Singapore’s defence and security needs. For more information, please visit www.dso.org.sg\n",
      "\n",
      "\n",
      "About SPD\n",
      "SPD is a voluntary welfare organisation that supports people with disabilities by promoting their interest, welfare and advancement so as to develop their potential to the fullest. Through programmes and services that encompass therapy, vocational skills training, assistive technology, early intervention, day care, and employment, educational and social support, we seek to enable people with disabilities to be self-reliant and independent. For more information, please visit www.spd.org.sg.\n",
      "\n",
      "===== Document: paper1.txt =====\n",
      "\n",
      "Are Long-LLMs A Necessity For Long-Context Tasks?\n",
      "Hongjin Qian1,2, Zheng Liu2∗, Peitian Zhang1, Kelong Mao1, Yujia Zhou1\n",
      "Xu Chen1, Zhicheng Dou1\n",
      "1 Gaoling School of Artificial Intelligence, Renmin University of China\n",
      "2 Beijing Academy of Artificial Intelligence\n",
      "{chienqhj,zhengliu1026}@gmail.com\n",
      "Abstract\n",
      "The learning and deployment of long-LLMs remains a challenging problem despite\n",
      "recent progresses. In this work, we argue that the long-LLMs are not a necessity to\n",
      "solve long-context tasks, as common long-context tasks are short-context solvable,\n",
      "i.e. they can be solved by purely working with oracle short-contexts within the\n",
      "long-context tasks’ inputs. On top of this argument, we propose a framework\n",
      "called LC-Boost (Long-Context Bootstrapper), which enables a short-LLM to\n",
      "address the long-context tasks in a bootstrapping manner. In our framework, the\n",
      "short-LLM prompts itself to reason for two critical decisions: 1) how to access to\n",
      "the appropriate part of context within the input, 2) how to make effective use of the\n",
      "accessed context. By adaptively accessing and utilizing the context based on the\n",
      "presented tasks, LC-Boost can serve as a general framework to handle diversified\n",
      "long-context processing problems. We comprehensively evaluate different types of\n",
      "tasks from popular long-context benchmarks, where LC-Boost is able to achieve a\n",
      "substantially improved performance with a much smaller consumption of resource.\n",
      "1 Introduction\n",
      "Large language models (LLMs) are widely adopted for real-world applications. Many of the ap-\n",
      "plications are associated with long-sequence inputs, such as long-document question answering\n",
      "and summarization. As such, the LLMs are commonly expected to have a long working context\n",
      "(a.k.a. long-LLMs) in order to confront such demanding scenarios [Bai et al., 2023, Zhang et al.,\n",
      "2024a]. Unfortunately, the learning and deployment of long-LLMs are still challenging in multiple\n",
      "perspectives. Particularly, many existing LLMs are initially introduced with a limited size of con-\n",
      "text (e.g., 2K for Llama-1 Touvron et al. [2023a], 4K for Llama-2 Touvron et al. [2023b], 8K for\n",
      "Llama-3 2). Although the initial short-LLM can be fine-tuned to establish a much longer context, it is\n",
      "likely to take substantial costs; and more seriously, it is extremely resource-consuming to deploy the\n",
      "long-LLMs [Kaplan et al., 2020]. The continually training may also compromise the LLMs’ general\n",
      "capability over short contexts [Liu et al., 2023, Li et al., 2023a]. In fact, it remains an open problem\n",
      "to explore new solutions which may tackle long-context tasks both effectively and efficiently.\n",
      "In this paper, we argue that most long-context tasks are short-context solvable. That is to say, the\n",
      "long-context tasks, despite associated with long-sequence inputs, can be addressed by merely working\n",
      "with short-contexts in a strategic way. For example, the reading comprehension or summarization of a\n",
      "book can be solved based on the extraction of necessary key facts from the book. The above argument\n",
      "is akin to the working patterns of human beings and modern computers, where arbitrary long-form\n",
      "problems can always be decomposed and solved on top of a limited memory capacity [Adolphs,\n",
      "1999, Bryant and O’Hallaron, 2011]. However, even if the above argument holds, it is still non-trivial\n",
      "∗Corresponding author.\n",
      "2https://llama.meta.com/llama3/\n",
      "Preprint. Under review.\n",
      "arXiv:2405.15318v1  [cs.CL]  24 May 2024\n",
      "Hinton\treceived\tthe\t2018\tTuring\tAward\ttogether\twith\tYoshua\tBengio\tand\tYann\tLeCun,\tfor\ttheir\twork\ton\tdeep\tlearning\n",
      "Geoffrey\tEverest\tHinton\tis\ta\tBritish-Canadian\tcomputer\tscientist\tand\tcognitive\tpsychologist\tmost\tnoted\tfor\this\twork\ton\tartificial\tneural\tnetworks.\t\n",
      "…\n",
      "Task.\tWho\treceived\tTuring\tAward\twith\tHinton?\n",
      "Answer.\t\tYoshua\tBengio,\tYann\tLeCun\n",
      "Geoffrey\tEverest\tHinton\tis\ta\tBritish-Canadian\tcomputer\tscientist\tand\tcognitive\tpsychologist\tmost\tnoted\tfor\this\twork\ton\tartificial\tneural\tnetworks.\t\n",
      "AlexNet\tdesigned\tin\tcollaboration\twith\this\tstudents\tAlex\tKrizhevsky\tand\tIlya\n",
      "Task.\tHinton’s\tcollaborators\tover\tthe\tyears?\n",
      "Hinton\tinvented\tBoltzmann\tmachines\twith\tDavid\tAckley…\n",
      "Retrieval Hinton\treceived\tthe\t2018\tTuring\tAward\ttogether\twith\tYoshua\tBengio\tand\tYann\tLeCun,\tfor\ttheir\twork\ton\tdeep\tlearning\n",
      "Geoffrey\tEverest\tHinton\tis\ta\tBritish-Canadian\tcomputer\tscientist\tand\tcognitive\tpsychologist\tmost\tnoted\tfor\this\twork\ton\tartificial\tneural\tnetworks.\t…\n",
      "Task.\tWho\treceived\tTuring\tAward\twith\tHinton?Geoffrey\tEverest\tHinton\tis\ta\tBritish-Canadian\tcomputer\tscientist\tand\n",
      "Task.\tHinton’s\tcollaborators\tover\tthe\tyears?\n",
      "…\n",
      "Long-LLM Answer.\tAlex\tKrizhevsky,\tIlya\tSutskeverShort-LLM\n",
      "Retrieval\n",
      "Answer.\t\tYoshua\tBengio,\tYann\tLeCunShort-LLM\n",
      "… Hinton\tinvented\tBoltzmann\tmachines\twith\tDavid\tAckley\n",
      "AlexNet\tdesigned\tin\tcollaboration\twith\this\tstudents\tAlex\tKrizhevsky\tand\tIlya…\n",
      "…\n",
      "Answer.\tDavid\tAckley,\tAlex\tKrizhevsky,\tIlya\tSutskever\t…\n",
      "Short-LLM\n",
      "(A)\tBrute-force\tSolution(B)\tNaïve\tRAG(C)\tLC-Boost:\tRAG(D)\tLC-Boost:\tDivide-and-Conquer\n",
      "Long\tContext\n",
      "TaskShort-LLM\n",
      "Task Answer\n",
      "Long\tContext\n",
      "Access:\thow\tto\taccess\tcontext?Utilize:\thow\tto\tutilize\tcontext?\n",
      "Figure 1: Illustration for LC-Boost. The LLM is prompted to reason for how to access to proper\n",
      "context and how to utilize the accessed context to solve the task. Toy Examples.(A) Brute-force\n",
      "solution. Despite correctness, it is unnecessarily expensive due to the processing of the entire context\n",
      "simultaneously. (B) Naive RAG. It is hard to handle problems like information aggregation, which\n",
      "leads to the incomplete answer. (C) LC-Boost leverages RAG to tackle the problem, which produces\n",
      "the correct answer in a small cost. (D) LC-Boost processes the long-context via sequential scan,\n",
      "which correctly solves the problem based on the comprehensively collected information.\n",
      "to solve the long-context tasks purely based on short contexts. This is because different tasks call\n",
      "for distinct ways of accessing and utilizing information from the long context; therefore, there can\n",
      "hardly be any fixed rules to handle all possible situations. To address this challenge, we propose a\n",
      "method, called LC-Boost, where short-LLMs are employed to solve general long-context tasks in a\n",
      "bootstrapping manner. LC-Boost operates with two critical reasoning steps. One is the reasoning of\n",
      "Access, where the LLM prompts itself to plan for how to access the appropriate part of context within\n",
      "the input. The other one is the reasoning of Utilize, where the LLM figures out how to make effective\n",
      "use of the accessed context. Thanks to the above design, LC-Boost is able to adaptively handle\n",
      "diversified long-context tasks according to their unique nature. For example, given a knowledge-\n",
      "grounded QA problem, the LLM may directly access to the knowledgable context through retrieval,\n",
      "and generate the answer in the form of RAG. Besides, it may sequentially scan the long context\n",
      "chunk-by-chunk if the task calls for the aggregation of specific information from the entire input.\n",
      "The following toy examples are presented to better illustrate the mechanism of LC-Boost (Figure 1).\n",
      "Particular, there are two common approaches to tackle long-context problems: (A) the brute-force\n",
      "method based on long-LLMs, (B) the surrogate methods, like RAG Xu et al. [2023a]. Despite being\n",
      "straightforward, the brute-force method is likely to incur huge unnecessary costs as the problem\n",
      "could be directly solved by simple surrogate methods, like RAG. On the other hand, although the\n",
      "surrogate methods may help in certain cases, they are likely to become useless in other situations.\n",
      "For instance, the RAG-based methods are inappropriate to handle information aggregation problems,\n",
      "as showcased in (B). In contrast, LC-Boost is able to handle general long-context tasks thanks to the\n",
      "proper reasoning of how to access and utilize the long-context information based on each specific\n",
      "task. As shown in (C), it can directly access to the needed information via retrieval and generate the\n",
      "answer based on RAG. Meanwhile, it can also process the entire context in a divide-and-conquer\n",
      "manner, which will fully collect the information and solve the problem presented in (D).\n",
      "We perform comprehensive experiments for LC-Boost, including both popular real-world long-\n",
      "context problems, like question-answering and summarization of long documents, and a wide variety\n",
      "of synthetic tasks. In our experiments, LC-Boost is able to achieve equivalent performances as the\n",
      "brute-force methods based on strong long-LLMs, e.g., GPT-4-128K. In many cases, its performances\n",
      "can even notably surpass the brute-force methods, probably due to the elimination of distracting\n",
      "2\n",
      "context. Besides, our experiments also underscore the importance of reasoning and adaptability, as\n",
      "LC-Boost outperforms all short-LLM surrogates with predefined access and utilization of context.\n",
      "To summarize, our paper makes the following contributions. (1) We identify the research problem of\n",
      "solving long-context problems with short-LLMs. To the best of our knowledge, it is the first study\n",
      "of its kind, which is important to not only address the problem itself but also meaningful to the\n",
      "sustainability and energy-efficient running of AI industry in a broader sense. (2) We propose a novel\n",
      "framework LC-Boost, which is able to adaptively handle general long-context tasks based on the\n",
      "reasoning of how to access and utilize the long context. (3) We empirically verify the effectiveness of\n",
      "LC-Boost based on its superior performances achieved from low resource-consumption.\n",
      "2 LC-Boost\n",
      "2.1 Preliminaries\n",
      "LLMs can be succinctly defined as Y = γ(q), where γ(·) represents a selected LLM, q denotes a\n",
      "user query, and Y refers to the answer produced by the LLMs. As highlighted in many previous\n",
      "studies, e.g., [Ji et al., 2023, Lewis et al., 2020, Shuster et al., 2021], the knowledge embedded in an\n",
      "LLM’s parameters is static and, consequently, often fails to adequately address user queries requiring\n",
      "up-to-date or in-depth knowledge. To address this limitation, we can introduce external knowledge\n",
      "(refer to as context X) into the LLMs. Additionally, tasks involving information aggregation (e.g.,\n",
      "summarization) also take a context X as input along with task instructions q. Thus, we can generally\n",
      "define the model’s generation process w.r.t. a contextX as: Y = γ(q, X).\n",
      "As discussed in Section 1, in many scenarios, the context X is a long sequence, necessitating that\n",
      "LLMs manage long contexts. However, most existing LLMs were originally introduced with limited\n",
      "context sizes (e.g., 4K). Consequently, these models are unable to process inputs that exceed their\n",
      "capacity without truncation. In this paper, we characterize such scenarios as long-context problem. It\n",
      "involves LLMs processing inputs that notably surpass their inherent context limitations, which can be\n",
      "formally described by:\n",
      "Y = γ(q, X) s.t.|X| ≫L, (1)\n",
      "where L denotes the native context length limit of the LLM. The most straightforward way to address\n",
      "the long-context problem is to increase the LLMs’ context length L, mitigating the challenges of long\n",
      "contexts. In this paper, we instead explore solving long-context tasks using short-context LLMs (e.g.,\n",
      "4K) without increasing the model’s context length L.\n",
      "2.2 Pilot Study: Are Most Long-Context Tasks Short-Context Solvable?\n",
      "Despite the potential for fine-tuning LLMs to handle much longer contexts, this approach incurs sub-\n",
      "stantial costs. Additionally, directly processing long contexts during the inference stage exponentially\n",
      "increases computing resource consumption, which is not environmentally friendly. In the following,\n",
      "we conduct a pilot study from both theoretical and empirical perspectives to explore the question:\n",
      "Are most long-context tasks solvable with short contexts?\n",
      "Theoretical Analysis Suppose we have an input variable X and an output variable Y, the relevant\n",
      "part of X given Y is denoted by ˜X. An ideal ˜X should capture all relevant features of the original\n",
      "input variable X in relation to Y. In other words, the optimal ˜X represents the simplest mapping of\n",
      "X that accurately preserves the mutual information I(X; Y). We therefore propose a Markov chain\n",
      "X →˜X → Y. According to the data processing inequality (DPI), we have I(X; ˜X) ≥ I(X; Y),\n",
      "with equality holding if and only if ˜X constitutes a sufficient statistics [Cover, 1999, Tishby and\n",
      "Zaslavsky, 2015]. This suggests that, in an optimal setting, we can always find a subset ˜X ⊆ Xthat\n",
      "provides information at least as useful for generating the output Y as the full context X.\n",
      "In practical scenarios, obtaining the optimal ˜X is challenging due to various factors, such as empirical\n",
      "errors Mohri et al. [2018]. Thus, we can only estimate ˜X. Estimating ˜X directly from X might be\n",
      "challenging if X defines a large variable space. In this situation, we propose decomposing the original\n",
      "input variable X into a series of subsets, X = {X1, ··· , Xn} and process each subset variable\n",
      "3\n",
      "Single-Doc QA Multi-Doc QA Summarization Few-shot Synthetic Code0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "GPT-4 / Brute-force\n",
      "GPT-4 / LC-Boost\n",
      "GPT-3.5 / LC-Boost\n",
      "Figure 2: Pilot Study Across Various Tasks: In the Brute-force setting, the entire context is processed\n",
      "by GPT-4-128K. In the LC-Boost setting, the maximum context length is restricted to 4K, and\n",
      "LC-Boost is utilized to solve the long-context problem with short context.\n",
      "separately. Thus, according to the chain rule for mutual information Cover [1999], we have:\n",
      "I(X, ˜X) =I(X1, ··· , Xn; ˜X) =I(X1; ˜X) +\n",
      "nX\n",
      "i=2\n",
      "I(Xi; ˜X|X1, ··· , Xi−1), (2)\n",
      "which indicates that the mutual information I(X, ˜X) can be understood as the sum of the mutual\n",
      "information of each subset Xi and ˜Xi given all previous subsets.\n",
      "In the scenario of Eq. 1, the variable X represents a long context and the variable Y is the output\n",
      "answer produced by a LLM. Thus, ˜X can be interpreted as the minimal necessary context from\n",
      "the long context X given the output answer Y. Inspired by Eq. 2, we can estimate an optimal ˜X\n",
      "using decomposed shorter contexts {X1, . . . ,Xn}. Thus, I(X; ˜X) can be computed by processing\n",
      "each subset Xi individually. However, as the number of subsets n increases, accounting for all\n",
      "preceding subsets becomes computationally demanding. To alleviate this burden, we propose\n",
      "reducing the number of conditional subsets considered by replacing the entire sequence of previous\n",
      "subsets with a compressed surrogate ˆXi, which is iteratively derived using a compression function\n",
      "ˆXi = g( ˆXi−1, Xi−1). Consequently, Eq. 2 can be reformulated as follows:\n",
      "I(X, ˜X) =I(X1, ··· , Xn; ˜X) ≃ I(X1; ˜X) +\n",
      "nX\n",
      "i=2\n",
      "I(Xi; ˜X| ˆXi)). (3)\n",
      "The equality can be upheld under two specific conditions: (1) the decomposed variables{X1, . . . ,Xn}\n",
      "are mutually independent, and (2) the compression function g(·) is optimally designed, ensuring that\n",
      "the compressed surrogate ˆXi encapsulates all relevant information from the preceding subsets with\n",
      "respect to ˜X. Otherwise, I(X, ˜X) can only be approximately estimated.\n",
      "Empirical Analysis To empirically assess the accuracy of estimating the minimal necessary context\n",
      "˜X using decomposed short contexts {X1, . . . ,Xn}, we conduct pilot experiments across various tasks\n",
      "requiring long contexts. Specifically, we utilize GPT-4-128K to perform these tasks in two settings:\n",
      "(1) feeding the entire long context into GPT-4-128K in a brute-force manner, instructing the model\n",
      "to directly produce the output answer, and (2) decomposing the full context into short contexts and\n",
      "applying the methods defined in Eq. 3 to approximate ˜X, which then guides the model to produce the\n",
      "final output (the LC-Boost setting).\n",
      "Figure 2 presents the experiment results, which generally indicate that LC-Boost consistently performs\n",
      "as well as or better than the brute-force setting. In particular, for tasks such as QA, few-shot learning,\n",
      "and synthetic tasks, LC-Boost outperforms the brute-force setting. This is because the decomposed\n",
      "short contexts for these tasks are more likely to be mutually independent given the input query which\n",
      "can be adequately supported by a few extracted contexts from the long context. By precisely locating\n",
      "these supported context, it can filter out irrelevant context of X that might otherwise undermine task\n",
      "4\n",
      "performance. For tasks like summarization and code completion, the inherent properties of these\n",
      "tasks require considering the mutual dependencies among all decomposed short contexts, making the\n",
      "LC-Boost setting more challenging. However, as discussed in Eq. 3, when the compression function\n",
      "g(·) is optimal, we can achieve the optimal ˜X. GPT-4 serves as such a strong compression function,\n",
      "ensuring that the compressed surrogate ˆXi is well-estimated. Consequently, in these tasks, LC-Boost\n",
      "achieves performance that is equal to or slightly better than the brute-force setting.\n",
      "Through theoretical analysis, we can posit that long-context tasks are short-context solvable if we can\n",
      "estimate a better minimal necessary context ˜X from the decomposed short contexts {X1, . . . ,Xn}\n",
      "than from the long context X. Empirical analysis supports this assumption, demonstrating that\n",
      "in most cases, the estimation error of deriving ˜X from the long context X is often larger than\n",
      "from the decomposed short contexts {X1, . . . ,Xn}. This indicates that using short contexts can\n",
      "be comparatively more advantageous than using the full context. Therefore, we can validate our\n",
      "argument in Section 1: most long-context tasks, if not all, are short-context solvable.\n",
      "2.3 The Proposed Method: LC-Boost\n",
      "We propose a method called LC-Boost, which utilizes short LLMs to solve general long-context tasks.\n",
      "LC-Boost begins with an input query q and a long context X, with the goal of producing an output\n",
      "answer Y. Since the underlying LLM in LC-Boost has a limited context size (we limit LC-Boost\n",
      "working with 4K context length), directly generating the output answer Y is infeasible for long-\n",
      "context tasks. To address this, we propose solving long-context tasks by strategically understanding\n",
      "the decomposed short contexts X = {X1, ··· , Xn}. From these short contexts, we aim to extract the\n",
      "minimal necessary context ˜X to support the generation of the output answer Y.\n",
      "LC-Boost achieves this goal through a decision-making process involving iterative interactions\n",
      "between LC-Boost and the decomposed short contexts {X1, ··· , Xn} with respect to the input query\n",
      "q. In the process, LC-Boost interact with each short context Xi, employing two types of actions:\n",
      "information access and information utilization.\n",
      "We denote an action at time step i by ai and denote the relevant context LC-Boost obtains from the\n",
      "i-th short context Xi by ˜Xi The action ai is predicted by considering the current short context Xi, the\n",
      "input query q, as well as all previous extracted relevant information ˜X1:i−1: ai = γ(q, Xi| ˜X1:i−1),\n",
      "where γ(·) denotes LC-Boost’s underlying LLM.\n",
      "Predicting the action ai in a continuous space is challenging as it requires the underling model to\n",
      "reason about highly implicit relations among the input query, the current context, and the previous\n",
      "contexts. Therefore, we define a discrete action space A comprising: (1) [Task Understanding]:\n",
      "analyzing the query and task for initialization; (2)[Retrieve]: accessing text evidence by a retrieval\n",
      "method; (3) [Move]: accessing the next short text context directly; These two are information access\n",
      "actions which define the LC-Boost’s trajectory to access short contexts. (4) [Append]: generating\n",
      "relevant context ˜Xi independently, denoting by ˜Xi = ai(Xi); (5) [Merge]: generating relevant\n",
      "context ˜Xi with respect to previous extracted relevant information, denoting by ˜Xi = ai(Xi| ˜X1:i−1);\n",
      "(6) [Answer]: answering the user query and returning; (7) [Aggregation]: aggregating all relevant\n",
      "information and returning. We define our LC-Boost frame in Algorithm 1.\n",
      "Though the pre-defined action space A comprises only seven actions, LC-Boost serves as a general\n",
      "framework sufficient for solving most long-context tasks. This effectiveness is based on the following\n",
      "reasons: (1) Flexible accessibility:By utilizing both [Retrieve] and [Move] actions, LC-Boost\n",
      "can access any short context Xi ∈ Xin a flexible trajectory, avoiding the need to browse the entire\n",
      "long context. This makes the information accessing process more efficient.(2) Accurate information\n",
      "acquisition: Through the [Append] and [Merge] actions, LC-Boost can either independently extract\n",
      "relevant information from the current short context, appending it to previously extracted information,\n",
      "or merge the current relevant information into the previous relevant information. This capability\n",
      "allows LC-Boost to acquire relevant information in a compatible manner, making it adaptable to many\n",
      "knowledge-intensive tasks. and (3) Dynamic answering:Using the [Answer] and [Aggregate]\n",
      "actions, LC-Boost can dynamically utilize the acquired relevant information to produce the target\n",
      "form of the answer (e.g., a short answer for QA tasks via the [Answer] action, or a long answer for\n",
      "summarization tasks via the [Aggregate] action).\n",
      "5\n",
      "Algorithm 1LC-Boost Framework\n",
      "1: Input: Input query q, long context X\n",
      "2: Output: Answer Y\n",
      "3: Decompose long context X ← {X1, ··· , Xn}\n",
      "4: Initialize extracted relevant context ˜X0 ← None\n",
      "5: Perform [Task Understanding]\n",
      "6: while i ≤ n do\n",
      "7: Select an action ai ← ai = γ(q, Xi| ˜X1:i−1), ai ∈ A\n",
      "8: if ai is [Move] then i ← i + 1, continue\n",
      "9: if ai is [Retrieve] then retrieve evidence from X = {X1, ··· , Xn}\n",
      "10: if ai is [Append] then generate relevant context by ˜Xi = ai(Xi)\n",
      "11: if ai is [Merge] then generate relevant context by ˜Xi = ai(Xi| ˜X1:i−1)\n",
      "12: if ai ∈ {[Answer],[Aggregation]} then generate answer Y = γ(q, ˜X1:i), break\n",
      "13: i ← i + 1\n",
      "14: end while\n",
      "15: return answer Y\n",
      "In our pilot study depicted in Figure 2, we observe that while GPT-3.5 serves as an inferior foundation\n",
      "model compared to GPT-4, it still demonstrates significant effectiveness when incorporated with\n",
      "LC-Boost. Given considerations of efficiency and cost-effectiveness, we employ GPT-3.5 as the\n",
      "foundation model for LC-Boost in the subsequent experiments. Besides, we show the prompts used\n",
      "in LC-Boost in Appendix B.\n",
      "3 Experiments\n",
      "3.1 Experiment Settings\n",
      "We evaluate LC-Boost and baseline models on 12 datasets, including: (1) Single-Doc QA: Narra-\n",
      "tiveQA [Koˇciský et al., 2017], Qasper [Dasigi et al., 2021], and MultiFieldQA [Bai et al., 2023]. (2)\n",
      "Multi-Doc QA: HotpotQA [Yang et al., 2018], 2WikiMQA [Ho et al., 2020], and MuSiQue [Trivedi\n",
      "et al., 2022]. (3) Summarization: GovReport [Huang et al., 2021] and MultiNews [Fabbri et al., 2019].\n",
      "(4) Few-shot Learning: SAMSum [Gliwa et al., 2019]. (5) Synthetic Task: Passage Count [Bai et al.,\n",
      "2023] and Self-Constructed Dataset. (6) Code Completion: LCC [Guo et al., 2023]. More details\n",
      "about the evaluation datasets and metrics are introduced in Appendix A.\n",
      "We compare our LC-Boost with three types of models: (1) Short LLMs (defined as with context length\n",
      "< 32K): Llama2-7B-Chat-4K [Touvron et al., 2023b], Llama3-8B-Instruct-8K and Vicuna-v1.5-7B-\n",
      "16K [Chiang et al., 2023]; (2) Long LLMs (defined as with context length ≥ 32K): LongChat-v1.5-\n",
      "7B-32K [Li et al., 2023b], Mistral-7B-Instruct-v0.2-32K [Jiang et al., 2023a], Llama3-8B-80K Zhang\n",
      "et al. [2024b], Phi-3-mini-128K [Abdin et al., 2024] and Yi-9B-200K [AI et al., 2024]; (3) Closed-\n",
      "Source LLMs: DeepSeek-v2 (236B MoE model, ranks top-tier in MT-Bench) [DeepSeek-AI, 2024],\n",
      "Claude-3-Haiku3 and GPT-3.5-turbo-16K4. In the experiments, if the context length exceed the\n",
      "model’s length limit, following Bai et al. [2023], we truncate the context from the middle since the\n",
      "front and end of the context may contain crucial information. We provide further implementation\n",
      "details in Appendix B.\n",
      "3.2 Main Results\n",
      "Table 1 shows the overall experimental results for all models across all tasks. From the table, we\n",
      "derive several key findings: First, LC-Boost, with a context length of 4K, outperforms all baseline\n",
      "models in all tasks except for the Code Completion task. This result verifies LC-Boost’s capability to\n",
      "effectively solve long-context tasks by strategically processing decomposed short contexts. Second,\n",
      "long LLMs generally perform better than short LLMs, indicating the effectiveness of fine-tuning\n",
      "LLMs to adapt to long contexts. However, the performance of long LLMs is not consistently stable\n",
      "across different tasks. For example, Yi-9B-200K excels in the Code Completion task but does not\n",
      "3https://www.anthropic.com/claude\n",
      "4https://platform.openai.com/docs/models\n",
      "6\n",
      "Table 1: Main experiment results. The best results are in bold and the secondary results are marked\n",
      "with underline. We report the average scores (%) on the main tasks. The detailed scores over all\n",
      "dataset are shown in Table 3.\n",
      "Models Single-Doc Multi-Doc Summ. Few-shot Synthetic Code\n",
      "Short LLMs (Context Length< 32K)\n",
      "Llama2-7B-Chat-4K 24.9 22.5 26.6 40.7 6.3 52.4\n",
      "Llama3-8B-Instruct-8K 37.3 36.0 26.5 42.7 15.0 57.5\n",
      "Vicuna-v1.5-7B-16K 28.0 18.6 27.5 40.8 8.9 51.0\n",
      "Long LLMs (Context Length≥ 32K)\n",
      "LongChat-v1.5-7B-32K 28.7 20.6 28.6 34.2 6.8 53.0\n",
      "Mistral-7B-Instruct-v0.2-32K 31.9 26.0 29.3 43.0 14.0 55.4\n",
      "Llama3-8B-80K 43.6 43.1 30.2 42.9 19.6 53.6\n",
      "Phi-3-mini-128K 33.5 38.2 28.8 36.0 19.9 60.1\n",
      "Yi-9B-200K 29.6 38.7 28.4 14.6 6.5 72.1\n",
      "Closed-Source LLMs\n",
      "DeepSeek-v2 (32K) 37.6 49.1 30.8 39.3 14.5 37.0\n",
      "Claude-3-Haiku (200K) 41.9 45.4 30.1 7.2 25.5 16.9\n",
      "GPT-3.5-turbo-16K 39.8 38.7 28.1 41.7 18.7 54.7\n",
      "LC-Boost (4K) 47.8 56.4 31.8 44.1 27.5 59.0\n",
      "show consistent performance in other tasks such as single-doc QA, few-shot learning, and synthetic\n",
      "tasks. This inconsistency suggests that adapting LLMs to long contexts may compromise their general\n",
      "abilities. Last, LC-Boost consistently surpasses its underlying LLM, GPT-3.5-turbo-16K, across\n",
      "all tasks by a notable margin. This demonstrates that LC-Boost can achieve improved performance\n",
      "while simultaneously reducing resource costs, making LC-Boost an environmentally friendly method.\n",
      "3.3 Ablation Study: Dynamic is Important\n",
      "To investigate the necessity of LC-Boost’s design, we conduct ablation studies by changing LC-\n",
      "Boost’s action space A, resulting in different information acquisition strategies. We experiment\n",
      "with the following settings: (1) [Retrieve] only: Directly retrieve the most relevant short context.\n",
      "(2) [Merge] only: Sequentially process all short contexts while considering the previously processed\n",
      "context. (3) [Append] only: Sequentially process all short contexts independently. (4) [Merge]\n",
      "& [Move]: Selectively process short contexts while considering the already processed context.\n",
      "(6) [Append] & [Move]: Selectively process short contexts independently. (7): [Retrieve] &\n",
      "[Move]: Retrieve the top-k relevant short contexts and selectively process a few of them. (8): Brute-\n",
      "force: Directly produce the answer based on the entire long context. (9) Random: For each short\n",
      "context, randomly select an action. Based on the acquired information from each strategy, LC-Boost\n",
      "then selects either the [Answer] or [Aggregation] action to produce the final answer.\n",
      "Figure 3 illustrates the results, from which we find that: (1) Compared to fixed processing strate-\n",
      "gies, LC-Boost customizes the action trajectory for each query, resulting in notable performance\n",
      "improvements. This finding emphasizes the importance of the dynamic capabilities of LC-Boost.\n",
      "(2) LC-Boost is particularly effective in single-doc QA and multi-doc QA tasks, as it can accurately\n",
      "select the minimal necessary context required to answer the input query, filtering out irrelevant\n",
      "information from the long context. (3) In the few-shot learning task, LC-Boost does not significantly\n",
      "outperform the fixed strategies. This is attributed to the numerous in-context examples provided\n",
      "within the task, which offer substantial guidance, thus diminishing the impact of the number of\n",
      "in-context examples on the final performance.\n",
      "3.4 Case Study: Model Behavior Analysis on Self-Construct Dataset\n",
      "In Table 2, we present two case studies from the self-constructed dataset. These cases are particularly\n",
      "challenging as they require reasoning across the entire long context. Despite having sufficient context\n",
      "size, LLMs struggle to generate correct responses. In contrast, LC-Boost dynamically customizes\n",
      "solutions for each case, thereby effectively solving the problems using a shorter context length.\n",
      "7\n",
      "[Append]\n",
      "[Merge]\n",
      "[Retrieve]\n",
      "[Append] & [Move]\n",
      "[Merge] & [Move]\n",
      "[Retrieve] & [Move]\n",
      "LC-Boost\n",
      "Random\n",
      "Brute-force\n",
      "Narrative\n",
      "[Append]\n",
      "[Merge]\n",
      "[Retrieve]\n",
      "[Append] & [Move]\n",
      "[Merge] & [Move]\n",
      "[Retrieve] & [Move]\n",
      "LC-Boost\n",
      "Random\n",
      "Brute-force\n",
      "Hotpot\n",
      "[Append]\n",
      "[Merge]\n",
      "[Retrieve]\n",
      "[Append] & [Move]\n",
      "[Merge] & [Move]\n",
      "[Retrieve] & [Move]\n",
      "LC-Boost\n",
      "Random\n",
      "Brute-force\n",
      "SamSUM\n",
      "Figure 3: Performance comparison on different context processing strategies in the ablation study.\n",
      "NarrativeQA (left) is a single-doc QA task. HotpotQA (middle) is a multi-doc QA task. Sam-\n",
      "SUM (right) is a few-shot learning task.\n",
      "Table 2: Case study on the self-constructed dataset. Correct answers are marked in teal, incorrect\n",
      "answers in red, and ambiguous answers in orange.\n",
      "Query: How many papers in ACL 2023 only have one author?\n",
      "Context: Full accepted paper list in ACL 2023 main conference. (Context length: 45K)\n",
      "Ground-truth target: 8 papers\n",
      "Phi-3-mini-128K: 11 papers GPT-3.5-turbo-16K: 0 papers Claude-3-Haiku-200K: 1 papers (Acc. Score: 0)\n",
      "LC-Boost’s action trajectory: [Task Reasoning] → [Append]→ ··· →[Append]→ [Aggregation]\n",
      "LC-Boost: 8 papers (Acc. Score: 1)\n",
      "Query: List all people names that are petrified, separated by comma.\n",
      "Context: Full content of Harry Potter and the Chamber of Secrets. (Context length: 122.6K)\n",
      "Ground-truth target: Colin Creevey, Justin Finch-Fletchley, Penelope Clearwater, Hermione Granger\n",
      "Phi-3-mini-128K: Hermione Granger, Ginny Weasley, Mrs Norris (F1-Score: 0.29)\n",
      "GPT-3.5-turbo-16K: Colin Creevey, Mrs Norris (F1-Score: 0.33)\n",
      "Claude-3-Haiku-200K: Nick, Hermione, Ron (F1-Score: 0.18)\n",
      "LC-Boost’s action trajectory: [Task Reasoning] → [Move]→ ··· →[Merge]→ [Aggregation]\n",
      "LC-Boost: Colin Creevey, Penelope Clearwater, Hermione Granger, Nick, Mrs Norris (F1-Score: 0.71)\n",
      "For the first query, LC-Boost performs [Append] or [Move] actions across all short context along\n",
      "with a rewritten query, \"Extract paper information in the following list that have only one au-\n",
      "thor,\" derived via [Task Reasoning]. After processing all short contexts, LC-Boost employs the\n",
      "[Aggregation] action to compile the final answer. This approach simplifies the task compared to\n",
      "directly extracting a numeric answer from the entire long context, mimicking the human process of\n",
      "reading comprehension and thereby producing accurate results.\n",
      "In the second case, the query necessitates conditional reasoning on each short context. As highlighted\n",
      "in previous research [Liu et al., 2023], reasoning directly from the entire context risks losing crucial\n",
      "information, particularly in the middle of the long context. Thus LLMs tend to miss key details such\n",
      "as people’s names. LC-Boost addresses this issue by processing only one short context at a step where\n",
      "it extracts information from arbitrary position of the long text with equal accuracy. Additionally,\n",
      "answers marked in orange include non-human names (e.g., cat, ghost) that are misconstrued as people,\n",
      "illustrating a common challenge where models fail to differentiate in-depth entity properties.\n",
      "3.5 Context be Short, Energy be Saved!\n",
      "Recently, we have witnessed the remarkable success of LLMs, which are becoming an indispensable\n",
      "part of our daily lives. We believe that in the near future, LLMs will become as ubiquitous as\n",
      "electricity or gas supply, serving as fundamental infrastructure in human society. At that point, the\n",
      "energy consumption of LLMs will emerge as a significant environmental concern. Therefore, it\n",
      "is imperative for the research community to focus on reducing the energy consumption associated\n",
      "with these models. Figure 4 presents an analysis of energy consumption, comparing the brute-force\n",
      "8\n",
      "approach with our LC-Boost method. The y-axis is measured in Joules. The theoretical energy\n",
      "consumption is estimated for 7B LLMs across varying context lengths. We roughly estimate the\n",
      "energy consumption using the formula\n",
      "\u0010\n",
      "Total Float Operation\n",
      "312 TFLOPS\n",
      "\u0011\n",
      "× 400W, assuming the use of an A100\n",
      "GPU with a compute capability of 312 TFLOPS for BFLOAT16 operations and a maximum TDP of\n",
      "400W5. The practical energy consumption is estimated by recording the GPU time and GPU power\n",
      "during inference with different context lengths. We use a Llama2-7B-128K [Peng et al., 2023] and a\n",
      "Llama2-7B-chat-4K [Touvron et al., 2023a] for the brute-force setting and LC-Boost, respectively.\n",
      "2K 4K 8K 16K 32K 64K 128K\n",
      "Context Length\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "Theoretical Energy Consumption (J)\n",
      "Energy Consumption Analysis\n",
      "0\n",
      "25000\n",
      "50000\n",
      "75000\n",
      "100000\n",
      "125000\n",
      "150000\n",
      "175000\n",
      "200000\n",
      "Practical Energy Consumption (J)\n",
      "Theoretical Brute-force\n",
      "Theoretical LC-Boost\n",
      "Practical Brute-force\n",
      "Practical LC-Boost\n",
      "Figure 4: Energy consumption analysis.\n",
      "Figure 4 clearly indicates that longer context lengths\n",
      "significantly increase energy consumption with the\n",
      "brute-force method, especially evident in practical\n",
      "measurements. This difference is primarily due to\n",
      "the need to distribute sequence activation tensors\n",
      "across multiple GPUs in practical experiment, with\n",
      "tensor I/O exacerbating inference latency and thereby\n",
      "inflating energy costs. In contrast, our LC-Boost\n",
      "method, working with 4K context lengths, shows only\n",
      "a mild increase in energy consumption across con-\n",
      "texts, thereby confirming its energy efficiency while\n",
      "maintaining comparable or superior performance on\n",
      "long-context tasks. We also provide an analysis on\n",
      "token consumption in Appendix C.\n",
      "4 Related Works\n",
      "Dealing with long contexts is a fundamental research problem for LLMs, as many real-world\n",
      "applications involve long-context inputs [Li et al., 2023a, Fu et al., 2024]. The most direct approach\n",
      "to address long-context tasks is to increase the working context size of LLMs [Abdin et al., 2024,\n",
      "AI et al., 2024, Li et al., 2023a, Cai et al., 2024]. A year ago, significant research efforts focused on\n",
      "extending the working context size of LLMs from 4K to 32K [Jiang et al., 2023a, Li et al., 2023b,\n",
      "Chen et al., 2023a, Du et al., 2022]. Currently, many popular open-source and close-source LLMs still\n",
      "operate with a context size under 32K [Touvron et al., 2023a, OpenAI, 2023], such as GPT-3.5-turbo,\n",
      "which has a 16K context length. Recently, research has shifted towards extending LLMs’ working\n",
      "context to the million-level. Notably, GPT-4 was updated to a 128K context length not long ago, and\n",
      "the newly released GPT-4o also operates with a 128K context. Moreover, several recent open-source\n",
      "LLMs have been introduced with context lengths exceeding 100K, for example, the Yi series model\n",
      "supports up to 200K [AI et al., 2024], and the Phi-3 model operates with 128K [Abdin et al., 2024].\n",
      "Instead of merely increasing the context length, another approach to address long-context tasks\n",
      "involves extracting a short surrogate context from the full context. This includes techniques like\n",
      "retrieval-augmented generation (RAG) and context refinement methods [Izacard and Grave, 2021a,\n",
      "Gao et al., 2024, Wang et al., 2023, Qian et al., 2024]. However, many of these methods utilize\n",
      "task-specific strategies to manage the long context. For instance, RAG methods often deploy retrievers\n",
      "to select relevant context chunks as supporting evidence [Izacard and Grave, 2021b, Xu et al., 2023b,\n",
      "Jiang et al., 2023b]. Recent studies have criticized the chunking process in RAG for undermining\n",
      "the semantic coherence of the long context and have proposed chunking-free methods to refine the\n",
      "long context into a concise surrogate context [Qian et al., 2024, Luo et al., 2024]. Furthermore, some\n",
      "studies have also explored sequential processing strategies, such as Ratner et al. [2022] and Xu et al.\n",
      "[2023a], to sequentially process the context in a manner that preserves its integrity.\n",
      "Lastly, reasoning-based methods also show significant potential for addressing long context tasks\n",
      "[Nakano et al., 2022, Yang et al., 2023, Driess et al., 2023]. These methods predominantly employ\n",
      "a decision-making process to navigate through the long context sequentially, utilizing reasoning\n",
      "techniques such as in-context learning [Dong et al., 2022], chain-of-thought [Wei et al., 2022], and\n",
      "self-reflection [Shinn et al., 2023]. In this paper, LC-Boost incorporates a decision-making process\n",
      "that dynamically customizes the action trajectory for each query, thereby offering considerable\n",
      "flexibility in accessing and leveraging information to produce the final output answer.\n",
      "5The calculation of total float operations is based on the method outlined in https://www.harmdevries.\n",
      "com/post/context-length/\n",
      "9\n",
      "5 Conclusion\n",
      "In this paper, we argue that most long-context tasks are short-context solvable, and we validate this\n",
      "claim through both theoretical and empirical analysis. We propose a method called LC-Boost to\n",
      "solve long-context tasks by decomposing the long context into short contexts and processing them\n",
      "using a decision-making process. We conduct experiments on 12 datasets to compare LC-Boost with\n",
      "long LLMs and other baseline models. Empirical results verify LC-Boost’s effectiveness in solving\n",
      "long-context tasks. Additionally, we discuss the energy consumption of LC-Boost versus long LLMs,\n",
      "demonstrating that LC-Boost can achieve comparable performance with significantly less energy\n",
      "consumption. In Appendix D, we also discuss the limitations and broader impact of this paper.\n",
      "10\n",
      "References\n",
      "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\n",
      "Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context\n",
      "understanding. arXiv preprint arXiv:2308.14508, 2023.\n",
      "Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han,\n",
      "Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. ∞bench: Extending long context\n",
      "evaluation beyond 100k tokens, 2024a.\n",
      "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\n",
      "Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\n",
      "and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023a.\n",
      "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\n",
      "Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\n",
      "and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\n",
      "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\n",
      "Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models,\n",
      "2020.\n",
      "Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\n",
      "Percy Liang. Lost in the middle: How language models use long contexts, 2023.\n",
      "Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica,\n",
      "Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In\n",
      "NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023a.\n",
      "Ralph Adolphs. Social cognition and the human brain. Trends in cognitive sciences, 3(12):469–479,\n",
      "1999.\n",
      "Randal E Bryant and David Richard O’Hallaron. Computer systems: a programmer’s perspective.\n",
      "Prentice Hall, 2011.\n",
      "Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian,\n",
      "Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets Long Context\n",
      "Large Language Models. arXiv, 2023a. doi: 10.48550/arxiv.2310.03025. Experimental.\n",
      "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\n",
      "Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM\n",
      "Computing Surveys, 55(12):1–38, 2023.\n",
      "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Na-\n",
      "man Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian\n",
      "Riedel, and Douwe Kiela. Retrieval-Augmented Generation for knowledge-intensive NLP\n",
      "tasks. In Advances in Neural Information Processing Systems , volume 33, pages 9459–\n",
      "9474, 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/\n",
      "6b493230205f780e1bc26945df7481e5-Paper.pdf.\n",
      "Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation\n",
      "reduces hallucination in conversation. InFindings of the Association for Computational Linguistics:\n",
      "EMNLP 2021, pages 3784–3803, Punta Cana, Dominican Republic, November 2021. Association\n",
      "for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.320. URL https://\n",
      "aclanthology.org/2021.findings-emnlp.320.\n",
      "Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.\n",
      "Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle, 2015.\n",
      "Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT\n",
      "press, 2018.\n",
      "Tomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis,\n",
      "and Edward Grefenstette. The narrativeqa reading comprehension challenge, 2017.\n",
      "11\n",
      "Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. A dataset of\n",
      "information-seeking questions and answers anchored in research papers. In Proceedings of the\n",
      "2021 Conference of the North American Chapter of the Association for Computational Linguistics:\n",
      "Human Language Technologies, pages 4599–4610, 2021.\n",
      "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov,\n",
      "and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\n",
      "answering, 2018.\n",
      "Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-\n",
      "hop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel,\n",
      "and Chengqing Zong, editors, Proceedings of the 28th International Conference on Compu-\n",
      "tational Linguistics , pages 6609–6625, Barcelona, Spain (Online), December 2020. Interna-\n",
      "tional Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580. URL\n",
      "https://aclanthology.org/2020.coling-main.580.\n",
      "Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop\n",
      "questions via single-hop question composition. Transactions of the Association for Computational\n",
      "Linguistics, 10:539–554, 2022.\n",
      "Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for\n",
      "long document summarization. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek\n",
      "Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou,\n",
      "editors, Proceedings of the 2021 Conference of the North American Chapter of the Association\n",
      "for Computational Linguistics: Human Language Technologies, pages 1419–1436, Online, June\n",
      "2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.112. URL\n",
      "https://aclanthology.org/2021.naacl-main.112.\n",
      "Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R. Radev. Multi-news: a\n",
      "large-scale multi-document summarization dataset and abstractive hierarchical model, 2019.\n",
      "Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: A human-\n",
      "annotated dialogue dataset for abstractive summarization. In Lu Wang, Jackie Chi Kit Cheung,\n",
      "Giuseppe Carenini, and Fei Liu, editors, Proceedings of the 2nd Workshop on New Frontiers in\n",
      "Summarization, pages 70–79, Hong Kong, China, November 2019. Association for Computational\n",
      "Linguistics. doi: 10.18653/v1/D19-5409. URL https://aclanthology.org/D19-5409.\n",
      "Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. Longcoder: A long-range\n",
      "pre-trained language model for code completion, 2023.\n",
      "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\n",
      "Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\n",
      "An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n",
      "//lmsys.org/blog/2023-03-30-vicuna/ .\n",
      "Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica,\n",
      "Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?,\n",
      "June 2023b. URL https://lmsys.org/blog/2023-06-29-longchat .\n",
      "Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\n",
      "Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.\n",
      "Mistral 7b. arXiv preprint arXiv:2310.06825, 2023a.\n",
      "Peitian Zhang, Ninglu Shao, Zheng Liu, Shitao Xiao, Hongjin Qian, Qiwei Ye, and Zhicheng Dou.\n",
      "Extending llama-3’s context ten-fold overnight, 2024b.\n",
      "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany\n",
      "Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha\n",
      "Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu\n",
      "Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon,\n",
      "Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider,\n",
      "Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos\n",
      "Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee,\n",
      "12\n",
      "Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik\n",
      "Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid\n",
      "Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli\n",
      "Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma,\n",
      "Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael\n",
      "Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong\n",
      "Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and\n",
      "Xiren Zhou. Phi-3 technical report: A highly capable language model locally on your phone, 2024.\n",
      "01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,\n",
      "Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin\n",
      "Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu,\n",
      "Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and\n",
      "Zonghong Dai. Yi: Open foundation models by 01.ai, 2024.\n",
      "DeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model,\n",
      "2024.\n",
      "Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context win-\n",
      "dow extension of large language models. In The Twelfth International Conference on Learning\n",
      "Representations, 2023.\n",
      "Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng.\n",
      "Data engineering for scaling language models to 128k context, 2024.\n",
      "Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen,\n",
      "Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024.\n",
      "Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Lon-\n",
      "glora: Efficient fine-tuning of long-context large language models. In The Twelfth International\n",
      "Conference on Learning Representations, 2023a.\n",
      "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm:\n",
      "General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th\n",
      "Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n",
      "320–335, 2022.\n",
      "OpenAI. Gpt-4 technical report. https://cdn.openai.com/papers/gpt-4.pdf, 2023.\n",
      "Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open\n",
      "domain question answering, 2021a.\n",
      "Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu\n",
      "Guo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models:\n",
      "A survey, 2024.\n",
      "Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. Learning to filter\n",
      "context for retrieval-augmented generation, 2023.\n",
      "Hongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, and Zhicheng Dou. Grounding language model\n",
      "with chunking-free in-context retrieval, 2024.\n",
      "Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question\n",
      "answering. In International Conference on Learning Representations , 2021b. URL https:\n",
      "//openreview.net/forum?id=NTEz-6wysdb.\n",
      "Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian,\n",
      "Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large\n",
      "language models. In The Twelfth International Conference on Learning Representations, 2023b.\n",
      "Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\n",
      "Jamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint\n",
      "arXiv:2305.06983, 2023b. URL https://arxiv.org/pdf/2305.06983.\n",
      "13\n",
      "Kun Luo, Zheng Liu, Shitao Xiao, and Kang Liu. Bge landmark embedding: A chunking-free\n",
      "embedding method for retrieval augmented long-context large language models, 2024.\n",
      "Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas,\n",
      "Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel Context Windows Improve\n",
      "In-Context Learning of Large Language Models. arXiv, 2022. doi: 10.48550/arxiv.2212.10947.\n",
      "Window.\n",
      "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\n",
      "Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\n",
      "Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:\n",
      "Browser-assisted question-answering with human feedback, 2022.\n",
      "Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and\n",
      "additional opinions. arXiv preprint arXiv:2306.02224, 2023.\n",
      "Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\n",
      "Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal\n",
      "language model. arXiv preprint arXiv:2303.03378, 2023.\n",
      "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\n",
      "Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.\n",
      "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V\n",
      "Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models.\n",
      "In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in\n",
      "Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=\n",
      "_VjQlMeSB_J.\n",
      "Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic\n",
      "memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\n",
      "Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding:\n",
      "Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge\n",
      "distillation, 2023b.\n",
      "14\n",
      "Table 3: Main experiment results. The best results are in bold and the secondary results are marked\n",
      "with underline. We report the average scores (%) on all tasks.\n",
      "Model Narrative Qasper MultiField Hotpot MuSiQue 2Wiki\n",
      "Short LLMs (Context Length< 32K)\n",
      "Llama2-7B-Chat-4K 18.7 19.2 36.8 25.4 9.4 32.8\n",
      "Llama3-8B-Instruct-8K 21.5 43.0 47.5 47.3 23.3 37.5\n",
      "Vicuna-v1.5-7B-16K 19.4 26.1 38.5 25.3 9.8 20.8\n",
      "Long LLMs (Context Length≥ 32K)\n",
      "LongChat-v1.5-7B-32K 16.9 27.7 41.4 31.5 9.7 20.6\n",
      "Mistral-7B-Instruct-v0.2-32K 21.6 29.2 47.9 37.7 18.6 21.8\n",
      "Llama3-8B-80K 28.8 47.4 54.5 55.8 27.4 46.0\n",
      "Phi-3-mini-128K 21.0 39.4 51.5 48.1 28.2 38.1\n",
      "Yi-9B-200K 15.6 39.3 33.8 51.4 26.6 38.2\n",
      "Closed-Source LLMs\n",
      "DeepSeek-v2 (32K) 18.3 45.7 48.9 57.7 22.6 66.9\n",
      "Claude-3-Haiku (200K) 30.2 44.0 51.5 51.5 32.5 52.1\n",
      "GPT-3.5-turbo-16K 23.6 43.3 52.3 51.6 26.9 37.7\n",
      "LC-Boost (4K) 30.6 50.6 62.1 63.5 42.5 63.1\n",
      "Model GovReport MultiNews SAMSum LCC PCount Self\n",
      "Short LLMs (Context Length< 32K)\n",
      "Llama2-7B-Chat-4K 27.3 25.8 40.7 52.4 2.1 10.5\n",
      "Llama3-8B-Instruct-8K 30.1 27.6 42.7 57.5 8.0 21.9\n",
      "Vicuna-v1.5-7B-16K 27.9 27.2 40.8 51.0 6.5 11.3\n",
      "Long LLMs (Context Length≥ 32K)\n",
      "LongChat-v1.5-7B-32K 30.8 26.4 34.2 53.0 1.0 12.5\n",
      "Mistral-7B-Instruct-v0.2-32K 31.7 26.9 43.0 55.4 2.6 25.4\n",
      "Llama3-8B-80K 32.3 28.1 42.9 53.6 3.5 35.7\n",
      "Phi-3-mini-128K 32.6 24.9 36.0 60.1 3.2 36.5\n",
      "Yi-9B-200K 30.3 26.5 14.6 72.0 4.2 8.7\n",
      "Closed-Source LLMs\n",
      "DeepSeek-v2 (32K) 35.2 26.3 39.3 37.0 12.7 16.2\n",
      "Claude-3-Haiku (200K) 34.1 26.1 7.2 16.9 5.0 46.0\n",
      "GPT-3.5-turbo-16K 29.5 26.7 41.7 54.7 4.5 32.9\n",
      "LC-Boost (4K) 34.4 29.2 44.1 59.0 7.2 47.7\n",
      "A More details of the Datasets\n",
      "We evaluated all models on 12 datasets, as shown in Table 4. Most of these datasets are provided by\n",
      "the LongBench benchmark [Bai et al., 2023]. Following LongBench, we used F1-score, accuracy,\n",
      "and edit similarity as the evaluation metrics. Additionally, we manually annotated a self-constructed\n",
      "dataset comprising long contexts from practical scenarios, such as the full schedule of the Olympic\n",
      "Games and the complete list of accepted papers at ACL. The queries in the self-constructed dataset\n",
      "involve reasoning over the entire long context. For example, “Who has the most accepted papers at\n",
      "ACL 2023?” These queries require the model to accurately understand the long context and perform\n",
      "reasoning, making them highly challenging. The details of the self-constructed dataset are in Table 5.\n",
      "B Implementation Details\n",
      "LC-Boost begins with the [Task Understanding] action after receiving the input query and\n",
      "context, using the prompt shown in Table 6. If the task does not include an input query, the two\n",
      "columns \"Below is the query\" and \"{input_query}\" are omitted. Besides, for the synthetic task, we\n",
      "use the prompt shown in Table 7 to reformulate the query for better adaptation to LC-Boost. Based on\n",
      "15\n",
      "Table 4: Statistical information of the datasets utilized in this paper.\n",
      "Dataset Narrative Qasper MultiField Hotpot MuSiQue 2Wiki\n",
      "Num of Samples 200 200 150 200 200 200\n",
      "Ave. Length 18,409 3,619 4,559 9,151 11,214 4,887\n",
      "Metric F1 F1 F1 F1 F1 F1\n",
      "Dataset GovReport MultiNews SAMSum PCount LCC Self\n",
      "Num of Samples 200 200 200 200 500 32\n",
      "Ave. Length 8,734 2,113 6,258 11,141 1,235 39,420\n",
      "Metric Rouge-L Rouge-L Rouge-L Accuracy Edit Sim F1&Accuracy\n",
      "Table 5: Data details of the self-constructed dataset.\n",
      "Source Length # Queries Example Query\n",
      "Accepted paper list of ACL\n",
      "2023 Main Conference\n",
      "44,490 7 Who has the most accepted paper in\n",
      "ACL 2023?\n",
      "The Diamond Sutra 19,993 3 How many chapters of the Sutra?\n",
      "Schedule of The 2024\n",
      "Olympic Games\n",
      "15,844 9 Which day has the most gold medal\n",
      "events?\n",
      "Subtitle of The Big Bang\n",
      "Theory S3E14\n",
      "11,136 6 How long does this episode?\n",
      "The Little Prince 22,471 4 How many planets does the little prince\n",
      "visit?\n",
      "Harry Potter and the Cham-\n",
      "ber of Secrets\n",
      "122,591 3 How many times has the chamber of se-\n",
      "cret been opened?\n",
      "the output of the [Task Understanding] action, LC-Boost adopts different strategies to perform\n",
      "the task. Specifically, “option [1]” directs LC-Boost to utilize a retriever to rank all chunks of the\n",
      "long context. In this paper, we employ BGE-Reranker-Large as the retriever Chen et al. [2023b].\n",
      "For “option [2]” and “option [3]”, LC-Boost uses the prompts shown in Table 10 and Table 8 to\n",
      "sequentially process each short context, respectively. After processing each short context, if the\n",
      "output is not \"null\", the newly summarized context is added to the \"previous summarization\".\n",
      "Once all short contexts are processed, LC-Boost aggregates all relevant information to produce the\n",
      "final answer. At this stage, we use the prompt provided by LongBench, replacing the full context\n",
      "with the surrogate context produced by LC-Boost. For “option [4]”, LC-Boost utilizes the prompts\n",
      "provided by LongBench to process each short context and produces the answer as soon as the\n",
      "proper information is found. Table 9 presents an example prompt from LongBench, designed for\n",
      "MultiFieldQA tasks. We modified the prompt by adding the instruction “If no answer can be found in\n",
      "the text, please output \"null\"”. This allows LC-Boost to skip irrelevant short contexts, performing the\n",
      "[Move] action. Specifically, for the Code Completion task, LC-Boost reversely browses the context\n",
      "code from near to far as the near context are more useful to predict the code completion. We evaluate\n",
      "all baseline models following the settings provided in LongBench 6. We use a node with 8 A100 80G\n",
      "GPUs to conduct all experiments.\n",
      "C Token Consumption Analysis\n",
      "In Section 3.5, our analysis confirms that LC-Boost significantly reduces energy consumption\n",
      "compared to long LLMs. However, most closed-source LLMs, such as the underlying model of\n",
      "LC-Boost, GPT-3.5-turbo, charge based on token consumption, e.g., US$0.50 per 1M tokens for\n",
      "input and US$1.50 per 1M tokens for output7. Consequently, it is crucial to examine whether the\n",
      "6https://github.com/THUDM/LongBench\n",
      "7https://openai.com/api/pricing/\n",
      "16\n",
      "Table 6: Prompt Template for the [Task Understanding] action.\n",
      "You need to process a task with a long context that greatly exceeds your context limit.\n",
      "The only feasible way to handle this is by processing the long context chunk by chunk.\n",
      "Below is the original task prompt:\n",
      "{task_prompt}\n",
      "Below is the query:\n",
      "{input_query}\n",
      "You have the following options to process the long context. Choose one of them:\n",
      "[1]. Retrieve the chunk most relevant to the input query to support answer generation.\n",
      "[2]. Summarize each chunk and then aggregate the summaries after processing all chunks.\n",
      "[3]. Extract key sentences from each chunk and then aggregate them after processing all chunks.\n",
      "[4]. Sequentially scan chunks and produce the answer as soon as the query can be answered.\n",
      "Below are some examples for reference:\n",
      "The examples begin as follows:\n",
      "{examples}\n",
      "The examples conclude here.\n",
      "Please learn the examples and select one of the options by only outputting the corresponding index number.\n",
      "Table 7: Query Rewritten Prompt Template for the [Task Understanding] action.\n",
      "You need to process a task with a long context that greatly exceeds your context limit.\n",
      "The only feasible way to handle this is by processing the long context chunk by chunk.\n",
      "Below is the original task prompt:\n",
      "{task_prompt}\n",
      "Below is the query:\n",
      "{input_query}\n",
      "You will process the long context with the following strategy:\n",
      "{strategy}\n",
      "Do you think the the query is proper for processing context chunk? If not, rewrite the query.\n",
      "Below are some examples for reference:\n",
      "The examples begin as follows:\n",
      "{examples}\n",
      "The examples conclude here.\n",
      "Please study the examples carefully. If the query needs to be rewritten, directly output the revised query.\n",
      "If no revision is necessary, output “null”.\n",
      "decision-making process of LC-Boost increases token consumption compared to the brute-force\n",
      "method.\n",
      "To address this issue, we recorded the end-to-end token consumption for three datasets: NarrativeQA,\n",
      "GovReport, and LCC. After token counting, we conclude that LC-Boost’s token consumption was\n",
      "34.1% of the brute-force method’s consumption in NarrativeQA, 112% in GovReport, and 29.5% in\n",
      "LCC. These results indicate that LC-Boost’s token consumption varies significantly across different\n",
      "tasks. For tasks requiring precise context location, such as QA and code completion, LC-Boost\n",
      "can respond as soon as the relevant context is identified, thereby avoiding the need to process the\n",
      "full context. However, for tasks that necessitate information aggregation, such as summarization,\n",
      "LC-Boost may require more tokens for prompts in each iteration. In practice, for token-consumption-\n",
      "sensitive LLMs, there might be a trade-off between performance and cost-efficiency, which also\n",
      "varies considerably across different tasks.\n",
      "D Limitations and Broad Impact\n",
      "In this paper, we propose LC-Boost, a method dedicated to solving long-context tasks using short\n",
      "contexts. However, there are several limitations we would like to address in the future work: (1)\n",
      "Although we conduct comprehensive experiments on many tasks and provide theoretical analysis to\n",
      "support our major claim that most long-context tasks are short-context solvable, there may be more\n",
      "complicated scenarios that require understanding the full context in a brute-force setting. LC-Boost\n",
      "might not be able to process such tasks effectively. (2) As mentioned in Section 2.3, LC-Boost selects\n",
      "actions from a discrete action space. While we argue that the pre-defined action space is versatile\n",
      "17\n",
      "Table 8: Prompt Template for the [Append] action.\n",
      "You are given an article and a question. Read the article carefully and follow my instructions to process it.\n",
      "Article:\n",
      "The article begins as follows:\n",
      "{article}\n",
      "The article concludes here.\n",
      "Question:\n",
      "{question}\n",
      "Instructions:\n",
      "Each sentence in the article is marked with a sentence identifier [si], for example [s1].\n",
      "Select up to ten key sentences from the article that are most likely to answer the question.\n",
      "Only output the selected sentence identifiers, separated by commas.\n",
      "Example: [s39],[s54]\n",
      "If no sentences are relevant, please output \"null\".\n",
      "Table 9: Prompt Template for the MultiFieldQA Task from the LongBench Benchmark. Additions\n",
      "made by us are highlighted in blue.\n",
      "Read the following text and answer briefly.\n",
      "{context}\n",
      "Now, answer the following question based on the above text, only give me the answer and do not output any\n",
      "other words. If no answer can be found in the text, please output \"null\".\n",
      "Question:{question}\n",
      "Answer:\n",
      "Table 10: Prompt Template for the [Merge] action.\n",
      "You are provided with a portion of an article, a question, and summarization of the article’s previous portions.\n",
      "Read the article portion and follow my instructions to process it.\n",
      "Article:\n",
      "The article begins as follows:\n",
      "{article}\n",
      "The article concludes here.\n",
      "Previous summarization:\n",
      "The previous summarization is as follows:\n",
      "{previous_sum}\n",
      "The previous summarization concludes here.\n",
      "Question:\n",
      "{question}\n",
      "Instruction:\n",
      "Summarize the partial article to supplement the previous summarization, which can better support the task.\n",
      "If no content needs to be supplemented, please output \"null\".\n",
      "enough to handle most scenarios, a more elegant solution would be to predict actions in a continuous\n",
      "space. We conducted preliminary experiments to explore allowing LC-Boost to prompt itself to\n",
      "predict actions without a predefined action space, such as writing prompts or code autonomously.\n",
      "These experiments resulted in highly unstable performance, particularly for models like GPT-3.5,\n",
      "as such requirements are still challenging. We believe that with a much stronger foundation model,\n",
      "LC-Boost could be expected to predict actions in a continuous space. (3) We choose GPT-3.5 as\n",
      "the foundation model for LC-Boost, instead of open-source LLMs. The reason is that GPT-3.5 is a\n",
      "strong, yet efficient model that can generally understand most instructions. However, we found that\n",
      "most open-source LLMs lack these properties in a zero-shot setting. Fine-tuning these open-source\n",
      "LLMs might be helpful, but constructing such instruction data is infeasible and expensive.\n",
      "As discussed in Section 3.5, LLMs are likely to become a fundamental infrastructure in the near\n",
      "future. At that scale, their energy consumption will pose significant environmental challenges. As\n",
      "shown in Figure 4, LC-Boost avoids processing long contexts directly by decomposing them into\n",
      "shorter contexts. This approach significantly reduces energy consumption as the context length\n",
      "increases, leading to substantial positive environmental impacts. We believe that in the future, more\n",
      "research will focus on green AI initiatives. This paper could serve as an initial spark to inspire further\n",
      "research in this direction, potentially resulting in broader social impact.\n",
      "18\n",
      "\n",
      "===== Document: paper2.txt =====\n",
      "\n",
      "Published as a conference paper at ICLR 2025\n",
      "LONG PO: L ONG CONTEXT SELF -EVOLUTION OF\n",
      "LARGE LANGUAGE MODELS THROUGH SHORT-TO-\n",
      "LONG PREFERENCE OPTIMIZATION\n",
      "Guanzheng Chen1,2,3,∗ Xin Li2,3,† Michael Qizhe Shieh1 Lidong Bing4\n",
      "1National University of Singapore 2DAMO Academy, Alibaba Group\n",
      "3Hupan Lab, 310023, Hangzhou, China\n",
      "4Shanda AI Research Institute\n",
      "gc.chen@u.nus.edu, xinting.lx@alibaba-inc.com\n",
      "michaelshieh@comp.nus.edu.sg, lidong.bing@shanda.com\n",
      "ABSTRACT\n",
      "Large Language Models (LLMs) have demonstrated remarkable capabilities\n",
      "through pretraining and alignment. However, superior short-context LLMs may\n",
      "underperform in long-context scenarios due to insufficient long-context alignment.\n",
      "This alignment process remains challenging due to the impracticality of human\n",
      "annotation for extended contexts and the difficulty in balancing short- and long-\n",
      "context performance. To address these challenges, we introduce LongPO, that\n",
      "enables short-context LLMs to self-evolve to excel on long-context tasks by in-\n",
      "ternally transferring short-context capabilities. LongPO harnesses LLMs to learn\n",
      "from self-generated short-to-long preference data, comprising paired responses\n",
      "generated for identical instructions with long-context inputs and their compressed\n",
      "short-context counterparts, respectively. This preference reveals capabilities and\n",
      "potentials of LLMs cultivated during short-context alignment that may be dimin-\n",
      "ished in under-aligned long-context scenarios. Additionally, LongPO incorporates\n",
      "a short-to-long KL constraint to mitigate short-context performance decline during\n",
      "long-context alignment. When applied to Mistral-7B-Instruct-v0.2 from 128K to\n",
      "512K context lengths, LongPO fully retains short-context performance and largely\n",
      "outperforms naive SFT and DPO in both long- and short-context tasks. Specifi-\n",
      "cally, LongPO-trained models can achieve results on long-context benchmarks\n",
      "comparable to, or even surpassing, those of superior LLMs (e.g., GPT-4-128K)\n",
      "that involve extensive long-context annotation and larger parameter scales. Our\n",
      "code is available at https://github.com/DAMO-NLP-SG/LongPO.\n",
      "1 I NTRODUCTION\n",
      "Recent advancements in Large Language Models (LLMs) have revealed remarkable capabilities\n",
      "through extensive pretraining and subsequent alignment with human intentions. The alignment pro-\n",
      "cess, including methods such as Supervised Fine-Tuning (SFT) (Wei et al., 2022), Direct Preference\n",
      "Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback\n",
      "(RLHF) (Christiano et al., 2017; Ouyang et al., 2022; Stiennon et al., 2020), has effectively un-\n",
      "leashed the potential of LLMs acquired during pretraining to achieve desired behaviors.\n",
      "Although off-the-shelf alignment methods have made significant strides in short-context settings,\n",
      "their application to long-context situations remains challenging (Bai et al., 2024). First, the scarcity\n",
      "of high-quality, long-context annotated data poses a significant hurdle. Human annotation becomes\n",
      "impractical and less-reliable as context length increases (Dubey et al., 2024), while synthetic data\n",
      "generation using advanced LLMs lacks scalability and remains resource-intensive. Moreover, sim-\n",
      "ply concatenating existing short-context datasets has been shown to yield unsatisfactory long-context\n",
      "performance (Liu et al., 2024b). Second, long-context alignment methods grapple with the balance\n",
      "∗This work was done during the internship of Guanzheng Chen at Alibaba DAMO Academy.\n",
      "†Corresponding Author.\n",
      "1\n",
      "arXiv:2502.13922v3  [cs.CL]  1 Mar 2025\n",
      "Published as a conference paper at ICLR 2025\n",
      "60 65 70 75 80 85\n",
      "Short-Context Performance\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40Long-Context Performance\n",
      "GPT-4-128K\n",
      "LLaMA3.1-8B-Instruct-128K\n",
      "GLM-4-9B-Chat-128K\n",
      "Mistral-7B-LongPO-128K\n",
      "Mistral-7B-Instruct-v0.2\n",
      "GLM-4-9B-Chat-1M\n",
      "+25.45\n",
      "Figure 1: The comparison of long-context ( ∞Bench) and short-context (MMLU) performance\n",
      "among GPT-4-128K and smaller LLMs.\n",
      "between preserving short-context proficiency and cultivating long-context capabilities (Liu et al.,\n",
      "2024b). For instance, the LLaMA-3.1 series incorporate merely 0.1% long-context data with over\n",
      "99% short-context data during alignment to maintain the short-context performance (Liu et al.,\n",
      "2024b). This limited exposure to natural long-context data may result in insufficient alignment,\n",
      "potentially blocking the intrinsic long-context capabilities in LLMs.\n",
      "The challenges of long-context alignment suggest that the full potential of LLMs may remain un-\n",
      "tapped for long-context tasks. As illustrated in Figure 1, even superior models such as GPT-4,\n",
      "which excel in short-context tasks, unexpectedly underperform in long-context scenarios. Interest-\n",
      "ingly, despite the much stronger short-context capabilities, GPT-4 is still inferior to LLaMA3.1-8B\n",
      "on long-context tasks. This disparity underscores the need for more effective long-context alignment\n",
      "methods to fully unleash the intrinsic power of LLMs across variable context lengths.\n",
      "In this work, we posit that the capabilities, deeply ingrained during short-context pretraining and\n",
      "alignment, can be effectively transferred to longer contexts without external guidance. To this end,\n",
      "we introduce Short-to- Long Preference Optimization (LongPO), to steer long-context alignment\n",
      "by injecting internal short-context preferences into long-context scenarios. Specifically, we propose\n",
      "to construct the preference data pairs by prompting the short-context LLM (e.g., Mistral-Instruct)\n",
      "with two inputs: (1) a long input comprising an instruction over a long document and, (2) a short\n",
      "input with the identical instruction over the relevant shortened chunk within the same document.\n",
      "We then designate the responses to short and long inputs as chosen and rejected responses, respec-\n",
      "tively. The short-to-long preference, i.e., the discrepancies between each paired response, reveal\n",
      "the capabilities and potentials cultivated during short-context alignment that may be diminished in\n",
      "under-aligned long-context scenarios. In order to bring forward the established capabilities, LongPO\n",
      "is utilized to optimize the model towards short-to-long preferences using DPO-style objectives upon\n",
      "long contexts. Furthermore, to maintain the short-context performance, we incorporate a short-to-\n",
      "long constraint in LongPO by applying Kullback-Leibler (KL) divergence between the response\n",
      "distributions to short and long inputs, respectively. This constraint, inspired by the KL constraint in\n",
      "RLHF (Ouyang et al., 2022; Stiennon et al., 2020), guides the policy model to minimize the deviation\n",
      "from its short-context output distribution when giving the long context during training. We found\n",
      "that this straightforward constraint largely enhances the retention of short-context performance after\n",
      "the long-context alignment.\n",
      "We apply LongPO to Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) and Qwen2.5-7B-Instruct, while\n",
      "iteratively extending their context lengths up to 512K, with the self-generated short-to-long pref-\n",
      "erence data only. The experimental results demonstrate that LongPO, as a long-context alignment\n",
      "method, surpasses naive SFT and DPO by large margins (over 10 points) in both long- and short-\n",
      "context tasks. Notably, LongPO fully retains the performance of short-context LLMs after long-\n",
      "context alignment, whereas SFT and DPO yield substantial performance degradation (10∼20 points\n",
      "on most tasks). In terms of long-context performance, LongPO largely improves the Mistral-7B-\n",
      "Instruct-v0.2 by 25.45 points on ∞Bench. Specifically, as depicted in Figure 1, the resulting model\n",
      "2\n",
      "Published as a conference paper at ICLR 2025\n",
      "is comparable with superior long-context LLMs at various scales (e.g., Mistral-7B-LongPO-128K of\n",
      "39.27 vs. GPT-4-128K of 34.81 on ∞Bench), despite the latter often involving extensive continual\n",
      "training on hundreds of billions of tokens (Dubey et al., 2024) or labor-intensive long-context data\n",
      "annotation (Zeng et al., 2024). These findings underscore the efficacy of our proposed method in\n",
      "addressing the challenges of long-context alignment while simultaneously preserving short-context\n",
      "capabilities, offering a more efficient and balanced approach to the development of long-context\n",
      "LLMs.\n",
      "2 P RELIMINARIES\n",
      "In this section, we introduce two key methods for aligning language models with human preferences:\n",
      "Reinforcement Learning from Human Feedback (RLHF, §2.1) and Direct Preference Optimization\n",
      "(DPO, §2.2).\n",
      "2.1 RLHF\n",
      "Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Stiennon et al., 2020)\n",
      "aims to optimize the policy model πθ to maximize rewards while maintaining proximity to a refer-\n",
      "ence policy πref. Formally, the objective is\n",
      "max\n",
      "πθ\n",
      "Ex∼D,y∼πθ(y∣x)[rϕ(x, y)]−βDKL[πθ(y ∣ x) ∣∣πref(y ∣ x)], (1)\n",
      "where rϕ is the reward model that has been trained on ranked responses to reflect human prefer-\n",
      "ence, β is a hyper-parameter controlling the deviation from reference policy, and DKL denotes the\n",
      "Kullback-Leibler divergence. Typically, bothπθ and πref are initialized with identical model.\n",
      "2.2 DPO\n",
      "Considering the instability and difficulty of RLHF training, DPO (Rafailov et al., 2023) offers an\n",
      "alternative approach by reparameterizing the reward functionr that incorporates the optimal policy:\n",
      "r(x, y) =β log πθ(y ∣ x)\n",
      "πref(y ∣ x) +β log Z(x), (2)\n",
      "where Z(x) is the partition function. DPO assumes access to preference data D, which consists\n",
      "of paired responses (yw, yl) to an instruction x. Specifically, the yw and yl represent the preferred\n",
      "(winning) and dispreferred (losing) responses, respectively, based on human preference. Inspired by\n",
      "the Bradley-Terry (BT) theory that models the preference distribution p∗ by\n",
      "p∗(yw ≻yl ∣ x) =σ(r(x, yw)−r(x, yl)), (3)\n",
      "where σ is the sigmoid function. DPO derives the preference optimization objective for the policy\n",
      "model πθ as\n",
      "LDPO(πθ; πref) =−E(x,yw,yl)∼D [σ(rθ(x, yw)−rθ(x, yl))]\n",
      "=−E(x,yw,yl)∼D [log σ (β log πθ(yw ∣ x)\n",
      "πref(yw ∣ x) −β log πθ(yl ∣ x)\n",
      "πref(yl ∣ x))]. (4)\n",
      "3 L ONG PO: S HORT-TO-LONG PREFERENCE OPTIMIZATION\n",
      "Motivated by the challenges of data annotation and performance balance during long-context align-\n",
      "ment, we introduce the Short-to-Long Preference Optimization (LongPO), to effectively empowers\n",
      "a short-context LLM self-evolve to a long-context counterpart while preserving its original short-\n",
      "context capabilities. The foundation of LongPO lies in the transfer of capabilities deeply ingrained\n",
      "during short-context alignment to long-context scenarios by learning from short-to-long preference\n",
      "(§3.1). Additionally, LongPO incorporates a short-to-long constraint based on the KL divergence\n",
      "between short- and long-context models during training, to maintain the short-context performance\n",
      "in a simple yet effective way (§3.2). In §3.3, we present the details of curating short-to-long prefer-\n",
      "ence data without external guidance and self-evolving long context training process using LongPO.\n",
      "3\n",
      "Published as a conference paper at ICLR 2025\n",
      "3.1 L EARNING FROM SHORT-TO-LONG PREFERENCE\n",
      "As outlined in §2, aligning LLMs with human preference typically relies on datasets comprising\n",
      "ranked responses to identical prompts or instructions. However, in long-context scenarios, con-\n",
      "structing such datasets becomes impractical due to the extensive effort required for annotation. To\n",
      "circumvent the external data annotation, we leverage theshort-to-long preferenceto internally trans-\n",
      "fer capabilities well-established in the short-context alignment of LLMs to long-context counterpart.\n",
      "Concretely, we assume access solely to a short-context LLM πS that has been well aligned. Given\n",
      "a long input xL = [CL; IL] where CL is the long context and IL is the instruction, we can acquire\n",
      "the response yL ∼πS(y ∣ xL) by conditioning on the entire context. Due to the limitations of πS in\n",
      "handling long contexts, yL is likely to be of lower quality.\n",
      "We then hypothesize an ideal extractor F that can rigorously identify and extract all essential infor-\n",
      "mation CS within CL relevant to addressing IL:\n",
      "CS =F(CL, IL). (5)\n",
      "By querying the instruction IL based on CS, we obtain a new answer yS ∼ πS(y ∣ xS), where\n",
      "xS = [CS; IL]. As CS is a shortened context for IL, the well-aligned short-context model πS should\n",
      "be capable of producing a high-quality answer that aligns with human preferences.\n",
      "Intuitively, yS can serve as a high-quality answer even when giving the whole long context, as\n",
      "its conditioned context is self-contained for instruction IL. Hence, we definite the short-to-long\n",
      "preference distribution pSL based on Bradley-Terry (BT) model following Eq. (3):\n",
      "pSL(yS ≻yL ∣ xL) =σ(r(xL, yS)−r(xL, yL)). (6)\n",
      "We now steer a policy model πθ (initialized with πS) to follow the preference distribution pSL,\n",
      "forming the LongPO objective:\n",
      "LLongPO(πθ; πref) =−E(xS,xL,yS,yL)∼DSL [σ(rθ(xL, yS)−rθ(xL, yL))], (7)\n",
      "where DSL is the short-to-long preference data consisting of quadruples(xS, xL, yS, yL). This objec-\n",
      "tive encourages the policy model to consistently accommodate the well-aligned short-context pref-\n",
      "erence while deviating the under-aligned long-context preference. Therefore, LongPO internally\n",
      "transfers preferences from short to long contexts without requiring external supervision, effectively\n",
      "addressing the challenge of long-context data annotation.\n",
      "3.2 S HORT-TO-LONG CONSTRAINT\n",
      "Long-context alignment often leads to an imbalance between long- and short-context performance.\n",
      "While this issue can be mitigated by carefully calibrating the scale and mixing proportion of long\n",
      "and short data across various context lengths, such an approach is resource-intensive and time-\n",
      "consuming. Moreover, an excessive incorporation of short-context data may inadvertently lead to\n",
      "insufficient long-context alignment. In LongPO, we recognize that the degradation in short-context\n",
      "performance during long-context alignment may be attributed to an improper (or missing) constraint\n",
      "in current alignment methods.\n",
      "Specifically, the RLHF and DPO objectives (implicitly) include a KL divergence term,βDKL[πθ(y ∣\n",
      "x) ∣∣ πref(y ∣ x)], which serves as a constraint to prevent excessive deviation from the reference\n",
      "model in Eq. (1). For a long input xL, this constraint C is expressed as:\n",
      "C =βDKL[πθ(y ∣ xL) ∣∣πref(y ∣ xL)]. (8)\n",
      "However, the reference model is typically the short-context model πS itself, which is not adept at\n",
      "handling long contexts. This results in a problematic reference distribution πref(y ∣ xL), leading to\n",
      "undesired deviation from the short-context model distribution.\n",
      "To address this issue, we propose a short-to-long constraint leveraging the quadruples introduced\n",
      "in Eq. (7). Recall that xS contains all the essential information from xL required to generate a\n",
      "satisfactory response, πS can serve as a proficient reference model conditioned on xS. While for an\n",
      "ideal reference model π∗\n",
      "ref capable of handling context lengths from short to long, we should have:\n",
      "DKL[π∗\n",
      "ref(y ∣ xL) ∣∣π∗\n",
      "ref(y ∣ xS)] =DKL[π∗\n",
      "ref(y ∣ xS) ∣∣πS(y ∣ xS)] =0, (9)\n",
      "4\n",
      "Published as a conference paper at ICLR 2025\n",
      "Figure 2: The procedure of generating short-to-long preference data from step 1 to 7.\n",
      "namely π∗\n",
      "ref(y ∣ xL) and πS(y ∣ xS) are identical distribution following Gibbs’ inequality. We\n",
      "hence derive an adjusted short-to-long constraint between short-context reference model and “long-\n",
      "context” policy model given contexts of different lengths:\n",
      "C′ =βDKL[πθ(y ∣ xL) ∣∣πS(y ∣ xS)]. (10)\n",
      "This refined constraint ensures that the policy model πθ operating on long contexts does not deviate\n",
      "significantly from the short-context model πS when provided with the essential information. By\n",
      "enforcing this constraint, we aim to preserve the short-context performance during long-context\n",
      "alignment, thereby addressing the imbalance issue in a more principled manner.\n",
      "By incorporating the short-to-long constraint in Eq. (2), we have a refined reward function for long\n",
      "input xL (following derivation in Appx. §A.1):\n",
      "rLongPO\n",
      "θ (xL, y) =β log πθ(y ∣ xL)\n",
      "πS (y ∣ xS) +β log Z(xL, xS), (11)\n",
      "where xS is extracted from xL as illustrated in Eq. (5). Hence we access the LongPO objective:\n",
      "LLongPO(πθ; πS) =−E(xS,xL,yS,yL)∼DSL [σ(rLongPO\n",
      "θ (xL, yS)−rLongPO\n",
      "θ (xL, yL))]\n",
      "=−E(xS,xL,yS,yL)∼DSL [log σ (β log πθ(yS ∣ xL)\n",
      "πS(yS ∣ xS) −β log πθ(yL ∣ xL)\n",
      "πS(yL ∣ xS))].\n",
      "(12)\n",
      "3.3 S ELF -EVOLVING\n",
      "Initialization. LongPO relies solely on access to a well-aligned short-context LLM, i.e., πS, in\n",
      "conjunction with a long-context plain corpus. Note that the long-context corpus need not be metic-\n",
      "ulously crafted, as it can be sampled and extracted from existing pretraining corpora of LLMs.\n",
      "Construction of short-to-long preference data. The construction of short-to-long preference\n",
      "data DSL introduced in §3.1 assumes an impractical extractor capable of retrieving essential in-\n",
      "formation from long contexts for each instruction. To satisfy this hypothesis, we reversely prompt\n",
      "πS to generate instructions for shortened chunks within long documents. This ensures that the short\n",
      "context information is self-contained for instructions. Concretely, our data construction process\n",
      "involves two steps as displayed in Figure 2:\n",
      "1. Instruction Generation. For each long document CL, we randomly sample a shortened\n",
      "chunk CS and prompt the πS to generate an instruction via the Self-Instruct (Wang et al.).\n",
      "To ensure the diversity of instructions, the model is prompted to generate an instruction\n",
      "pool first and then we randomly sample an instruction IL from this pool.\n",
      "2. Response Generation. Using the generated instruction IL, we prompt πS to produce two\n",
      "responses: a chosen response yS ∼πS(y ∣ xS) based on the short context xS, and a rejected\n",
      "response yL ∼πS(y ∣ xL) derived from the long context xL.\n",
      "5\n",
      "Published as a conference paper at ICLR 2025\n",
      "Iterative self-evolving training with LongPO. LongPO employs an iterative process to extend\n",
      "LLM context length. Initially, a short-context LLM πS generates short-to-long preference data for\n",
      "documents of length L1. The resulting model after LongPO training, now capable of handling L1\n",
      "contexts, then serves as the new “short-context LLM” for the next iteration, generating data for an\n",
      "extended length L2. This process repeats, progressively increasing context length capacity.\n",
      "As there are multiple short chunks CS = {Ci\n",
      "S}n\n",
      "i=1 within a long document CL, we collect the\n",
      "instruction-response triples (Ii\n",
      "L, yi\n",
      "S, yi\n",
      "L) for each chunk within identical long document, to form a\n",
      "multi-turn dataset ˆDSL. We then aggregate the probabilities across all turns to produce a multi-turn\n",
      "LongPO objective:\n",
      "LMT\n",
      "LongPO(πθ; πS) =−E(xS,xL,yS,yL)∼ ˆDSL [log σ (β log ∑n\n",
      "i=1 πθ(yi\n",
      "S ∣ xi\n",
      "L)\n",
      "∑n\n",
      "i=1 πS(yi\n",
      "S ∣ Ci\n",
      "S) −β log ∑n\n",
      "i=1 πθ(yi\n",
      "L ∣ xi\n",
      "L)\n",
      "∑n\n",
      "i=1 πS(yi\n",
      "L ∣ Ci\n",
      "S))],\n",
      "(13)\n",
      "where xS ={[Ci\n",
      "S; Ii\n",
      "L]}n\n",
      "i=1, xL ={[CL; Ii\n",
      "L]}n\n",
      "i=1, yS ={yi\n",
      "S}n\n",
      "i=1, and yL ={yi\n",
      "L}n\n",
      "i=1. LLMs trained with\n",
      "LongPO do not necessarily involve continual training before, which may lead to instability when\n",
      "processing long contexts. To address this issue and stabilize the training process, we incorporate\n",
      "a continual training objective following Pang et al. (2024b). Specifically, we add the negative log-\n",
      "likelihood (NLL) loss over entire long chosen sequencesSL =[xL; {Ii\n",
      "L; yi\n",
      "S}n\n",
      "i=1] to LongPO objective.\n",
      "Thus, our final training objective is:\n",
      "Lθ =λ ⋅LMT\n",
      "LongPO(πθ; πS)+LNLL(πθ; SL) =λ ⋅LMT\n",
      "LongPO(πθ; πS)+ πθ(SL)\n",
      "∣SL∣ . (14)\n",
      "4 E XPERIMENTAL SETUP\n",
      "4.1 T RAINING SETUP\n",
      "Data Curation Details. We curate the short-to-long preference data based on a long-context cor-\n",
      "pus sampled from the Book and ArXiv subsets of Long-Data-Collection1, and the GitHub subset of\n",
      "RedPajama (Computer, 2023). For a specific target length (e.g., 128K tokens), we filter the corpus\n",
      "to include only documents that are shorter than this length but longer than 64K tokens. Each long\n",
      "document is then segmented into chunks of up to 32K tokens, with a maximum of 4 randomly-\n",
      "sampled chunks retained per document. For instruction generation, we prompt short-context models\n",
      "to generate 4 instructions per document, from which we randomly select one for further use. Af-\n",
      "ter filtering undesired (censored and repetitive) responses, we collect 45K, 16K, 2.5K multi-turn\n",
      "instruction-response samples of 128K, 256K, and 512K tokens for Mistral-7B, and 32K samples of\n",
      "128K tokens for Qwen2.5-7B. More details are listed in Appx. §B.1.\n",
      "Training Details. We extend the context length of Mistral-7B and Qwen2.5-7B using our LongPO\n",
      "on short-to-long preference data specifically generated by models themselves. The training pro-\n",
      "cess begins with utilizing Mistral-7B/Qwen2.5-7B to generate data with a length of 128K and ex-\n",
      "tend the context length to 128K. To investigate the scalability, we utilize the resulting Mistral-7B-\n",
      "LongPO-128K to generate data with lengths of 256K/512K and further extend the context length\n",
      "to 256K/512K. We leverage Deepspeed-Ulysses (Jacobs et al., 2023) for sequence parallelism and\n",
      "employ Flash Attention (Dao et al., 2022; Dao, 2023) for efficient computation. All models are\n",
      "optimized using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 5e-7. We set the\n",
      "margin β in Eq. (13) to 0.1 and the weighting factor λ in Eq. (14) to 0.01. RoPE θ is set as 1e7 and\n",
      "the batch size is set as 8. More details are listed in Appx. §B.3.\n",
      "4.2 E VALUATION BENCHMARKS\n",
      "We assess both the long- and short-context capabilities of our models against baselines. The long-\n",
      "context evaluation utilizes the following benchmarks:\n",
      "• ∞Bench (Zhang et al.). We evaluate all models on three tasks in this benchmark: sum-\n",
      "marization (En.Sum), long-book question answering (En.QA), and multi-choice question-\n",
      "answering (En.MC). The evaluation length is beyond 100K.\n",
      "1https://huggingface.co/datasets/togethercomputer/Long-Data-Collections\n",
      "6\n",
      "Published as a conference paper at ICLR 2025\n",
      "• RULER (Hsieh et al., 2024). This benchmark comprises four types of synthetic tasks\n",
      "across variable sequence lengths (4K to 128K): Needle-in-a-haystack (NIAH) retrieval,\n",
      "Multi-hop Tracing with Variable Tracking (VT), Aggregation, and Question Answering\n",
      "(QA). We exclude the Aggregation tasks, which involve word frequency counting within\n",
      "the context, since they present challenges in word counting beyond mere long-context ca-\n",
      "pabilities that current LLMs still struggle in.\n",
      "• LongBench-Chat (Bai et al., 2024). This benchmark assesses instruction-following abil-\n",
      "ities over long contexts (10K to 100K tokens), employing GPT-4-128K as an impartial\n",
      "judge to evaluate model-generated responses. We filter out the English samples for fair\n",
      "comparison across different models.\n",
      "For short-context evaluation, we employ MMLU (Hendrycks et al., 2021), ARC-C (Clark et al.,\n",
      "2018), Hellaswag (Zellers et al., 2019) and Winogrande (Sakaguchi et al., 2019) for assessing the\n",
      "general language understanding and reasoning capabilities, and MT-Bench (Zheng et al., 2023) for\n",
      "assessing instruction-following capability. More details are listed in Appx. §B.2.\n",
      "4.3 B ASELINES\n",
      "We train our LongPO on Mistral-7B-Instruct-v0.2 (denoted as Mistral-7B) and Qwen2.5-7B-\n",
      "Instruct (denoted as Qwen2.5-7B), comparing them against a range of powerful LLMs including\n",
      "GPT-4-128K, Qwen2-72B-Instruct (Yang et al., 2024), LLaMA-3.1-70B, LLaMA-3.1-8B, GLM-\n",
      "4-9B-Chat, GLM-4-9B-Chat-1M, LWM-Text-Chat-1M (Liu et al., 2024b), and Yarn-Mistral-7b-\n",
      "128k (Peng et al., 2023). Additionally, we establish baselines using Mistral-7B trained with conven-\n",
      "tional SFT and DPO on the same dataset used by LongPO.\n",
      "For short-context evaluation, we primarily compare the performance of naive LLMs against their\n",
      "counterparts post-trained with SFT, DPO, and LongPO on our synthetic data. To provide a more\n",
      "comprehensive comparison, we also include two series of open-source long-context language mod-\n",
      "els: GLM-4-9B-Chat versus GLM-4-9B-Chat-1M, and LWM-Text-Chat-128k versus LWM-Text-\n",
      "Chat-1M. This allows us to assess the effectiveness of our LongPO to maintain the short-context\n",
      "performance during long-context alignment, comparing with baselines utilizing various strategies.\n",
      "5 R ESULTS AND ANALYSES\n",
      "In this section, we demonstrate the exceptional effectiveness of LongPO through two types of com-\n",
      "parisons: (1) comparison with naive SFT and DPO trained on identical models and datasets; (2)\n",
      "comparison with SOTA long-context LLMs.\n",
      "5.1 C OMPARISON WITH SFT AND DPO\n",
      "We first compare LongPO with conventional SFT and DPO using identical LLM (Mistral-7B). All\n",
      "models are trained on equivalent self-generated datasets, as detailed in §4.1. Given the inability of\n",
      "SFT to leverage preference data, we apply it to the instructions paired with chosen responses.\n",
      "LongPO exhibits superior performance over SFT and DPO. The experimental results, illus-\n",
      "trated in Table 1, reveal consistent and substantial performance gains (10 to 20+ points) of LongPO\n",
      "over SFT and DPO across a diverse range of long-context tasks. Crucially, as depicted in Figure 3,\n",
      "LongPO maintains robust short-context performance compared with original short-context LLMs\n",
      "(59.99 vs 59.15 on MMLU), whereas SFT and DPO exhibit notable degradation in short-context\n",
      "scenarios after long-context alignment process.\n",
      "The performance disparity between LongPO and SFT can be attributed to the explicit integration\n",
      "of short-to-long preference in LongPO, which is either absent or merely implicit in the chosen re-\n",
      "sponses utilized by SFT. While both LongPO and DPO leverage the proposed short-to-long prefer-\n",
      "ence data, the pivotal difference lies in the short-to-long constraint introduced in §3.2. The marked\n",
      "performance gaps between LongPO and DPO, observed across both long- and short-context tasks,\n",
      "highlight the effectiveness of the proposed constraint for successfully mitigating the problematic\n",
      "limitations in DPO and retaining the short-context performance during long-context training. More\n",
      "ablations are detailed in §5.3.\n",
      "7\n",
      "Published as a conference paper at ICLR 2025\n",
      "Table 1: Long-Context Performance of our LongPO compared with baselines. Higher is better for\n",
      "all metrics. Results marked with ♭ are evaluated by ourselves, while other results of baselines are\n",
      "sourced from the original benchmarks. Full results on RULER are listed in Table 2.\n",
      "Model Train/Claimed∞Bench RULER LongBench-Length En.Sum En.QA En.MC A VG. NIAH VT QA A VG. Chat (EN)GPT-4-128K 128K 14.73 22.44 67.25 34.81 95.4 99.9 70.3 88.53 8.40Qwen2-72B 128K 24.32♭ 7.03♭72.05♭34.47♭ 88.6 95.7 66.7 83.67 7.72♭LLaMA 3.1-70B 128K 33.55♭36.08♭69.00♭46.21♭ 96.1 93.2 67.8 85.7 6.67♭LLaMA 3.1-8B 128K 28.06♭30.47♭58.08♭38.87♭ 97.93 91.4 64.7 84.68 6.22♭GLM-4-9B 128K 14.84♭ 9.51♭67.25♭30.53♭96.51♭97.3♭64.8♭0 86.20♭ 5.67♭GLM-4-9B-1M 1M 28.3 9.7 68.6 35.53 98.2 99.4 69.4 89.0 5.03♭LWM-7B-1M 1M 4.33♭ 0.0♭ 3.06♭ 2.46♭ 87.20 57.5 56.4 67.03 1.25♭YaRN-Mistral-7B 128K 9.09 9.55 27.95 15.53 63.4 36.1 25.9 41.8 -Mistral-7B 32K 22.13 4.93 14.41 13.82 72.60 74.40 52.2 66.4 4.10- SFT 128K 23.44 13.45 53.21 30.03 88.73 79.64 51.08 73.15 4.25- DPO 128K 15.21 10.34 48.14 25.56 74.25 72.36 50.24 65.62 4.08- LongPO (iter1)128K27.0523.5167.2539.2796.8896.4964.8186.065.42- LongPO (iter2)256K28.1624.4366.3539.6596.8097.064.8786.225.48- LongPO (iter3)512K29.1027.8566.6741.2197.2897.4864.9286.565.80Qwen2.5-7B 128K 22.89 6.08 52.4 27.12 82.1 80.09 54.30 72.16 5.80- LongPO (iter1)128K32.0617.3272.0540.4895.8189.7159.481.645.75\n",
      "MMLU ARC-C Hellaswag Winogrande MT-Bench\n",
      "-10.0\n",
      "-1.0\n",
      "0.0\n",
      "1.0\n",
      "Margin\n",
      "0.84\n",
      "0.08\n",
      "-0.21\n",
      "0.13 0.10\n",
      "0.32\n",
      "1.02\n",
      "-0.06\n",
      "-0.26\n",
      "0.40\n",
      "-25.74\n",
      "-14.80\n",
      "-10.77 -14.11\n",
      "-19.90\n",
      "-14.04\n",
      "-9.73 -9.07 -12.15\n",
      "-22.00\n",
      "-0.42\n",
      "-1.11 -1.12\n",
      "-0.55\n",
      "-0.40\n",
      "-1.03\n",
      "-0.51\n",
      "-0.25\n",
      "-1.02\n",
      "-2.40\n",
      "LongPO (iter1) LongPO (iter2) SFT DPO GLM LWM\n",
      "Figure 3: The margins of the short-context performance of Mistral-7B-LongPO and baselines rela-\n",
      "tive to corresponding base model. GLM and LWM refer to the margins of GLM-9B-1M and LWM-\n",
      "7B-1M over GLM-9B-128K and LWM-7B-128K, respectively. MT-Bench metrics ( ∈[0, 10]) are\n",
      "linearly scaled to [0, 100] for better comparability across tasks. See numerical results in Table 3.\n",
      "5.2 C OMPARISON WITH SOTA LONG -CONTEXT LLM S\n",
      "To further substantiate the efficacy of LongPO, we conducted an extensive comparison between our\n",
      "LongPO-trained Mistral-7B and leading long-context LLMs across varying model scales.\n",
      "LongPO demonstrates exceptional competitiveness at similar scale. As detailed in Table 1,\n",
      "LongPO demonstrates formidable competitiveness in terms of models at similar scales. For exam-\n",
      "ple, Mistral-7B-LongPO significantly outperforms some established long-context models, including\n",
      "LWM-7B and YaRN-Mistral, across all long-context tasks in ∞Bench and RULER. Remarkably,\n",
      "Mistral-7B-LongPO-128K surpasses GLM-4-9B (39.27 vs. 30.53 on ∞Bench and 86.06 vs. 86.20\n",
      "on RULER), although the latter is training on manually annotated long-context data spanning up\n",
      "to 128K sequence length. Moreover, GLM-4-9B-1M, an extension of GLM-4-9B trained on con-\n",
      "texts up to 1M tokens, demonstrates slightly superior performance than LongPO on the RULER\n",
      "benchmark. However, these performance gains come at the costs of degenerated short-context per-\n",
      "formance (0.41 on MMLU) and long-context instruction-following capability(0.64 on LongBench-\n",
      "Chat (EN)) as illustrated in Figure 3. Notably, our models still outperform GLM-4-9B-1M on\n",
      "∞Bench even trained with substantially shorter sequences. These results underscore the exceptional\n",
      "efficiency of LongPO in transferring performance from short to long contexts through self-evolution,\n",
      "thereby circumventing the need for extensive manual annotation.\n",
      "8\n",
      "Published as a conference paper at ICLR 2025\n",
      "0 1000 2000 3000 4000 5000\n",
      "Training Steps\n",
      "20\n",
      "40\n",
      "60\n",
      "80RULER-/glyph1197IAH\n",
      "0 1000 2000 3000 4000 5000\n",
      "Training Steps\n",
      "40\n",
      "50\n",
      "60MMLU\n",
      "LongPO LongPO (w/o NLL) SFT-Chosen SFT-Rejected DPO SFT-Chosen-Constraint\n",
      "Figure 4: Long- and short-context performance comparison among LongPO, SFT on chosen re-\n",
      "sponses (SFT-Chosen), SFT on rejected responses ( SFT-Rejected), DPO, and SFT on chosen re-\n",
      "sponses with short-to-long constraint (SFT-Chosen-Constraint).\n",
      "Long-context annotation is not sufficient. The superiority of our approach is particularly evident\n",
      "in the En.QA task within ∞Bench, which involves complex free-form question answering over\n",
      "extensive book-length contexts. In this challenging task, our models surpass both GLM-4-9B and\n",
      "GLM-4-9B-1M by substantial margins (10+ points). The inherent difficulty of such task, which\n",
      "poses challenges even for human annotators, highlights the limitations of relying solely on manually\n",
      "annotated long-context data. By effectively transferring short-context capabilities to long-context\n",
      "scenarios, LongPO demonstrates superior scalability and efficacy across diverse and intricate tasks.\n",
      "Superior LLMs Yet to Dominate Long-Context Scenarios When benchmarked against leading\n",
      "models such as GPT-4-128K, our LongPO-trained models still exhibit comparable or even superior\n",
      "long-context performance (e.g., Mistral-7B-LongPO-128K of 39.27 vs. GPT-4-128K of 34.81 on\n",
      "∞Bench), despite being based on significantly smaller Mistral-7B. This observation reveals that\n",
      "even the most advanced LLMs have not yet achieved the same level of dominance in long-context\n",
      "scenarios as they have in short-context tasks. This performance gap can be attributed primarily to the\n",
      "scarcity of high-quality, large-scale long-context training data. The dearth of such data is particularly\n",
      "impactful for larger LLMs, given the established scaling laws in language model training. This\n",
      "finding underscores the potential of LongPO for enhanced performance without the requirement for\n",
      "externally annotated long-context datasets.\n",
      "5.3 A BLATION STUDIES\n",
      "We conduct comprehensive ablation studies to investigate the efficacy of components in LongPO:\n",
      "Effectiveness of short-to-long preference. The core of LongPO is learning the short-to-long pref-\n",
      "erence between chosen and rejected responses given short and long contexts, respectively. To evalu-\n",
      "ate this component’s effectiveness, we compare LongPO with two baseline methods: SFT on chosen\n",
      "responses (SFT-Chosen) and on rejected responses (SFT-Rejected). SFT-Chosen implicitly incor-\n",
      "porates short-context preference, while SFT-Rejected entirely omits it. As illustrated in Figure 4,\n",
      "LongPO consistently outperforms both SFT variants in long-context performance (RULER-NIAH)\n",
      "throughout the training process. This substantial improvement underscores the efficacy of our short-\n",
      "to-long preference approach in enhancing long-context capabilities.\n",
      "Effectiveness of short-to-long constraint. To assess the impact of our short-to-long constraint,\n",
      "we compare LongPO with DPO upon short-to-long preference that removes this constraint. As evi-\n",
      "dent in Figure 4, the unconstrained DPO demonstrates markedly inferior performance throughout the\n",
      "training process, both in long- and short-context tasks. Notably, short-context capabilities degrade\n",
      "rapidly in DPO during the initial training. Conversely, when we apply our short-to-long constraint to\n",
      "naive SFT without explicit short-to-long preference, the model maintains short-context performance\n",
      "on par with the original LLMs, even after long-context alignment. These results demonstrate the\n",
      "crucial role of our short-to-long constraint in preserving short-context capabilities.\n",
      "Impact of NLL loss. We investigate the effect of incorporating a negative log-likelihood (NLL)\n",
      "loss over long context input and chosen response in Eq. (14) during LongPO training. As shown\n",
      "in Figure 4, removing the NLL loss significantly degrades the long-context performance of LongPO\n",
      "9\n",
      "Published as a conference paper at ICLR 2025\n",
      "across the training procedure. Specifically, the convergence of training for long-context performance\n",
      "becomes slower. This demonstrates the crucial role of NLL loss in enhancing long-context capabil-\n",
      "ities without resorting to continual training on long data.\n",
      "6 R ELATED WORK\n",
      "Alignment of LLMs. Aligning Large Language Models (LLMs) with human preferences and val-\n",
      "ues has been crucial to unlocking their full potential from large-scale pretraining. The typical align-\n",
      "ment process begins with Supervised Finetuning (SFT) on annotated instruction-response pairs. This\n",
      "is followed by Reinforcement Learning from Human Feedback (RLHF), which aligns LLMs more\n",
      "closely with human intentions through reward model training and policy optimization (Christiano\n",
      "et al., 2017; Ouyang et al., 2022; Bai et al., 2022; Stiennon et al., 2020). To streamline RLHF train-\n",
      "ing, Direct Preference Optimization (DPO) (Rafailov et al., 2023) and its variants (Ethayarajh et al.,\n",
      "2024; Azar et al., 2023; Pang et al., 2024a; Hong et al., 2024; Meng et al., 2024) have been pro-\n",
      "posed, eliminating the need for explicit reward model training by learning preferences directly from\n",
      "human-ranked response pairs. While these alignment methods have shown significant success, they\n",
      "heavily rely on human-annotated data. This reliance becomes problematic for long-context data,\n",
      "where human annotation is both challenging and potentially less reliable.\n",
      "Long-context extending of LLMs. Extending the context length of LLMs has been approached\n",
      "through various methods. Some techniques involve scaling the rotary position embedding (Su et al.,\n",
      "2022) followed by continual training on a small corpus of long documents (Chen et al., 2023b; Peng\n",
      "et al., 2023; Rozière et al., 2023; Chen et al., 2023a). Alternative approaches, such as those proposed\n",
      "by Jin et al. (2024); An et al. (2024), introduce hierarchical or chunked attention mechanisms to ex-\n",
      "tend context length without additional training. However, these methods often involve limitations in\n",
      "practical applications. Recent advancements include the work of Dubey et al. (2024), who proposed\n",
      "continual pretraining on a massive long-context corpus (800B tokens) and incorporating a small\n",
      "fraction (0.1%) of long-context data during SFT to enhance long-context capabilities. Zeng et al.\n",
      "(2024) utilizes human-annotated long-context data for SFT and DPO to align long-context LLMs.\n",
      "Despite their effectiveness, these methods require either extensive training or human annotation of\n",
      "long-context data, making them prohibitively expensive and lack scalability.\n",
      "Self-Evolving LLMs. Recent works (Yuan et al., 2024; Liu et al., 2024a; Li et al., 2024) have\n",
      "unveiled the remarkable capability of Large Language Models (LLMs) to evolve from relatively\n",
      "weak to significantly stronger performance through self-augmented data. Yuan et al. (2024);\n",
      "Liu et al. (2024a) leverage iterative training on model-generated responses, ranked by LLM-as-\n",
      "a-Judge (Zheng et al., 2023) prompting, to enhance model itself. Li et al. (2024) introduces the\n",
      "instruction backtranslation to produce self-augmenting data that further enhances model capabili-\n",
      "ties. Our work first extends the self-evolution property to the context length, to develop long-context\n",
      "LLMs without relying on external annotations.\n",
      "7 C ONCLUSION AND DISCUSSION\n",
      "In this work, we propose LongPO, a novel long-context alignment method that enables LLMs to ef-\n",
      "fectively transfer their short-context capabilities to long-context scenarios. Our approach addresses\n",
      "key challenges in long-context alignment by leveraging intrinsic model knowledge, eliminating the\n",
      "need for external long-context annotated data. LongPO is built on short-to-long preference data,\n",
      "comprising paired responses for the same instruction given a long context and relevant shortened\n",
      "chunk, respectively. By steering the policy model to learn from the discrepancies within these paired\n",
      "responses, LongPO facilitates the transfer of established capabilities from short to long contexts. In\n",
      "addition, LongPO incorporates a short-to-long constraint using KL divergence, that effectively pre-\n",
      "serve short-context performance during training. Experimental results demonstrate that LongPO\n",
      "significantly improves long-context performance across various tasks, outperforming existing align-\n",
      "ment methods and even surpassing more sophisticated models. Importantly, this improvement is\n",
      "achieved without sacrificing short-context proficiency. The success of LongPO highlights the poten-\n",
      "tial of leveraging internal model knowledge for alignment tasks, opening new avenues for efficient\n",
      "adaptation of LLMs to diverse context lengths.\n",
      "10\n",
      "Published as a conference paper at ICLR 2025\n",
      "ACKNOWLEDGMENTS\n",
      "This work was supported by DAMO Academy through DAMO Academy Research Intern Program.\n",
      "REFERENCES\n",
      "Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong.\n",
      "Training-free long-context scaling of large language models, 2024.\n",
      "Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal\n",
      "Valko, and Rémi Munos. A general theoretical paradigm to understand learning from human\n",
      "preferences. ArXiv, abs/2310.12036, 2023. URL https://api.semanticscholar.org/\n",
      "CorpusID:264288854.\n",
      "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\n",
      "Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Ols-\n",
      "son, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-\n",
      "Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse,\n",
      "Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mer-\n",
      "cado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna\n",
      "Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Con-\n",
      "erly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario\n",
      "Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai:\n",
      "Harmlessness from ai feedback. 2022. URL https://arxiv.org/abs/2212.08073.\n",
      "Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi\n",
      "Li. Longalign: A recipe for long context alignment of large language models. 2024. URL\n",
      "https://arxiv.org/abs/2401.18058.\n",
      "Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen\n",
      "Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard\n",
      "(2023-2024). https://huggingface.co/spaces/open-llm-leaderboard-old/\n",
      "open_llm_leaderboard, 2023.\n",
      "Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Li Bing. Clex: Continuous length\n",
      "extrapolation for large language models. ArXiv, abs/2310.16450, 2023a. URL https://api.\n",
      "semanticscholar.org/CorpusID:264451707.\n",
      "Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window\n",
      "of large language models via positional interpolation. 2023b. URL https://arxiv.org/\n",
      "abs/2306.15595.\n",
      "Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\n",
      "reinforcement learning from human preferences. Advances in neural information processing sys-\n",
      "tems, 30, 2017.\n",
      "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\n",
      "and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\n",
      "challenge. ArXiv, abs/1803.05457, 2018. URL https://api.semanticscholar.org/\n",
      "CorpusID:3922816.\n",
      "Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\n",
      "URL https://github.com/togethercomputer/RedPajama-Data.\n",
      "Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.\n",
      "URL https://arxiv.org/abs/2307.08691.\n",
      "Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and\n",
      "memory-efficient exact attention with IO-awareness. InAdvances in Neural Information Process-\n",
      "ing Systems, 2022. URL https://arxiv.org/abs/2205.14135.\n",
      "11\n",
      "Published as a conference paper at ICLR 2025\n",
      "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\n",
      "Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\n",
      "arXiv preprint arXiv:2407.21783, 2024.\n",
      "Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model\n",
      "alignment as prospect theoretic optimization. ArXiv, abs/2402.01306, 2024. URL https://\n",
      "api.semanticscholar.org/CorpusID:267406810.\n",
      "Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Fos-\n",
      "ter, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muen-\n",
      "nighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lin-\n",
      "tang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework\n",
      "for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/\n",
      "12608602.\n",
      "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Ja-\n",
      "cob Steinhardt. Measuring massive multitask language understanding. In International Confer-\n",
      "ence on Learning Representations, 2021. URL https://openreview.net/forum?id=\n",
      "d7KBjmI3GmQ.\n",
      "Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without\n",
      "reference model. ArXiv, abs/2403.07691, 2024. URL https://api.semanticscholar.\n",
      "org/CorpusID:268363309.\n",
      "Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang\n",
      "Zhang, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language\n",
      "models? arXiv preprint arXiv:2404.06654, 2024.\n",
      "Sam Adé Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song, Samyam Ra-\n",
      "jbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training\n",
      "of extreme long sequence transformer models. ArXiv, abs/2309.14509, 2023. URL https:\n",
      "//api.semanticscholar.org/CorpusID:262826014.\n",
      "Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\n",
      "Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lu-\n",
      "cile Saulnier, L’elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,\n",
      "Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. ArXiv,\n",
      "abs/2310.06825, 2023. URL https://api.semanticscholar.org/CorpusID:\n",
      "263830494.\n",
      "Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan\n",
      "Chen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning, 2024.\n",
      "Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Yoshua\n",
      "Bengio and Yann LeCun (eds.),3rd International Conference on Learning Representations, ICLR\n",
      "2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:\n",
      "//arxiv.org/abs/1412.6980.\n",
      "Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and\n",
      "Mike Lewis. Self-alignment with instruction backtranslation, 2024. URL https://arxiv.\n",
      "org/abs/2308.06259.\n",
      "Aiwei Liu, Haoping Bai, Zhiyun Lu, Xiang Kong, Simon Wang, Jiulong Shan, Meng Cao, and\n",
      "Lijie Wen. Direct large language model alignment through self-rewarding contrastive prompt\n",
      "distillation, 2024a. URL https://arxiv.org/abs/2402.11907.\n",
      "Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video\n",
      "and language with ringattention. arXiv preprint, 2024b. URL https://arxiv.org/abs/\n",
      "2402.08268.\n",
      "Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization\n",
      "with a reference-free reward. ArXiv, abs/2405.14734, 2024. URL https://api.\n",
      "semanticscholar.org/CorpusID:269983560.\n",
      "12\n",
      "Published as a conference paper at ICLR 2025\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\n",
      "Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel-\n",
      "ton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano,\n",
      "Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human\n",
      "feedback. ArXiv, abs/2203.02155, 2022. URL https://api.semanticscholar.org/\n",
      "CorpusID:246426909.\n",
      "Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason\n",
      "Weston. Iterative reasoning preference optimization. ArXiv, abs/2404.19733, 2024a. URL\n",
      "https://api.semanticscholar.org/CorpusID:269457506.\n",
      "Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason\n",
      "Weston. Iterative reasoning preference optimization, 2024b. URL https://arxiv.org/\n",
      "abs/2404.19733.\n",
      "Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\n",
      "extension of large language models. 2023. URL https://arxiv.org/abs/2309.00071.\n",
      "Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and\n",
      "Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\n",
      "In NeurIPS, 2023.\n",
      "Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\n",
      "Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton,\n",
      "Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez,\n",
      "Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and\n",
      "Gabriel Synnaeve. Code llama: Open foundation models for code. 2023. URL https://\n",
      "arxiv.org/abs/2308.12950.\n",
      "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver-\n",
      "sarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019.\n",
      "Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,\n",
      "Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances\n",
      "in Neural Information Processing Systems, 33:3008–3021, 2020.\n",
      "Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: En-\n",
      "hanced transformer with rotary position embedding. 2022. URLhttps://arxiv.org/abs/\n",
      "2104.09864.\n",
      "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and\n",
      "Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In\n",
      "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume\n",
      "1: Long Papers). URL https://aclanthology.org/2023.acl-long.754.\n",
      "Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\n",
      "Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Interna-\n",
      "tional Conference on Learning Representations, 2022. URL https://openreview.net/\n",
      "forum?id=gEZrGCozdqR.\n",
      "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\n",
      "Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang,\n",
      "Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jin-\n",
      "gren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin\n",
      "Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao,\n",
      "Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wen-\n",
      "bin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng\n",
      "Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu,\n",
      "Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL\n",
      "https://arxiv.org/abs/2407.10671.\n",
      "13\n",
      "Published as a conference paper at ICLR 2025\n",
      "Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason\n",
      "Weston. Self-rewarding language models. ArXiv, abs/2401.10020, 2024. URL https://api.\n",
      "semanticscholar.org/CorpusID:267035293.\n",
      "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma-\n",
      "chine really finish your sentence? In Annual Meeting of the Association for Computational Lin-\n",
      "guistics, 2019. URL https://api.semanticscholar.org/CorpusID:159041722.\n",
      "Team Glm Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu\n",
      "Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng,\n",
      "Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Ming yue Liu,\n",
      "Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao,\n",
      "Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiaoyu Xia, Xiaohan Zhang, Xiaotao\n",
      "Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yi An, Yifan\n",
      "Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang,\n",
      "Zhenyi Yang, Zhengxiao Du, Zhen-Ping Hou, and Zihan Wang. Chatglm: A family of large\n",
      "language models from glm-130b to glm-4 all tools. ArXiv, abs/2406.12793, 2024. URL https:\n",
      "//api.semanticscholar.org/CorpusID:270562306.\n",
      "Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen\n",
      "Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. ∞Bench: Extending long context evaluation\n",
      "beyond 100K tokens. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings\n",
      "of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\n",
      "Papers). URL https://aclanthology.org/2024.acl-long.814.\n",
      "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\n",
      "Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph Gonzalez, and Ion Stoica.\n",
      "Judging llm-as-a-judge with mt-bench and chatbot arena. ArXiv, abs/2306.05685, 2023. URL\n",
      "https://api.semanticscholar.org/CorpusID:259129398.\n",
      "14\n",
      "Published as a conference paper at ICLR 2025\n",
      "A M ATHEMATICAL DERIVATIONS\n",
      "A.1 D ERIVING THE LONG PO O BJECTIVE\n",
      "In this section, we will derive the reward function of our LongPO objective in Eq. (11) by incorporat-\n",
      "ing short-to-long constraint in Eq. (10). Starting from RLHF objective in Eq. (1) with short-to-long\n",
      "constraint, we have\n",
      "max\n",
      "π\n",
      "ExL∼D,y∼π[r(xL, y)]−βDKL[π(y∣xL)∣∣πS(y∣xS)] (15)\n",
      "Following the DPO derivation process (Rafailov et al., 2023), we have:\n",
      "max\n",
      "π\n",
      "E(xL,xS)∼ ˆDSL,y∼π(y∣xL)[r(xL, y)]−βDKL[π(y∣xL) ∣∣πS(y∣xS)]\n",
      "=max\n",
      "π\n",
      "E(xL,xS)∼ ˆDSL Ey∼π(y∣xL) [r(xL, y)−β log π(y∣xL)\n",
      "πS(y∣xS)]\n",
      "=min\n",
      "π\n",
      "E(xL,xS)∼ ˆDSL Ey∼π(y∣xL) [log π(y∣xL)\n",
      "πS(y∣xS) − 1\n",
      "β r(xL, y)]\n",
      "=min\n",
      "π\n",
      "E(xL,xS)∼ ˆDSL Ey∼π(y∣xL)\n",
      "⎡⎢⎢⎢⎢⎢⎣\n",
      "log π(y∣xL)\n",
      "1\n",
      "Z(xL,xS) πS(y∣xS)exp (1\n",
      "β r(xL, y))\n",
      "−log Z(xL, xS)\n",
      "⎤⎥⎥⎥⎥⎥⎦\n",
      ",\n",
      "(16)\n",
      "where we have partition function:\n",
      "Z(xL, xS) =∑\n",
      "y\n",
      "πS(y∣xS)exp (1\n",
      "β r(xL, y)).\n",
      "The partition function is only related to xL, xS, and original short-context LLM πS. Hence we have\n",
      "the optimal solution following Rafailov et al. (2023):\n",
      "π∗(y∣xL) = 1\n",
      "Z(xL, xS)πS(y∣xS)exp (1\n",
      "β r(xL, y)). (17)\n",
      "The optimal reward function would be derived:\n",
      "r∗(xL, y) =β log π∗(y∣xL)\n",
      "πS(y∣xS) +β log Z(xL, xS). (18)\n",
      "We thus have:\n",
      "p∗(y1 ≻y2∣xL) = exp (r∗(xL, y1))\n",
      "exp (r∗(xL, y1))+exp (r∗(xL, y2))\n",
      "=\n",
      "exp (β log π∗(y1∣xL)\n",
      "πS(y1∣xS) +β log Z(xL, xS))\n",
      "exp (β log π∗(y1∣xL)\n",
      "πS(y1∣xS) +β log Z(xL, xS))+exp (β log π∗(y2∣xL)\n",
      "πS(y2∣xS) +β log Z(xL, xS))\n",
      "= 1\n",
      "1 +exp (β log π∗(y2∣xL)\n",
      "πS(y2∣xS) −β log π∗(y1∣xL)\n",
      "πS(y1∣xS) )\n",
      "=σ (β log π∗(y1∣xL)\n",
      "πS(y1∣xS) −β log π∗(y2∣xL)\n",
      "πS(y2∣xS)).\n",
      "By optimizing the πθ towards the optimal policy π∗, we finally access the objective of LongPO\n",
      "in Eq. (12).\n",
      "B E XPERIMENTAL DETAILS\n",
      "B.1 D ATA CONSTRUCTION DETAILS\n",
      "We prompt the Mistral-7B-Instruct-v0.2 to generate instructions with decode parameters of temper-\n",
      "ature T = 0.7 and p = 0.9. The prompt of Self-Instruct to generate an instruction pool is shown\n",
      "15\n",
      "Published as a conference paper at ICLR 2025\n",
      "Figure 5: The prompt for generating instruction pool.\n",
      "in Figure 5. For generating the corresponding responses, we directly concatenate the short or long\n",
      "context with corresponding instructions and adopt the greedy decoding to maintain the deterministic\n",
      "behaviour of LLMs. As shown in Figure 6, the model would tend to prefer the high-quality chosen\n",
      "response and deviate from the low-quality rejected response over long context, hence improve the\n",
      "long-context capabilities.\n",
      "B.2 E VALUATION DETAILS\n",
      "On long-context benchmarks InfiniteBench and RULER, we evaluate our models and all baselines\n",
      "following the settings in the original benchmarks. For short-context evaluation, we utilize the lm-\n",
      "evaluaton-harness framework (Gao et al., 2024) and following the evaluation settings in (Beeching\n",
      "et al., 2023): 5-shots for MMLU, 25-shots for ARC-C, 10-shots for Hellaswag, and 5-shots for\n",
      "Winogrande. We use GPT-4-Turbo-1106-Preview as the judge for MT-Bench and LongBench-Chat\n",
      "evaluation.\n",
      "B.3 M ORE TRAINING DETAILS\n",
      "Leveraging the DeepSpeed-Ulysses sequence parallel framework, we train the Mistral-7B/Qwen2.5-\n",
      "7B with a sequence length of 128K on an 8 ×A800 80GB, achieving a throughput of 4,401 tokens\n",
      "per second. For sequence lengths of 256K and 512K, the models are trained on a 16 ×A800 80GB,\n",
      "yielding throughputs of 4,120 tokens per second and 2,744 tokens per second, respectively. To\n",
      "facilitate a comparison with standard LLM alignment methods, we train Mistral-7B using SFT and\n",
      "DPO utilizing the same short-to-long preference data of LongPO. For DPO training, we apply the\n",
      "same settings as LongPO outlined in §4.1, but excluding the short-to-long constraint of LongPO\n",
      "introduced in §3.2. Since SFT cannot utilize paired responses within preference data, we train it\n",
      "using only the chosen responses provided alongside long context inputs. The hyperparameters for\n",
      "SFT remain unchanged, except for an increase in the learning rate to 2e-5.\n",
      "16\n",
      "Published as a conference paper at ICLR 2025\n",
      "(a) The rewards for chosen response during training.\n",
      " (b) The rewards for rejected response during training.\n",
      "Figure 6: The chosen and rejected rewards during the training of Mistral-7B-LongPO-128K.\n",
      "Table 2: Full results on 13 tasks of RULER benchmark. The bold values denote the average score\n",
      "of 13 tasks in RULER over various context lengths.\n",
      "Model Category 4k 8k 16k 32k 64k 128k A VG\n",
      "Qwen2.5-7B-Instruct\n",
      "NIAH 99.69 98.45 97.82 95.24 74.56 26.86 82.10\n",
      "VT 99.88 99.72 96.24 96.44 81.44 6.84 80.09\n",
      "AGG 92.52 89.78 92.08 81.93 62.48 28.23 74.50\n",
      "QA 71.00 65.30 64.00 58.70 46.80 19.99 54.30\n",
      "A VG (13 tasks) 94.19 92.11 91.61 87.66 68.96 24.47 76.50\n",
      "Qwen2.5-7B-LongPO-128K\n",
      "NIAH 99.64 98.97 97.80 95.54 94.80 88.15 95.82\n",
      "VT 99.96 99.92 96.12 86.24 78.20 77.80 89.71\n",
      "AGG 95.50 86.12 91.75 82.56 66.31 49.81 78.67\n",
      "QA 70.00 64.00 62.70 57.70 53.00 49.00 59.40\n",
      "A VG (13 tasks) 94.47 91.69 91.34 87.00 82.71 75.43 87.11\n",
      "Mistral-7B-LongPO-128K\n",
      "NIAH 99.43 98.64 98.09 97.84 95.82 91.44 96.88\n",
      "VT 99.40 99.16 98.08 96.36 92.80 93.12 96.49\n",
      "AGG 88.31 82.91 92.23 72.775 46.305 46.79 71.55\n",
      "QA 71.10 70.15 66.60 65.80 61.00 54.20 64.81\n",
      "A VG (13 tasks) 93.36 91.88 92.35 88.94 82.61 78.97 88.02\n",
      "Mistral-7B-LongPO-256K\n",
      "NIAH 99.16 97.79 98.02 97.76 96.53 91.54 96.80\n",
      "VT 99.40 99.20 97.96 97.72 94.21 93.52 97.00\n",
      "AGG 87.40 76.59 89.03 72.20 45.17 44.47 69.14\n",
      "QA 71.50 69.50 66.70 64.30 60.80 56.40 64.87\n",
      "A VG (13 tasks) 93.11 90.28 91.81 88.68 82.95 79.04 87.65\n",
      "Mistral-7B-LongPO-512K\n",
      "NIAH 99.19 97.78 98.06 97.69 96.62 94.36 97.28\n",
      "VT 99.44 99.16 98.04 97.80 95.92 94.52 97.48\n",
      "AGG 87.56 76.71 88.95 72.70 44.93 44.51 69.22\n",
      "QA 71.40 69.50 66.40 64.50 60.60 57.10 64.92\n",
      "A VG (13 tasks) 93.14 90.29 91.78 88.75 83.07 80.97 88.00\n",
      "Table 3: Performance on short-context tasks.\n",
      "Model MMLU ARC-C Hellaswag Winogrande MT-Bench\n",
      "Mistral-7B-Instruct-v0.2 59.15 59.26 83.2 78.4 6.34\n",
      "Mistral-7B-LongPO-128K 59.99 59.34 82.99 78.53 6.35\n",
      "Mistral-7B-LongPO-256K 59.47 60.28 83.14 78.14 6.38\n",
      "Mistral-7B-LongPO-512K 59.51 60.58 82.87 77.66 6.34\n",
      "Qwen2.5-7B-Instruct 74.28 67.15 81.41 74.66 7.30\n",
      "Qwen2.5-7B-LongPO-128K 73.64 65.70 80.82 74.98 7.62\n",
      "17\n",
      "Total tokens in concatenated context: 42257\n",
      "INFO 09-15 11:20:15 [config.py:243] Replacing legacy 'type' key with 'rope_type'\n",
      "INFO 09-15 11:20:15 [config.py:1604] Using max model len 128000\n",
      "INFO 09-15 11:20:15 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-15 11:20:20 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 09-15 11:20:21 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-15 11:20:21 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/home/jovyan/dummy_model', speculative_config=None, tokenizer='/home/jovyan/dummy_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/jovyan/dummy_model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 09-15 11:20:23 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 09-15 11:20:23 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-15 11:20:23 [gpu_model_runner.py:1843] Starting to load model /home/jovyan/dummy_model...\n",
      "INFO 09-15 11:20:23 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 09-15 11:20:24 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.75s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.42s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.47s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-15 11:20:27 [default_loader.py:262] Loading weights took 3.01 seconds\n",
      "INFO 09-15 11:20:27 [gpu_model_runner.py:1892] Model loading took 7.1694 GiB and 3.215795 seconds\n",
      "INFO 09-15 11:20:32 [backends.py:530] Using cache directory: /home/jovyan/.cache/vllm/torch_compile_cache/60cf5f0370/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-15 11:20:32 [backends.py:541] Dynamo bytecode transform time: 4.95 s\n",
      "INFO 09-15 11:20:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.553 s\n",
      "INFO 09-15 11:20:40 [monitor.py:34] torch.compile takes 4.95 s in total\n",
      "INFO 09-15 11:20:41 [gpu_worker.py:255] Available KV cache memory: 30.86 GiB\n",
      "INFO 09-15 11:20:42 [kv_cache_utils.py:833] GPU KV cache size: 252,784 tokens\n",
      "INFO 09-15 11:20:42 [kv_cache_utils.py:837] Maximum concurrency for 128,000 tokens per request: 1.97x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:03<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-15 11:20:45 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.57 GiB\n",
      "INFO 09-15 11:20:45 [core.py:193] init engine (profile, create kv cache, warmup model) took 17.75 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.35s/it, est. speed input: 5752.73 toks/s, output: 1.50 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.36s/it, est. speed input: 5752.73 toks/s, output: 1.50 toks/s]\u001b[A\n",
      "Inference:  33%|███▎      | 1/3 [00:07<00:14,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thought:  \n",
      "answer:  The first clue says: \"People Passion Innovation\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00,  8.74it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s, est. speed input: 68910.96 toks/s, output: 17.92 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s, est. speed input: 68910.96 toks/s, output: 17.92 toks/s]\u001b[A\n",
      "Inference:  67%|██████▋   | 2/3 [00:08<00:03,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thought:  \n",
      "answer:  The second clue is: \"People Passion Innovation.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00,  7.60it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.01s/it, est. speed input: 3251.49 toks/s, output: 18.30 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.01s/it, est. speed input: 3251.49 toks/s, output: 18.30 toks/s]\u001b[A\n",
      "Inference: 100%|██████████| 3/3 [00:21<00:00,  7.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thought:  \n",
      "answer:  Clue #3 is hidden within the description of DSO National Laboratories and Science Centre Singapore, mentioning their roles and capabilities. The phrase \"People Passion Innovation\" appears as part of the organization's profile, hinting at a name or slogan. By examining the provided excerpts, we can't find a direct mention of a slogan or motto containing exactly \"People Passion Innovation.\" However, looking at the structure of the documents, we notice repeated mentions of DSO and SCS, possibly representing the acronym or initials for their full titles, Department of Scientific Organizations and Centers of Science and Safety.\n",
      "\n",
      "Given the format of the articles and the hints provided, it's plausible that \"People Passion Innovation\" is meant to symbolize something related to the departments involved in organizing the Singapore Amazing Flying Machine Competition, perhaps as their catchphrase or theme. But strictly speaking, none of the texts explicitly confirm this phrase as their official slogan or motto. There seems to be a discrepancy between the search for the clue and what's actually present in the documents. If there's supposed to be a match elsewhere or if we're overlooking a subtle inclusion, we'd need to revisit the materials or consider that this might be a trick clue and check for alternative interpretations.\n",
      "Results saved to dummy_model.csv\n",
      "⏱️ Time taken: 54.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W915 11:21:08.415402203 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "run_inference(model_path, \"dummy_files/hard\", system_prompt, question_list, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a03213-341e-4517-a5e4-c5f0cf58d1de",
   "metadata": {},
   "source": [
    "Interesting... the model forgot what the second and third clue were! Can you explain why, and when hallucinations occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da050c5a-c4f0-4612-9f26-e024ac452c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does the first clue say?</td>\n",
       "      <td>The first clue says: \"People Passion Innovation\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the 2nd clue?</td>\n",
       "      <td>The second clue is: \"People Passion Innovation.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Clue #3?</td>\n",
       "      <td>Clue #3 is hidden within the description of DS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question  \\\n",
       "0  What does the first clue say?   \n",
       "1          What is the 2nd clue?   \n",
       "2               What is Clue #3?   \n",
       "\n",
       "                                             answers  \n",
       "0   The first clue says: \"People Passion Innovation\"  \n",
       "1   The second clue is: \"People Passion Innovation.\"  \n",
       "2  Clue #3 is hidden within the description of DS...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"dummy_model.csv\")[[\"question\", \"answers\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
