{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f1bed7-2d94-40f4-865e-cede69d4f4ef",
   "metadata": {},
   "source": [
    "### Does a long context lead to increased hallucinations in LLM's?\n",
    "In this hackathon, we will investigate if it is possible to detect hallucinations in LLM's given a long context.\n",
    "<br>In this case, we define an LLM to be hallucinating if the output is unfaithful to the given information.\n",
    "<br>Your task is to: <br>1) Create a model agnostic solution that can detect any hallucination(s) in the answer that deviate from the context. <br>2) Provide research insights on the problem and lesson's learnt from this hackathon. <br>This is a starter kit for you to start tinkering with the models and to observe it's outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce18f17-581d-4225-b02c-1c75decef078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-23 09:55:18 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoProcessor\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581b822e-e905-4693-8001-18f07ad2dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "max_length = 128000 # can only be as long as your VRAM allows\n",
    "model_path = \"/home/jovyan/dummy_model\"\n",
    "docs_folder = \"dummy_files/easy\"\n",
    "system_prompt = \"You are a helpful assistant. You will be given a long context of concatenated documents with clues hidden in them.\"\n",
    "question_list = \"dummy_questions.csv\"\n",
    "output_file = \"dummy_model.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ab4f46-5232-46cb-99aa-0877b9bf8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supports thinking models\n",
    "def extract_thinking_and_summary(text: str, bot: str = \"<think>\", eot: str = \"</think>\") -> Tuple[str, str]:\n",
    "    if bot in text and eot not in text:\n",
    "        return \"\", text\n",
    "    if eot in text:\n",
    "        if bot in text:\n",
    "            return (\n",
    "                text[text.index(bot) + len(bot):text.index(eot)].strip(),\n",
    "                text[text.index(eot) + len(eot):].strip(),\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                text[:text.index(eot)].strip(),\n",
    "                text[text.index(eot) + len(eot):].strip(),\n",
    "            )\n",
    "    return \"\", text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f9ef0d-eb05-4a11-be07-78080cfa3fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load paths of documents (.txt files) in docs_folder\n",
    "def load_documents(folder_path: str) -> List[Tuple[str, str]]:\n",
    "    docs = []\n",
    "    for file in sorted(Path(folder_path).glob(\"*.txt\")):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            docs.append((file.name, f.read()))\n",
    "            # print(docs[-1]) # for debugging\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac7ad112-be8f-415f-b3d2-e9cd047699ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine documents into one long context\n",
    "def concat_documents(docs: List[Tuple[str, str]], tokenizer, max_tokens=max_length) -> Tuple[str, int]:\n",
    "    combined_text = \"\"\n",
    "    token_count = 0\n",
    "    for name, text in docs:\n",
    "        header = f\"\\n\\n===== Document: {name} =====\\n\\n\"\n",
    "        combined_text += header + text\n",
    "        tokens = tokenizer.encode(combined_text, add_special_tokens=False)\n",
    "        token_count = len(tokens)\n",
    "        print(f\"Added {name}: cumulative tokens = {token_count}\") # provides sensing of token contribution of each document\n",
    "        if token_count > max_tokens:\n",
    "            print(f\"⚠️ Warning: Context size exceeded {max_tokens} tokens!\") \n",
    "    return combined_text, token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1dddcf7-616c-4a3c-aa71-0a745945e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model_path: str, docs_folder: str, system_prompt: str, question_list: str, output_file: str):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load documents\n",
    "    docs = load_documents(docs_folder)\n",
    "    if not docs:\n",
    "        print(f\"No text files found in {docs_folder}.\")\n",
    "        return\n",
    "\n",
    "    # Initialize tokenizer & processor\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "    # Concatenate documents with headers\n",
    "    combined_text, total_tokens = concat_documents(docs, tokenizer)\n",
    "    print(f\"Concatenated context snippet: {combined_text[:1000]}\")\n",
    "    print(f\"Total tokens in concatenated context: {total_tokens}\")\n",
    "\n",
    "    # Prepare questions from .csv file\n",
    "    df = pd.read_csv(question_list)\n",
    "    \n",
    "    thoughts = []\n",
    "    answers = []\n",
    "    \n",
    "    # Initialize vLLM model\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        trust_remote_code=True,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization=0.9,\n",
    "        max_model_len=max_length\n",
    "    )\n",
    "\n",
    "    # Set sampling parameters\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        top_p=1,\n",
    "        repetition_penalty=1.05,\n",
    "        max_tokens=8196,\n",
    "        stop_token_ids=[]\n",
    "    )\n",
    "    \n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Inference\"):\n",
    "        try:\n",
    "            # Safe parsing of messages\n",
    "            qn = row['question']\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": combined_text + qn}]\n",
    "            \n",
    "            prompt = processor.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "           \n",
    "            llm_inputs = {\"prompt\": prompt}\n",
    "\n",
    "            outputs = llm.generate([llm_inputs], sampling_params=sampling_params)\n",
    "            generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "            thought, answer = extract_thinking_and_summary(generated_text)\n",
    "            print(\"thought: \", thought)\n",
    "            print(\"answer: \", answer)\n",
    "            thoughts.append(thought)\n",
    "            answers.append(answer)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Error on row {i}: {e}\")\n",
    "            thoughts.append(\"\")\n",
    "            answers.append(\"\")\n",
    "\n",
    "    df[\"thoughts\"] = thoughts\n",
    "    df[\"answers\"] = answers\n",
    "\n",
    "    # Save result as .csv\n",
    "    model_name = os.path.basename(model_path).replace(\"/\", \"_\")\n",
    "    output_path = Path(output_file or f\"{model_name}_results.csv\")\n",
    "    df.to_csv(output_path)\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"⏱️ Time taken: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69764fc-8213-4fc0-a080-d961886b7f2b",
   "metadata": {},
   "source": [
    "Let's see if the model can extract 3 hidden clues in the concatenated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b654d258-e8fc-4846-8459-3714908d40ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added article1.txt: cumulative tokens = 1575\n",
      "Added article2.txt: cumulative tokens = 2347\n",
      "Added article3.txt: cumulative tokens = 3420\n",
      "Added article4.txt: cumulative tokens = 3858\n",
      "Added article5.txt: cumulative tokens = 4621\n",
      "Added article6.txt: cumulative tokens = 5461\n",
      "Added article7.txt: cumulative tokens = 6666\n",
      "Added article8.txt: cumulative tokens = 7472\n",
      "Added article9.txt: cumulative tokens = 8299\n",
      "Concatenated context snippet: \n",
      "\n",
      "===== Document: article1.txt =====\n",
      "\n",
      "Pushing Boundaries in Flight: The 16th Singapore Amazing Flying Machine Competition Honours Innovation with Thrilling Demonstrations and a Celebration of Creativity\n",
      "\n",
      "Singapore, 5 April 2025 – The Awards Ceremony for the 16th Singapore Amazing Flying Machine Competition (SAFMC) 2025 took place today at the Singapore University of Technology and Design (SUTD). Co-organised by DSO National Laboratories (DSO) and Science Centre Singapore (SCS), this year’s competition brought together 1,865 participants across 584 teams, competing in five distinct categories.\n",
      "\n",
      "\n",
      "The ceremony's Guest of Honour, Mr Heng Chee How, Senior Minister of State for Defence, experienced the thrill of the teams' innovative flying machines through live demonstrations and presented awards to the winners.\n",
      "\n",
      " \n",
      "\n",
      "Speaking at the ceremony, Mr Heng highlighted, “This ceremony reminds us of the journey our participants have undertaken – a journey fuelled by creativity, the spirit of discove\n",
      "Total tokens in concatenated context: 8299\n",
      "INFO 09-23 09:55:22 [config.py:243] Replacing legacy 'type' key with 'rope_type'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 09:55:22,729\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-23 09:55:29 [config.py:1604] Using max model len 128000\n",
      "INFO 09-23 09:55:29 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-23 09:55:34 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 09-23 09:55:36 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-23 09:55:36 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/home/jovyan/dummy_model', speculative_config=None, tokenizer='/home/jovyan/dummy_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/jovyan/dummy_model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 09-23 09:55:38 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 09-23 09:55:38 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-23 09:55:38 [gpu_model_runner.py:1843] Starting to load model /home/jovyan/dummy_model...\n",
      "INFO 09-23 09:55:38 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 09-23 09:55:38 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.65s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.38s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.42s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-23 09:55:43 [default_loader.py:262] Loading weights took 4.93 seconds\n",
      "INFO 09-23 09:55:44 [gpu_model_runner.py:1892] Model loading took 7.1694 GiB and 5.127595 seconds\n",
      "INFO 09-23 09:55:49 [backends.py:530] Using cache directory: /home/jovyan/.cache/vllm/torch_compile_cache/60cf5f0370/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-23 09:55:49 [backends.py:541] Dynamo bytecode transform time: 4.95 s\n",
      "INFO 09-23 09:55:57 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.244 s\n",
      "INFO 09-23 09:55:57 [monitor.py:34] torch.compile takes 4.95 s in total\n",
      "INFO 09-23 09:55:59 [gpu_worker.py:255] Available KV cache memory: 30.86 GiB\n",
      "INFO 09-23 09:55:59 [kv_cache_utils.py:833] GPU KV cache size: 252,784 tokens\n",
      "INFO 09-23 09:55:59 [kv_cache_utils.py:837] Maximum concurrency for 128,000 tokens per request: 1.97x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:02<00:00, 22.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-23 09:56:02 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.57 GiB\n",
      "INFO 09-23 09:56:02 [core.py:193] init engine (profile, create kv cache, warmup model) took 18.33 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 29.23it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s, est. speed input: 9121.94 toks/s, output: 10.95 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s, est. speed input: 9121.94 toks/s, output: 10.95 toks/s]\u001b[A\n",
      "Inference:  33%|███▎      | 1/3 [00:00<00:01,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thought:  \n",
      "answer:  The first clue says \"People Passion Innovation.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 37.49it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s, est. speed input: 24510.70 toks/s, output: 41.18 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.91it/s, est. speed input: 24510.70 toks/s, output: 41.18 toks/s]\u001b[A\n",
      "Inference:  67%|██████▋   | 2/3 [00:01<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thought:  \n",
      "answer:  The second clue provided in the text is \"DSO53.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 34.46it/s]\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s, est. speed input: 31385.28 toks/s, output: 41.43 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s, est. speed input: 31385.28 toks/s, output: 41.43 toks/s]\u001b[A\n",
      "Inference: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thought:  \n",
      "answer:  Clue #3 is \"I love IEL!\"\n",
      "Results saved to dummy_model.csv\n",
      "⏱️ Time taken: 44.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W923 09:56:06.946705048 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "run_inference(model_path, docs_folder, system_prompt, question_list, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf498940-6904-4c5d-86b1-a07296d99e36",
   "metadata": {},
   "source": [
    "It seems like a context length of 8k isn't that big of a problem for the model. What if we extend the context length with more documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0398c382-4051-4ec6-9b46-25a594bc72d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does the first clue say?</td>\n",
       "      <td>The first clue says \"People Passion Innovation.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the 2nd clue?</td>\n",
       "      <td>The second clue provided in the text is \"DSO53.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Clue #3?</td>\n",
       "      <td>Clue #3 is \"I love IEL!\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question  \\\n",
       "0  What does the first clue say?   \n",
       "1          What is the 2nd clue?   \n",
       "2               What is Clue #3?   \n",
       "\n",
       "                                            answers  \n",
       "0  The first clue says \"People Passion Innovation.\"  \n",
       "1  The second clue provided in the text is \"DSO53.\"  \n",
       "2                          Clue #3 is \"I love IEL!\"  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"dummy_model.csv\")[[\"question\", \"answers\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "835f70c4-5eea-46a6-8d62-24f086bc2f31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added article1.txt: cumulative tokens = 1575\n",
      "Added article2.txt: cumulative tokens = 2347\n",
      "Added article3.txt: cumulative tokens = 3420\n",
      "Added article4.txt: cumulative tokens = 3858\n",
      "Added article5.txt: cumulative tokens = 4621\n",
      "Added article6.txt: cumulative tokens = 5461\n",
      "Added article7.txt: cumulative tokens = 6666\n",
      "Added article8.txt: cumulative tokens = 7472\n",
      "Added article9.txt: cumulative tokens = 8299\n",
      "Added paper1.txt: cumulative tokens = 25430\n",
      "Added paper2.txt: cumulative tokens = 42257\n",
      "Concatenated context snippet: \n",
      "\n",
      "===== Document: article1.txt =====\n",
      "\n",
      "Pushing Boundaries in Flight: The 16th Singapore Amazing Flying Machine Competition Honours Innovation with Thrilling Demonstrations and a Celebration of Creativity\n",
      "\n",
      "Singapore, 5 April 2025 – The Awards Ceremony for the 16th Singapore Amazing Flying Machine Competition (SAFMC) 2025 took place today at the Singapore University of Technology and Design (SUTD). Co-organised by DSO National Laboratories (DSO) and Science Centre Singapore (SCS), this year’s competition brought together 1,865 participants across 584 teams, competing in five distinct categories.\n",
      "\n",
      "\n",
      "The ceremony's Guest of Honour, Mr Heng Chee How, Senior Minister of State for Defence, experienced the thrill of the teams' innovative flying machines through live demonstrations and presented awards to the winners.\n",
      "\n",
      " \n",
      "\n",
      "Speaking at the ceremony, Mr Heng highlighted, “This ceremony reminds us of the journey our participants have undertaken – a journey fuelled by creativity, the spirit of discove\n",
      "Total tokens in concatenated context: 42257\n",
      "INFO 09-23 09:56:20 [config.py:243] Replacing legacy 'type' key with 'rope_type'\n",
      "INFO 09-23 09:56:20 [config.py:1604] Using max model len 128000\n",
      "INFO 09-23 09:56:20 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-23 09:56:25 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 09-23 09:56:26 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 09-23 09:56:26 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/home/jovyan/dummy_model', speculative_config=None, tokenizer='/home/jovyan/dummy_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/jovyan/dummy_model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 09-23 09:56:28 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 09-23 09:56:28 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-23 09:56:28 [gpu_model_runner.py:1843] Starting to load model /home/jovyan/dummy_model...\n",
      "INFO 09-23 09:56:29 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 09-23 09:56:29 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.46s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.23s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.27s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-23 09:56:31 [default_loader.py:262] Loading weights took 2.68 seconds\n",
      "INFO 09-23 09:56:32 [gpu_model_runner.py:1892] Model loading took 7.1694 GiB and 2.868716 seconds\n",
      "INFO 09-23 09:56:37 [backends.py:530] Using cache directory: /home/jovyan/.cache/vllm/torch_compile_cache/60cf5f0370/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-23 09:56:37 [backends.py:541] Dynamo bytecode transform time: 4.98 s\n",
      "INFO 09-23 09:56:44 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.582 s\n",
      "INFO 09-23 09:56:45 [monitor.py:34] torch.compile takes 4.98 s in total\n",
      "INFO 09-23 09:56:46 [gpu_worker.py:255] Available KV cache memory: 30.86 GiB\n",
      "INFO 09-23 09:56:46 [kv_cache_utils.py:833] GPU KV cache size: 252,784 tokens\n",
      "INFO 09-23 09:56:46 [kv_cache_utils.py:837] Maximum concurrency for 128,000 tokens per request: 1.97x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:02<00:00, 22.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-23 09:56:50 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.57 GiB\n",
      "INFO 09-23 09:56:50 [core.py:193] init engine (profile, create kv cache, warmup model) took 17.68 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00,  7.32it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.39s/it, est. speed input: 5721.32 toks/s, output: 1.49 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.40s/it, est. speed input: 5721.32 toks/s, output: 1.49 toks/s]\u001b[A\n",
      "Inference:  33%|███▎      | 1/3 [00:07<00:15,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thought:  \n",
      "answer:  The first clue says: \"People Passion Innovation\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00,  8.38it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s, est. speed input: 68427.66 toks/s, output: 17.80 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s, est. speed input: 68427.66 toks/s, output: 17.80 toks/s]\u001b[A\n",
      "Inference:  67%|██████▋   | 2/3 [00:08<00:03,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thought:  \n",
      "answer:  The second clue is: \"People Passion Innovation.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00,  8.40it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.97s/it, est. speed input: 3259.45 toks/s, output: 18.34 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.98s/it, est. speed input: 3259.45 toks/s, output: 18.34 toks/s]\u001b[A\n",
      "Inference: 100%|██████████| 3/3 [00:21<00:00,  7.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thought:  \n",
      "answer:  Clue #3 is hidden within the description of DSO National Laboratories and Science Centre Singapore, mentioning their roles and capabilities. The phrase \"People Passion Innovation\" appears as part of the organization's profile, hinting at a name or slogan. By examining the provided excerpts, we can't find a direct mention of a slogan or motto containing exactly \"People Passion Innovation.\" However, looking at the structure of the documents, we notice repeated mentions of DSO and SCS, possibly representing the acronym or initials for their full titles, Department of Scientific Organizations and Centers of Science and Safety.\n",
      "\n",
      "Given the format of the articles and the hints provided, it's plausible that \"People Passion Innovation\" is meant to symbolize something related to the departments involved in organizing the Singapore Amazing Flying Machine Competition, perhaps as their catchphrase or theme. But strictly speaking, none of the texts explicitly confirm this phrase as their official slogan or motto. There seems to be a discrepancy between the search for the clue and what's actually present in the documents. If there's supposed to be a match elsewhere or if we're overlooking a subtle inclusion, we'd need to revisit the materials or consider that this might be a trick clue and check for alternative interpretations.\n",
      "Results saved to dummy_model.csv\n",
      "⏱️ Time taken: 53.45 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W923 09:57:13.947703006 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "run_inference(model_path, \"dummy_files/hard\", system_prompt, question_list, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a03213-341e-4517-a5e4-c5f0cf58d1de",
   "metadata": {},
   "source": [
    "Interesting... the model forgot what the second and third clue were! Can you explain why, and when hallucinations occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da050c5a-c4f0-4612-9f26-e024ac452c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does the first clue say?</td>\n",
       "      <td>The first clue says: \"People Passion Innovation\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the 2nd clue?</td>\n",
       "      <td>The second clue is: \"People Passion Innovation.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Clue #3?</td>\n",
       "      <td>Clue #3 is hidden within the description of DS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question  \\\n",
       "0  What does the first clue say?   \n",
       "1          What is the 2nd clue?   \n",
       "2               What is Clue #3?   \n",
       "\n",
       "                                             answers  \n",
       "0   The first clue says: \"People Passion Innovation\"  \n",
       "1   The second clue is: \"People Passion Innovation.\"  \n",
       "2  Clue #3 is hidden within the description of DS...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"dummy_model.csv\")[[\"question\", \"answers\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
